<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>SIGGRAPH Asia 2025 - Technical Papers</title>
    <link rel="icon" href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'><text y='.9em' font-size='90'>üé¨</text></svg>">
    <style>

:root {
    --bg-primary: #fef9f3;
    --bg-secondary: #fff5eb;
    --bg-card: #ffffff;
    --bg-card-hover: #fff8f0;
    --text-primary: #2d2a3e;
    --text-secondary: #6b6880;
    --accent: #ff6b6b;
    --accent-secondary: #4ecdc4;
    --accent-tertiary: #ffe66d;
    --accent-gradient: linear-gradient(135deg, #ff6b6b 0%, #feca57 50%, #4ecdc4 100%);
    --border: #f0e6dc;
    --shadow: rgba(255, 107, 107, 0.15);
}

@import url('https://fonts.googleapis.com/css2?family=Fredoka:wght@400;500;600;700&family=Nunito:wght@400;500;600;700&display=swap');

* {
    margin: 0;
    padding: 0;
    box-sizing: border-box;
}

html {
    scroll-behavior: smooth;
}

body {
    font-family: 'Nunito', -apple-system, BlinkMacSystemFont, sans-serif;
    background: var(--bg-primary);
    color: var(--text-primary);
    line-height: 1.7;
    min-height: 100vh;
    font-size: 17px;
}

.bg-pattern {
    position: fixed;
    top: 0;
    left: 0;
    right: 0;
    bottom: 0;
    background-image: 
        radial-gradient(circle at 10% 20%, rgba(255, 107, 107, 0.12) 0%, transparent 50%),
        radial-gradient(circle at 90% 80%, rgba(78, 205, 196, 0.1) 0%, transparent 50%),
        radial-gradient(circle at 50% 50%, rgba(255, 230, 109, 0.08) 0%, transparent 60%);
    pointer-events: none;
    z-index: 0;
}

.container {
    max-width: 1500px;
    margin: 0 auto;
    padding: 2rem;
    position: relative;
    z-index: 1;
}

header {
    text-align: center;
    padding: 5rem 2rem 4rem;
    position: relative;
}

header::before {
    content: '';
    position: absolute;
    top: -50px;
    left: 50%;
    transform: translateX(-50%);
    width: 500px;
    height: 500px;
    background: radial-gradient(ellipse at center, rgba(255, 230, 109, 0.25) 0%, transparent 70%);
    pointer-events: none;
    border-radius: 50%;
}

.logo {
    font-family: 'Fredoka', sans-serif;
    font-size: 1rem;
    letter-spacing: 0.2em;
    text-transform: uppercase;
    color: var(--accent);
    margin-bottom: 1.5rem;
    font-weight: 600;
}

header h1 {
    font-family: 'Fredoka', sans-serif;
    font-size: 4.5rem;
    font-weight: 700;
    letter-spacing: -0.02em;
    background: var(--accent-gradient);
    -webkit-background-clip: text;
    -webkit-text-fill-color: transparent;
    background-clip: text;
    margin-bottom: 0.75rem;
    line-height: 1.1;
}

header .subtitle {
    font-family: 'Fredoka', sans-serif;
    font-size: 1.6rem;
    font-weight: 500;
    color: var(--text-secondary);
    margin-bottom: 2rem;
}

.meta-info {
    display: inline-flex;
    align-items: center;
    gap: 2rem;
    padding: 1rem 2rem;
    background: var(--bg-card);
    border: 2px solid var(--border);
    border-radius: 100px;
    font-size: 1rem;
    box-shadow: 0 4px 20px var(--shadow);
}

.meta-info span {
    display: flex;
    align-items: center;
    gap: 0.5rem;
    color: var(--text-secondary);
    font-weight: 600;
}

.meta-info .icon {
    font-size: 1.2rem;
}

.stats-bar {
    display: flex;
    justify-content: center;
    gap: 4rem;
    margin-top: 3rem;
    padding-top: 2rem;
    border-top: 2px dashed var(--border);
}

.stat-item {
    text-align: center;
}

.stat-value {
    font-family: 'Fredoka', sans-serif;
    font-size: 3.5rem;
    font-weight: 700;
    background: var(--accent-gradient);
    -webkit-background-clip: text;
    -webkit-text-fill-color: transparent;
    background-clip: text;
    line-height: 1;
}

.stat-label {
    font-size: 0.9rem;
    text-transform: uppercase;
    letter-spacing: 0.1em;
    color: var(--text-secondary);
    margin-top: 0.5rem;
    font-weight: 600;
}

.session {
    margin-bottom: 4rem;
}

.session-header {
    display: flex;
    align-items: center;
    gap: 1rem;
    margin-bottom: 2rem;
    padding-bottom: 1rem;
    border-bottom: 3px dashed var(--border);
}

.session-header::before {
    content: '‚ú¶';
    font-size: 1.5rem;
    color: var(--accent);
}

.session h2 {
    font-family: 'Fredoka', sans-serif;
    font-size: 1.7rem;
    font-weight: 600;
    flex: 1;
    color: var(--text-primary);
}

.session-count {
    font-size: 0.9rem;
    font-weight: 700;
    color: var(--accent);
    background: rgba(255, 107, 107, 0.1);
    padding: 0.5rem 1.2rem;
    border-radius: 100px;
    border: 2px solid rgba(255, 107, 107, 0.2);
}

.papers-grid {
    display: grid;
    grid-template-columns: repeat(auto-fill, minmax(340px, 1fr));
    gap: 1.75rem;
}

.paper-card {
    background: var(--bg-card);
    border-radius: 20px;
    overflow: hidden;
    border: 2px solid var(--border);
    transition: all 0.3s cubic-bezier(0.4, 0, 0.2, 1);
    display: flex;
    flex-direction: column;
    box-shadow: 0 4px 15px rgba(0, 0, 0, 0.05);
}

.paper-card:hover {
    transform: translateY(-8px) rotate(-0.5deg);
    box-shadow: 
        0 20px 40px rgba(255, 107, 107, 0.15),
        0 0 0 3px rgba(255, 107, 107, 0.1);
    border-color: var(--accent);
}

.thumbnail-wrapper {
    position: relative;
    width: 100%;
    padding-top: 56.25%;
    overflow: hidden;
    background: var(--bg-secondary);
}

.thumbnail {
    position: absolute;
    top: 0;
    left: 0;
    width: 100%;
    height: 100%;
    object-fit: cover;
    transition: transform 0.5s cubic-bezier(0.4, 0, 0.2, 1);
}

.paper-card:hover .thumbnail {
    transform: scale(1.1);
}

.thumbnail.placeholder {
    display: flex;
    align-items: center;
    justify-content: center;
    font-size: 3.5rem;
    color: var(--border);
    background: linear-gradient(135deg, var(--bg-secondary) 0%, var(--bg-card) 100%);
}

.card-content {
    padding: 1.5rem;
    flex: 1;
    display: flex;
    flex-direction: column;
}

.paper-card h3 {
    font-family: 'Fredoka', sans-serif;
    font-size: 1.1rem;
    font-weight: 600;
    line-height: 1.4;
    margin-bottom: 0.85rem;
    display: -webkit-box;
    -webkit-line-clamp: 3;
    -webkit-box-orient: vertical;
    overflow: hidden;
    color: var(--text-primary);
}

.authors {
    font-size: 0.95rem;
    color: var(--text-secondary);
    line-height: 1.6;
    margin-top: auto;
    display: -webkit-box;
    -webkit-line-clamp: 2;
    -webkit-box-orient: vertical;
    overflow: hidden;
    font-weight: 500;
}

.paper-title-link {
    color: inherit;
    text-decoration: none;
}

.paper-title-link:hover {
    text-decoration: underline;
    text-decoration-thickness: 3px;
    text-underline-offset: 3px;
    text-decoration-color: rgba(255, 107, 107, 0.7);
}

.paper-actions {
    display: flex;
    gap: 0.5rem;
    margin-top: 1rem;
    padding-top: 1rem;
    border-top: 2px dashed var(--border);
    align-items: center;
    flex-wrap: wrap;
}

.paper-links {
    display: flex;
    gap: 0.5rem;
}

.abstract-toggle {
    margin-top: 0;
    padding-top: 0;
    border-top: none;
}

.abstract-toggle summary {
    list-style: none;
    cursor: pointer;
    display: inline-flex;
    align-items: center;
    gap: 0.5rem;
    padding: 0.45rem 0.9rem;
    border-radius: 100px;
    background: rgba(255, 230, 109, 0.25);
    border: 2px solid rgba(255, 230, 109, 0.45);
    font-weight: 800;
    font-size: 0.9rem;
    color: var(--text-primary);
    user-select: none;
}

.abstract-toggle summary::-webkit-details-marker {
    display: none;
}

.abstract-toggle summary:hover {
    transform: scale(1.03);
}

.abstract-toggle .abstract-body {
    margin-top: 0.75rem;
    padding: 0.9rem 1rem;
    background: rgba(78, 205, 196, 0.08);
    border: 2px solid rgba(78, 205, 196, 0.18);
    border-radius: 14px;
    color: var(--text-primary);
    font-size: 0.98rem;
    line-height: 1.7;
    white-space: pre-wrap;
}

.abstract-toggle .abstract-missing {
    opacity: 0.75;
    font-style: italic;
}

.paper-link {
    display: inline-flex;
    align-items: center;
    gap: 0.4rem;
    padding: 0.5rem 1rem;
    border-radius: 100px;
    font-size: 0.85rem;
    font-weight: 800;
    text-decoration: none;
    transition: all 0.2s ease;
    background: rgba(78, 205, 196, 0.12);
    color: #0f766e;
    border: 2px solid rgba(78, 205, 196, 0.35);
    position: relative;
}

.paper-link:hover {
    transform: scale(1.04);
    background: rgba(78, 205, 196, 0.18);
}

.paper-link .edit-icon-inline {
    margin-left: 0.3rem;
    padding: 0.2rem 0.4rem;
    background: rgba(255, 107, 107, 0.2);
    border-radius: 50%;
    font-size: 0.7rem;
    cursor: pointer;
    transition: all 0.2s ease;
    display: inline-flex;
    align-items: center;
    justify-content: center;
}

.paper-link .edit-icon-inline:hover {
    background: rgba(255, 107, 107, 0.35);
    transform: scale(1.15);
}

.edit-btn, .edit-btn-inline {
    background: rgba(255, 107, 107, 0.15);
    border: 2px solid rgba(255, 107, 107, 0.35);
    border-radius: 100px;
    padding: 0.4rem 0.7rem;
    font-size: 0.75rem;
    font-weight: 700;
    color: #c62828;
    cursor: pointer;
    transition: all 0.2s ease;
    display: inline-flex;
    align-items: center;
    gap: 0.3rem;
    margin-left: 0.3rem;
}

.edit-btn:hover, .edit-btn-inline:hover {
    background: rgba(255, 107, 107, 0.25);
    transform: scale(1.05);
}

.edit-btn-inline {
    margin-left: 0.5rem;
    padding: 0.25rem 0.5rem;
    font-size: 0.7rem;
}

.abstract-toggle summary {
    display: flex;
    align-items: center;
    justify-content: space-between;
}

/* Edit Modal */
.edit-modal {
    display: none;
    position: fixed;
    top: 0;
    left: 0;
    right: 0;
    bottom: 0;
    background: rgba(0, 0, 0, 0.6);
    z-index: 1000;
    align-items: center;
    justify-content: center;
    padding: 2rem;
}

.edit-modal.active {
    display: flex;
}

.edit-modal-content {
    background: var(--bg-card);
    border: 3px solid var(--accent);
    border-radius: 20px;
    padding: 2rem;
    max-width: 600px;
    width: 100%;
    max-height: 80vh;
    overflow-y: auto;
    box-shadow: 0 20px 60px rgba(0, 0, 0, 0.3);
}

.edit-modal-header {
    display: flex;
    justify-content: space-between;
    align-items: center;
    margin-bottom: 1.5rem;
    padding-bottom: 1rem;
    border-bottom: 2px dashed var(--border);
}

.edit-modal-header h3 {
    font-family: 'Fredoka', sans-serif;
    font-size: 1.3rem;
    color: var(--text-primary);
    margin: 0;
}

.edit-modal-close {
    background: rgba(255, 107, 107, 0.2);
    border: 2px solid rgba(255, 107, 107, 0.4);
    border-radius: 100px;
    width: 2rem;
    height: 2rem;
    display: flex;
    align-items: center;
    justify-content: center;
    cursor: pointer;
    font-size: 1.2rem;
    color: var(--accent);
    transition: all 0.2s ease;
}

.edit-modal-close:hover {
    background: rgba(255, 107, 107, 0.3);
    transform: scale(1.1);
}

.edit-modal-body {
    margin-bottom: 1.5rem;
}

.edit-modal-body label {
    display: block;
    font-weight: 700;
    margin-bottom: 0.5rem;
    color: var(--text-primary);
    font-size: 0.9rem;
}

.edit-modal-body input,
.edit-modal-body textarea {
    width: 100%;
    padding: 0.75rem;
    border: 2px solid var(--border);
    border-radius: 12px;
    font-family: 'Nunito', sans-serif;
    font-size: 0.95rem;
    background: var(--bg-secondary);
    color: var(--text-primary);
    resize: vertical;
}

.edit-modal-body textarea {
    min-height: 150px;
    line-height: 1.6;
}

.edit-modal-body input:focus,
.edit-modal-body textarea:focus {
    outline: none;
    border-color: var(--accent);
    box-shadow: 0 0 0 3px rgba(255, 107, 107, 0.1);
}

.edit-modal-footer {
    display: flex;
    gap: 0.75rem;
    justify-content: flex-end;
}

.edit-modal-btn {
    padding: 0.6rem 1.5rem;
    border-radius: 100px;
    font-weight: 700;
    font-size: 0.9rem;
    cursor: pointer;
    transition: all 0.2s ease;
    border: 2px solid;
}

.edit-modal-btn.save {
    background: rgba(78, 205, 196, 0.2);
    border-color: rgba(78, 205, 196, 0.4);
    color: #0f766e;
}

.edit-modal-btn.save:hover {
    background: rgba(78, 205, 196, 0.3);
    transform: scale(1.05);
}

.edit-modal-btn.cancel {
    background: rgba(255, 107, 107, 0.1);
    border-color: rgba(255, 107, 107, 0.3);
    color: var(--text-secondary);
}

.edit-modal-btn.cancel:hover {
    background: rgba(255, 107, 107, 0.2);
}

.export-btn-container {
    position: fixed;
    bottom: 2rem;
    right: 2rem;
    z-index: 100;
}

.export-btn {
    background: var(--accent-gradient);
    color: white;
    border: none;
    border-radius: 100px;
    padding: 1rem 1.5rem;
    font-family: 'Fredoka', sans-serif;
    font-weight: 700;
    font-size: 0.9rem;
    cursor: pointer;
    box-shadow: 0 4px 20px rgba(255, 107, 107, 0.3);
    transition: all 0.2s ease;
}

.export-btn:hover {
    transform: translateY(-2px);
    box-shadow: 0 6px 25px rgba(255, 107, 107, 0.4);
}

footer {
    text-align: center;
    padding: 3rem 2rem;
    margin-top: 2rem;
    border-top: 3px dashed var(--border);
    color: var(--text-secondary);
    font-size: 1rem;
}

footer a {
    color: var(--accent);
    text-decoration: none;
    font-weight: 600;
}

footer a:hover {
    text-decoration: underline;
}

@media (max-width: 768px) {
    header h1 {
        font-size: 2.8rem;
    }
    
    .papers-grid {
        grid-template-columns: 1fr;
    }
    
    .stats-bar {
        gap: 2rem;
    }
    
    .meta-info {
        flex-direction: column;
        gap: 0.75rem;
    }
}

    </style>
</head>
<body>
    <div class="bg-pattern"></div>
    
    <header>
        <div class="logo">ACM SIGGRAPH</div>
        <h1>SIGGRAPH Asia 2025</h1>
        <p class="subtitle">Technical Papers Collection</p>
        
        <div class="meta-info">
            <span><span class="icon">üìç</span> Hong Kong</span>
            <span><span class="icon">üìÖ</span> December 13-19, 2025</span>
        </div>
        
        <div class="stats-bar">
            <div class="stat-item">
                <div class="stat-value">301</div>
                <div class="stat-label">Technical Papers</div>
            </div>
            <div class="stat-item">
                <div class="stat-value">54</div>
                <div class="stat-label">Sessions</div>
            </div>
        </div>
    </header>
    
    <main class="container">
        
        <section class="session">
            <div class="session-header">
                <h2>3D Reconstruction & Intelligent Geometry</h2>
                <span class="session-count">5 papers</span>
            </div>
            <div class="papers-grid">
                
            <article class="paper-card" data-paper-id="papers_1338">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/uXyc7AiLBPdeze6v.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://cic.tju.edu.cn/faculty/zhangjiawan/Jiawan_Zhang_files/paper/gaofangzhou2025.pdf" target="_blank" rel="noopener">RCTrans: Transparent Object Reconstruction in Natural Scene via Refractive Correspondence Estimation</a></h3>
                    <p class="authors">Fangzhou Gao, Yuzhen Kang, Lianghao Zhang, Li Wang, Qishen Wang, Jiawan Zhang</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://cic.tju.edu.cn/faculty/zhangjiawan/Jiawan_Zhang_files/paper/gaofangzhou2025.pdf" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1338', 'url', 'https://cic.tju.edu.cn/faculty/zhangjiawan/Jiawan_Zhang_files/paper/gaofangzhou2025.pdf')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1338', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_2088">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/aCmWEZoZeNuWLVpH.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="http://guochch.github.io/TaoGS/" target="_blank" rel="noopener">Topology-Aware Optimization of Gaussian Primitives for Human-Centric Volumetric Videos</a></h3>
                    <p class="authors">Yuheng Jiang, Chengcheng Guo, Yize Wu, Yu Hong, Shengkun Zhu, Zhehao Shen</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="http://guochch.github.io/TaoGS/" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_2088', 'url', 'http://guochch.github.io/TaoGS/')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_2088', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_2078">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/FDzY8ZrJoTAkjegb.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://lukas.uzolas.com/SurfaceAware3DFeatures/" target="_blank" rel="noopener">Surface-Aware Distilled 3D Semantic Features</a></h3>
                    <p class="authors">Lukas Uzolas, Elmar Eisemann, Petr Kellnhofer</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://lukas.uzolas.com/SurfaceAware3DFeatures/" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_2078', 'url', 'https://lukas.uzolas.com/SurfaceAware3DFeatures/')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_2078', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_2076">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/aAdxTnb3kkj3JCAk.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://tau-vailab.github.io/Lang3D-XL/" target="_blank" rel="noopener">Lang3D-XL: Language Embedded 3D Gaussians for Large-scale Scenes</a></h3>
                    <p class="authors">Shai Krakovsky, Gal Fiebelman, Sagie Benaim, Hadar Averbuch-Elor</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://tau-vailab.github.io/Lang3D-XL/" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_2076', 'url', 'https://tau-vailab.github.io/Lang3D-XL/')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_2076', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1315">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/YSd2Zx3PHjyJiyGt.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://armanmaesumi.github.io/poissonnet/" target="_blank" rel="noopener">PoissonNet: A Local-Global Approach for Learning on Surfaces</a></h3>
                    <p class="authors">Arman Maesumi, Tanish Makadia, Thibault Groueix, Vladimir Kim, Daniel Ritchie, Noam Aigerman</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://armanmaesumi.github.io/poissonnet/" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1315', 'url', 'https://armanmaesumi.github.io/poissonnet/')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1315', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            </div>
        </section>
        
        <section class="session">
            <div class="session-header">
                <h2>Dynamic Generative Video: From Synthesis to Real-Time Editing</h2>
                <span class="session-count">6 papers</span>
            </div>
            <div class="papers-grid">
                
            <article class="paper-card" data-paper-id="papers_1105">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/Yy9X6HXAY6QRYy5r.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://arxiv.org/pdf/2506.24108" target="_blank" rel="noopener">Navigating with Annealing Guidance Scale in Diffusion Space</a></h3>
                    <p class="authors">Shai Yehezkel, Omer Dahary, Andrey Voynov, Daniel Cohen-Or</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://arxiv.org/pdf/2506.24108" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1105', 'url', 'https://arxiv.org/pdf/2506.24108')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1105', 'abstract', 'Denoising diffusion models excel at generating high-quality images conditioned on text prompts, yet their effectiveness heavily relies on careful guidance during the sampling process. Classifier-Free Guidance (CFG) provides a widely used mechanism for steering generation by setting the guidance scale, which balances image quality and prompt alignment. However, the choice of the guidance scale has a critical impact on the convergence toward a visually appealing and prompt-adherent image. In this work, we propose an annealing guidance scheduler which dynamically adjusts the guidance scale over time based on the conditional noisy signal. By learning a scheduling policy, our method addresses the temperamental behavior of CFG. Empirical results demonstrate that our guidance scheduler significantly enhances image quality and alignment with the text prompt, advancing the performance of text-to-image generation. Notably, our novel scheduler requires no additional activations or memory consumption, and can seamlessly replace the common classifier-free guidance, offering an improved trade-off between prompt alignment and quality.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">Denoising diffusion models excel at generating high-quality images conditioned on text prompts, yet their effectiveness heavily relies on careful guidance during the sampling process. Classifier-Free Guidance (CFG) provides a widely used mechanism for steering generation by setting the guidance scale, which balances image quality and prompt alignment. However, the choice of the guidance scale has a critical impact on the convergence toward a visually appealing and prompt-adherent image. In this work, we propose an annealing guidance scheduler which dynamically adjusts the guidance scale over time based on the conditional noisy signal. By learning a scheduling policy, our method addresses the temperamental behavior of CFG. Empirical results demonstrate that our guidance scheduler significantly enhances image quality and alignment with the text prompt, advancing the performance of text-to-image generation. Notably, our novel scheduler requires no additional activations or memory consumption, and can seamlessly replace the common classifier-free guidance, offering an improved trade-off between prompt alignment and quality.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1125">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/3yYSFxZpM86UuLjX.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://arxiv.org/pdf/2510.02617" target="_blank" rel="noopener">Input-Aware Sparse Attention for Real-Time Co-Speech Video Generation</a></h3>
                    <p class="authors">Beijia Lu, Ziyi Chen, Jing Xiao, Jun-Yan Zhu</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://arxiv.org/pdf/2510.02617" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1125', 'url', 'https://arxiv.org/pdf/2510.02617')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1125', 'abstract', 'Diffusion models can synthesize realistic co-speech video from audio for various applications, such as video creation and virtual agents. However, existing diffusion-based methods are slow due to numerous denoising steps and costly attention mechanisms, preventing real-time deployment. In this work, we distill a many-step diffusion video model into a few-step student model. Unfortunately, directly applying recent diffusion distillation methods degrades video quality and falls short of real-time performance. To address these issues, our new video distillation method leverages input human pose conditioning for both attention and loss functions. We first propose using accurate correspondence between input human pose keypoints to guide attention to relevant regions, such as the speaker\'s face, hands, and upper body. This input-aware sparse attention reduces redundant computations and strengthens temporal correspondences of body parts, improving inference efficiency and motion coherence. To further enhance visual quality, we introduce an input-aware distillation loss that improves lip synchronization and hand motion realism. By integrating our input-aware sparse attention and distillation loss, our method achieves real-time performance with improved visual quality compared to recent audio-driven and input-driven methods. We also conduct extensive experiments showing the effectiveness of our algorithmic design choices.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">Diffusion models can synthesize realistic co-speech video from audio for various applications, such as video creation and virtual agents. However, existing diffusion-based methods are slow due to numerous denoising steps and costly attention mechanisms, preventing real-time deployment. In this work, we distill a many-step diffusion video model into a few-step student model. Unfortunately, directly applying recent diffusion distillation methods degrades video quality and falls short of real-time performance. To address these issues, our new video distillation method leverages input human pose conditioning for both attention and loss functions. We first propose using accurate correspondence between input human pose keypoints to guide attention to relevant regions, such as the speaker&#x27;s face, hands, and upper body. This input-aware sparse attention reduces redundant computations and strengthens temporal correspondences of body parts, improving inference efficiency and motion coherence. To further enhance visual quality, we introduce an input-aware distillation loss that improves lip synchronization and hand motion realism. By integrating our input-aware sparse attention and distillation loss, our method achieves real-time performance with improved visual quality compared to recent audio-driven and input-driven methods. We also conduct extensive experiments showing the effectiveness of our algorithmic design choices.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1753">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/cugD3FX17Ytw1M6F.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dvirsamuel.github.io/omnimattezero.github.io/" target="_blank" rel="noopener">OmnimatteZero: Fast Training-free Omnimatte with Pre-trained Video Diffusion Models</a></h3>
                    <p class="authors">Dvir Samuel, Matan Levy, Nir Darshan, Gal Chechik, Rami Ben-Ari</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dvirsamuel.github.io/omnimattezero.github.io/" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1753', 'url', 'https://dvirsamuel.github.io/omnimattezero.github.io/')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1753', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_2070">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/CtCqdyKENc8yzjV2.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3763350" target="_blank" rel="noopener">STGlight: Online Indoor Lighting Estimation via Spatio-Temporal Gaussian Fusion</a></h3>
                    <p class="authors">Shiyuan Shen, Zhongyun Bao, Hong Ding, Wenju Xu, Tenghui Lai, Chunxia Xiao</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3763350" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_2070', 'url', 'https://dl.acm.org/doi/10.1145/3763350')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_2070', 'abstract', 'Estimating lighting in indoor scenes is particularly challenging due to diverse distribution of light sources and complexity of scene geometry. Previous methods mainly focused on spatial variability and consistency for a single image or temporal consistency for video sequences. However, these approaches fail to achieve spatio-temporal consistency in video lighting estimation, which restricts applications such as compositing animated models into videos. In this paper, we propose STGlight, a lightweight and effective method for spatio-temporally consistent video lighting estimation, where our network processes a stream of LDR RGB-D video frames while maintaining incrementally updated global representations of both geometry and lighting, enabling the prediction of HDR environment maps at arbitrary locations for each frame. We model indoor lighting with three components: visible light sources providing direct illumination, ambient lighting approximating indirect illumination, and local environment textures producing high-quality specular reflections on glossy objects. To capture spatial-varying lighting, we represent scene geometry with point clouds, which support efficient spatio-temporal fusion and allow us to handle moderately dynamic scenes. To ensure temporal consistency, we apply a transformer-based fusion block that propagates lighting features across frames. Building on this, we further handle dynamic lighting with moving objects or changing light conditions by applying intrinsic decomposition on the point cloud and integrating the decomposed components with a neural fusion module. Experiments show that our online method can effectively predict lighting for any position within the video stream, while maintaining spatial variability and spatio-temporal consistency. Code is available at: https://github.com/nauyihsnehs/STGlight.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">Estimating lighting in indoor scenes is particularly challenging due to diverse distribution of light sources and complexity of scene geometry. Previous methods mainly focused on spatial variability and consistency for a single image or temporal consistency for video sequences. However, these approaches fail to achieve spatio-temporal consistency in video lighting estimation, which restricts applications such as compositing animated models into videos. In this paper, we propose STGlight, a lightweight and effective method for spatio-temporally consistent video lighting estimation, where our network processes a stream of LDR RGB-D video frames while maintaining incrementally updated global representations of both geometry and lighting, enabling the prediction of HDR environment maps at arbitrary locations for each frame. We model indoor lighting with three components: visible light sources providing direct illumination, ambient lighting approximating indirect illumination, and local environment textures producing high-quality specular reflections on glossy objects. To capture spatial-varying lighting, we represent scene geometry with point clouds, which support efficient spatio-temporal fusion and allow us to handle moderately dynamic scenes. To ensure temporal consistency, we apply a transformer-based fusion block that propagates lighting features across frames. Building on this, we further handle dynamic lighting with moving objects or changing light conditions by applying intrinsic decomposition on the point cloud and integrating the decomposed components with a neural fusion module. Experiments show that our online method can effectively predict lighting for any position within the video stream, while maintaining spatial variability and spatio-temporal consistency. Code is available at: https://github.com/nauyihsnehs/STGlight.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_2057">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/VoqfEij6PsDFtRg5.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://genlit.is.tue.mpg.de/" target="_blank" rel="noopener">GenLit: Reformulating Single-Image Relighting as Video Generation</a></h3>
                    <p class="authors">Shrisha Bharadwaj, Haiwen Feng, Giorgio Becherini, Victoria Fernandez Abrevaya, Michael J. Black</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://genlit.is.tue.mpg.de/" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_2057', 'url', 'https://genlit.is.tue.mpg.de/')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_2057', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_2542">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/qu5JLh9Fipbb2VZi.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dynvfx.github.io/" target="_blank" rel="noopener">DynVFX: Augmenting Real Videos with Dynamic Content</a></h3>
                    <p class="authors">Danah Yatim, Rafail Fridman, Omer Bar-Tal, Tali Dekel</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dynvfx.github.io/" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_2542', 'url', 'https://dynvfx.github.io/')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_2542', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            </div>
        </section>
        
        <section class="session">
            <div class="session-header">
                <h2>Global Illumination & Real-Time Rendering</h2>
                <span class="session-count">5 papers</span>
            </div>
            <div class="papers-grid">
                
            <article class="paper-card" data-paper-id="papers_1079">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/VQm5yGUcg9RciEUJ.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://arxiv.org/pdf/2508.07852" target="_blank" rel="noopener">Vertex Features for Neural Global Illumination</a></h3>
                    <p class="authors">Rui Su, Honghao Dong, Haojie Jin, Yisong Chen, Guoping Wang, Sheng Li</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://arxiv.org/pdf/2508.07852" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1079', 'url', 'https://arxiv.org/pdf/2508.07852')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1079', 'abstract', 'Recent research on learnable neural representations has been widely adopted in the field of 3D scene reconstruction and neural rendering applications. However, traditional feature grid representations often suffer from substantial memory footprint, posing a significant bottleneck for modern parallel computing hardware. In this paper, we present neural vertex features, a generalized formulation of learnable representation for neural rendering tasks involving explicit mesh surfaces. Instead of uniformly distributing neural features throughout 3D space, our method stores learnable features directly at mesh vertices, leveraging the underlying geometry as a compact and structured representation for neural processing. This not only optimizes memory efficiency, but also improves feature representation by aligning compactly with the surface using task-specific geometric priors. We validate our neural representation across diverse neural rendering tasks, with a specific emphasis on neural radiosity. Experimental results demonstrate that our method reduces memory consumption to only one-fifth (or even less) of grid-based representations, while maintaining comparable rendering quality and lowering inference overhead.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">Recent research on learnable neural representations has been widely adopted in the field of 3D scene reconstruction and neural rendering applications. However, traditional feature grid representations often suffer from substantial memory footprint, posing a significant bottleneck for modern parallel computing hardware. In this paper, we present neural vertex features, a generalized formulation of learnable representation for neural rendering tasks involving explicit mesh surfaces. Instead of uniformly distributing neural features throughout 3D space, our method stores learnable features directly at mesh vertices, leveraging the underlying geometry as a compact and structured representation for neural processing. This not only optimizes memory efficiency, but also improves feature representation by aligning compactly with the surface using task-specific geometric priors. We validate our neural representation across diverse neural rendering tasks, with a specific emphasis on neural radiosity. Experimental results demonstrate that our method reduces memory consumption to only one-fifth (or even less) of grid-based representations, while maintaining comparable rendering quality and lowering inference overhead.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1990">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/SyeFvdfXZNMG7keA.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/full/10.1145/3757377.3763958" target="_blank" rel="noopener">NeLiF: Neural Lighting Function Generation for Real-Time Indoor Rendering</a></h3>
                    <p class="authors">Hongtao Sheng, Yuchi Huo, Chuankun Zheng, Guangzhi Han, Bin Zang, Yifan Peng</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/full/10.1145/3757377.3763958" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1990', 'url', 'https://dl.acm.org/doi/full/10.1145/3757377.3763958')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1990', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1013">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/aEgGkgW2aXD5z5YR.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://cseweb.ucsd.edu/~ravir/zhengsiga.pdf" target="_blank" rel="noopener">ReSTIR PG: Path Guiding with Spatiotemporally Resampled Paths</a></h3>
                    <p class="authors">ZHENG ZENG, Markus Kettunen, Chris Wyman, Lifan Wu, Ravi Ramamoorthi, Ling-Qi Yan</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://cseweb.ucsd.edu/~ravir/zhengsiga.pdf" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1013', 'url', 'https://cseweb.ucsd.edu/~ravir/zhengsiga.pdf')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1013', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1809">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/UC1FnCHd3NjoFro6.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://research.nvidia.com/labs/rtr/publication/hong2025partition/hong2025partition_paper.pdf" target="_blank" rel="noopener">Sample Space Partitioning and Spatiotemporal Resampling for Specular Manifold Sampling</a></h3>
                    <p class="authors">Pengpei Hong, Meng Duan, Beibei Wang, Cem Yuksel, Tizian Zeltner, Daqi Lin</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://research.nvidia.com/labs/rtr/publication/hong2025partition/hong2025partition_paper.pdf" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1809', 'url', 'https://research.nvidia.com/labs/rtr/publication/hong2025partition/hong2025partition_paper.pdf')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1809', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1083">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/xcWApRSUc812nB4r.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/abs/10.1145/3763276" target="_blank" rel="noopener">Frame-Free Representation of Polarized Light for Resolving Stokes Vector Singularities</a></h3>
                    <p class="authors">Shinyoung Yi, Jiwoong Na, Seungmin Hwang, Inseung Hwang, Min H. Kim</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/abs/10.1145/3763276" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1083', 'url', 'https://dl.acm.org/doi/abs/10.1145/3763276')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1083', 'abstract', 'Stokes parameters are the standard representation of polarized light intensity in Mueller calculus and are widely used in polarization-aware computer graphics. However, their reliance on local frames-aligned with ray propagation directions-introduces a fundamental limitation: numerical discontinuities in Stokes vectors despite physically continuous fields of polarized light. This issue originates from the Hairy Ball Theorem, which guarantees unavoidable singularities in any frame-dependent function defined over spherical directional domains. In this paper, we overcome this long-standing challenge by introducing the first frame-free representation of Stokes vectors. Our key idea is to reinterpret a Stokes vector as a Dirac delta function over the directional domain and project it onto spin-2 spherical harmonics, retaining only the lowest-frequency coefficients. This compact representation supports coordinate-invariant interpolation and distance computation between Stokes vectors across varying ray directions-without relying on local frames. We demonstrate the advantages of our approach in two representative applications: spherical resampling of polarized environment maps (e.g., between cube map and equirectangular formats), and view synthesis from polarized radiance fields. In both cases, conventional frame-dependent methods produce singularity artifacts. In contrast, our frame-free representation eliminates these artifacts, improves numerical robustness, and simplifies implementation by decoupling polarization encoding from local frames.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">Stokes parameters are the standard representation of polarized light intensity in Mueller calculus and are widely used in polarization-aware computer graphics. However, their reliance on local frames-aligned with ray propagation directions-introduces a fundamental limitation: numerical discontinuities in Stokes vectors despite physically continuous fields of polarized light. This issue originates from the Hairy Ball Theorem, which guarantees unavoidable singularities in any frame-dependent function defined over spherical directional domains. In this paper, we overcome this long-standing challenge by introducing the first frame-free representation of Stokes vectors. Our key idea is to reinterpret a Stokes vector as a Dirac delta function over the directional domain and project it onto spin-2 spherical harmonics, retaining only the lowest-frequency coefficients. This compact representation supports coordinate-invariant interpolation and distance computation between Stokes vectors across varying ray directions-without relying on local frames. We demonstrate the advantages of our approach in two representative applications: spherical resampling of polarized environment maps (e.g., between cube map and equirectangular formats), and view synthesis from polarized radiance fields. In both cases, conventional frame-dependent methods produce singularity artifacts. In contrast, our frame-free representation eliminates these artifacts, improves numerical robustness, and simplifies implementation by decoupling polarization encoding from local frames.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            </div>
        </section>
        
        <section class="session">
            <div class="session-header">
                <h2>High-Performance Simulation Algorithms</h2>
                <span class="session-count">4 papers</span>
            </div>
            <div class="papers-grid">
                
            <article class="paper-card" data-paper-id="papers_1638">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/v3xANkmb9m4ZAXmx.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://cunchengzhu.github.io/project_pages/ViscousVortex2025.html" target="_blank" rel="noopener">Viscous Vortex Dynamics on Surfaces</a></h3>
                    <p class="authors">Cuncheng Zhu, Hang Yin, Albert Chern</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://cunchengzhu.github.io/project_pages/ViscousVortex2025.html" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1638', 'url', 'https://cunchengzhu.github.io/project_pages/ViscousVortex2025.html')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1638', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1733">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/sYMqM6u4xmuenSAP.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3763327" target="_blank" rel="noopener">Fast Galerkin Multigrid Method for Unstructured Meshes</a></h3>
                    <p class="authors">Jia-Ming Lu, Tailing Yuan, Zhe-Han Mo, Shi-Min Hu</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3763327" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1733', 'url', 'https://dl.acm.org/doi/10.1145/3763327')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1733', 'abstract', 'We present a novel multigrid solver framework that significantly advances the efficiency of physical simulation for unstructured meshes. While multi-grid methods theoretically offer linear scaling, their practical implementation for deformable body simulations faces substantial challenges, particularly on GPUs. Our framework achieves up to 6.9√ó speedup over traditional methods through an innovative combination of matrix-free vertex block Jacobi smoothing with a Full Approximation Scheme (FAS), enabling both piecewise constant and linear Galerkin formulations without the computational burden of dense coarse matrices. Our approach demonstrates superior performance across varying mesh resolutions and material stiffness values, maintaining consistent convergence even under extreme deformations and challenging initial configurations. Comprehensive evaluations against state-of-the-art methods confirm our approach achieves lower simulation error with reduced computational cost, enabling simulation of tetrahedral meshes with over one million vertices at approximately one frame per second on modern GPUs.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">We present a novel multigrid solver framework that significantly advances the efficiency of physical simulation for unstructured meshes. While multi-grid methods theoretically offer linear scaling, their practical implementation for deformable body simulations faces substantial challenges, particularly on GPUs. Our framework achieves up to 6.9√ó speedup over traditional methods through an innovative combination of matrix-free vertex block Jacobi smoothing with a Full Approximation Scheme (FAS), enabling both piecewise constant and linear Galerkin formulations without the computational burden of dense coarse matrices. Our approach demonstrates superior performance across varying mesh resolutions and material stiffness values, maintaining consistent convergence even under extreme deformations and challenging initial configurations. Comprehensive evaluations against state-of-the-art methods confirm our approach achieves lower simulation error with reduced computational cost, enabling simulation of tetrahedral meshes with over one million vertices at approximately one frame per second on modern GPUs.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_2069">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/KhYjZLJ4MRjThY15.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3763349" target="_blank" rel="noopener">A Stack-Free Parallel h-Adaptation Algorithm for Dynamically Balanced Trees on GPUs</a></h3>
                    <p class="authors">Lixin Ren, Xiaowei He, Shusen Liu, Yuzhong Guo, Enhua Wu</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3763349" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_2069', 'url', 'https://dl.acm.org/doi/10.1145/3763349')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_2069', 'abstract', 'Prior research has demonstrated the efficacy of balanced trees as spatially adaptive grids for large-scale simulations. However, state-of-the-art methods for balanced tree construction are restricted by the iterative nature of the ripple effect, thus failing to fully leverage the massive parallelism offered by modern GPU architectures. We propose to reframe the construction of balanced trees as a process to merge N -balanced Minimum Spanning Trees ( N -balanced MSTs) generated from a collection of seed points. To ensure optimal performance, we propose a stack-free parallel strategy for constructing all internal nodes of a specified N -balanced MST. This approach leverages two 32-bit integer registers as buffers rather than relying on an integer array as a stack during construction, which helps maintain balanced workloads across different GPU threads. We then propose a dynamic update algorithm utilizing refinement counters for all internal nodes to enable parallel insertion and deletion operations of N -balanced MSTs. This design achieves significant efficiency improvements compared to full reconstruction from scratch, thereby facilitating fluid simulations in handling dynamic moving boundaries. Our approach is fully compatible with GPU implementation and demonstrates up to an order-of-magnitude speedup compared to the state-of-the-art method [Wang et al. 2024]. The source code for the paper is publicly available at https://github.com/peridyno/peridyno.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">Prior research has demonstrated the efficacy of balanced trees as spatially adaptive grids for large-scale simulations. However, state-of-the-art methods for balanced tree construction are restricted by the iterative nature of the ripple effect, thus failing to fully leverage the massive parallelism offered by modern GPU architectures. We propose to reframe the construction of balanced trees as a process to merge N -balanced Minimum Spanning Trees ( N -balanced MSTs) generated from a collection of seed points. To ensure optimal performance, we propose a stack-free parallel strategy for constructing all internal nodes of a specified N -balanced MST. This approach leverages two 32-bit integer registers as buffers rather than relying on an integer array as a stack during construction, which helps maintain balanced workloads across different GPU threads. We then propose a dynamic update algorithm utilizing refinement counters for all internal nodes to enable parallel insertion and deletion operations of N -balanced MSTs. This design achieves significant efficiency improvements compared to full reconstruction from scratch, thereby facilitating fluid simulations in handling dynamic moving boundaries. Our approach is fully compatible with GPU implementation and demonstrates up to an order-of-magnitude speedup compared to the state-of-the-art method [Wang et al. 2024]. The source code for the paper is publicly available at https://github.com/peridyno/peridyno.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_2479">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/1Yebq719DHFczFhz.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3764005" target="_blank" rel="noopener">Implicit Position Based Fluids</a></h3>
                    <p class="authors">Elie Diaz, Jerry Hsu, Eisen Montalvo-Ruiz, Chris Giles, Cem Yuksel</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3764005" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_2479', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3764005')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_2479', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            </div>
        </section>
        
        <section class="session">
            <div class="session-header">
                <h2>Mesh Processing</h2>
                <span class="session-count">4 papers</span>
            </div>
            <div class="papers-grid">
                
            <article class="paper-card" data-paper-id="papers_1093">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/6MR4USZnEk3aa5UC.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://arxiv.org/pdf/2409.15458" target="_blank" rel="noopener">Simplifying Textured Triangle Meshes in the Wild</a></h3>
                    <p class="authors">Hsueh-Ti Derek Liu, Xiaoting Zhang, Cem Yuksel</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://arxiv.org/pdf/2409.15458" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1093', 'url', 'https://arxiv.org/pdf/2409.15458')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1093', 'abstract', 'This paper introduces a method for simplifying textured surface triangle meshes in the wild while maintaining high visual quality. While previous methods achieve excellent results on manifold meshes by using the quadric error metric, they struggle to produce high-quality outputs for meshes in the wild, which typically contain non-manifold elements and multiple connected components. In this work, we propose a method for simplifying these wild textured triangle meshes. We formulate mesh simplification as a problem of decimating simplicial 2-complexes to handle multiple non-manifold mesh components collectively. Building on the success of quadric error simplification, we iteratively collapse 1-simplices (vertex pairs). Our approach employs a modified quadric error that converges to the original quadric error metric for watertight manifold meshes, while significantly improving the results on wild meshes. For textures, instead of following existing strategies to preserve UVs, we adopt a novel perspective which focuses on computing mesh correspondences throughout the decimation, independent of the UV layout. This combination yields a textured mesh simplification system that is capable of handling arbitrary triangle meshes, achieving to high-quality results on wild inputs without sacrificing the excellent performance on clean inputs. Our method guarantees to avoid common problems in textured mesh simplification, including the prevalent problem of texture bleeding. We extensively evaluate our method on multiple datasets, showing improvements over prior techniques through qualitative, quantitative, and user study evaluations.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">This paper introduces a method for simplifying textured surface triangle meshes in the wild while maintaining high visual quality. While previous methods achieve excellent results on manifold meshes by using the quadric error metric, they struggle to produce high-quality outputs for meshes in the wild, which typically contain non-manifold elements and multiple connected components. In this work, we propose a method for simplifying these wild textured triangle meshes. We formulate mesh simplification as a problem of decimating simplicial 2-complexes to handle multiple non-manifold mesh components collectively. Building on the success of quadric error simplification, we iteratively collapse 1-simplices (vertex pairs). Our approach employs a modified quadric error that converges to the original quadric error metric for watertight manifold meshes, while significantly improving the results on wild meshes. For textures, instead of following existing strategies to preserve UVs, we adopt a novel perspective which focuses on computing mesh correspondences throughout the decimation, independent of the UV layout. This combination yields a textured mesh simplification system that is capable of handling arbitrary triangle meshes, achieving to high-quality results on wild inputs without sacrificing the excellent performance on clean inputs. Our method guarantees to avoid common problems in textured mesh simplification, including the prevalent problem of texture bleeding. We extensively evaluate our method on multiple datasets, showing improvements over prior techniques through qualitative, quantitative, and user study evaluations.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1207">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/fahRxqBNpJAgHPyw.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763843" target="_blank" rel="noopener">PartUV: Part-Based UV Unwrapping of 3D Meshes</a></h3>
                    <p class="authors">Zhaoning Wang, Xinyue Wei, Ruoxi Shi, Xiaoshuai Zhang, Hao Su, Minghua Liu</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763843" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1207', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763843')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1207', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1239">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/yKjPNeuzxdxv74Xh.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763847" target="_blank" rel="noopener">Solid-Shell Labeling for Discrete Surfaces</a></h3>
                    <p class="authors">Siqi Wang, Janos Meny, Izak Grguric, Mehdi Rahimzadeh, Denis Zorin, Daniele Panozzo</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763847" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1239', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763847')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1239', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_2322">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/mS2vYA6kJF6Pfgfn.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763994" target="_blank" rel="noopener">RibbonSculpt: Voronoi Ball based 3D Sculpting from Sparse VR Ribbons</a></h3>
                    <p class="authors">Anandhu SURESHKUMAR, Amal Dev PARAKKAT, Georges-Pierre BONNEAU, Stefanie HAHMANN, Marie-Paule CANI</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763994" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_2322', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763994')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_2322', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            </div>
        </section>
        
        <section class="session">
            <div class="session-header">
                <h2>Camera Control and Directed Storytelling in Video Generation</h2>
                <span class="session-count">6 papers</span>
            </div>
            <div class="papers-grid">
                
            <article class="paper-card" data-paper-id="papers_1022">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/pVPFXfH1T2ezec1r.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763816" target="_blank" rel="noopener">Shape-for-Motion: Precise and Consistent Video Editing With 3D Proxy</a></h3>
                    <p class="authors">Yuhao Liu, Tengfei Wang, Fang Liu, Zhenwei Wang, Rynson W.H. Lau</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763816" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1022', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763816')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1022', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1132">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/zXLMtd58LH4vxs5d.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763833" target="_blank" rel="noopener">Context as Memory: Scene-Consistent Interactive Long Video Generation with Memory Retrieval</a></h3>
                    <p class="authors">Jiwen Yu, Jianhong Bai, Yiran Qin, Quande Liu, Xintao Wang, Pengfei Wan</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763833" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1132', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763833')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1132', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1191">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/VMx4dxTTbWGvJi6Z.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763841" target="_blank" rel="noopener">CamCloneMaster: Enabling Reference-based Camera Control for Video Generation</a></h3>
                    <p class="authors">Yawen Luo, Xiaoyu Shi, Jianhong Bai, Menghan Xia, Tianfan Xue, Xintao Wang</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763841" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1191', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763841')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1191', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1200">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/u2Wyq9em1kQdL315.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763842" target="_blank" rel="noopener">Uni3C: Unifying Precisely 3D-Enhanced Camera and Human Motion Controls for Video Generation</a></h3>
                    <p class="authors">Chenjie Cao, Jingkai Zhou, Shikai Li, Jingyun Liang, Chaohui Yu, Fan Wang</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763842" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1200', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763842')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1200', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1610">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/wA2soics4dQyk6sV.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763896" target="_blank" rel="noopener">Cut2Next: Generating Next Shot via In-Context Tuning</a></h3>
                    <p class="authors">Jingwen He, Hongbo Liu, Jiajun Li, Ziqi Huang, Qiao Yu, Wanli Ouyang</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763896" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1610', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763896')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1610', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_2281">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/RX7YQfUBs7SXwDsK.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763990" target="_blank" rel="noopener">CamPVG: Camera-Controlled Panoramic Video Generation with Epipolar-Aware Diffusion</a></h3>
                    <p class="authors">Chenhao Ji, Chaohui Yu, Junyao Gao, Fan Wang, Cairong Zhao</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763990" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_2281', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763990')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_2281', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            </div>
        </section>
        
        <section class="session">
            <div class="session-header">
                <h2>Material & Texture Modeling</h2>
                <span class="session-count">6 papers</span>
            </div>
            <div class="papers-grid">
                
            <article class="paper-card" data-paper-id="papers_1270">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/iwLWJkoctU3ydqUc.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3763293" target="_blank" rel="noopener">Scattering-Aware Color Calibration for 3D Printers Using a Simple Calibration Target</a></h3>
                    <p class="authors">Tom√°≈° Iser, Tobias Rittig, Alexander Wilkie</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3763293" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1270', 'url', 'https://dl.acm.org/doi/10.1145/3763293')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1270', 'abstract', 'We present a novel method for accurately calibrating the optical properties of full-color 3D printers using only a single, directly printable calibration target. Our approach is based on accurate multiple-scattering light transport and estimates the single-scattering albedo and extinction coefficient for each resin. These parameters are essential for both soft-proof rendering of 3D printouts and for advanced, scattering-aware 3D halftoning algorithms. In contrast to previous methods that rely on thin, precisely fabricated resin samples and labor-intensive manual processing, our technique achieves higher accuracy with significantly less effort. Our calibration target is specifically designed to enable algorithmic recovery of each resin\'s optical properties through a series of one-dimensional and two-dimensional numerical optimizations, applied first on the white and black resins, and then on any remaining resins. The method supports both RGB and spectral calibration, depending on whether a camera or spectrometer is used to capture the calibration target. It also scales linearly with the number of resins, making it well-suited for modern multi-material printers. We validate our approach extensively, first on synthetic and then on real resins across 242 color mixtures, printed thin translucent samples, printed surface textures, and fully textured 3D models with complex geometry, including an eye model and a figurine.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">We present a novel method for accurately calibrating the optical properties of full-color 3D printers using only a single, directly printable calibration target. Our approach is based on accurate multiple-scattering light transport and estimates the single-scattering albedo and extinction coefficient for each resin. These parameters are essential for both soft-proof rendering of 3D printouts and for advanced, scattering-aware 3D halftoning algorithms. In contrast to previous methods that rely on thin, precisely fabricated resin samples and labor-intensive manual processing, our technique achieves higher accuracy with significantly less effort. Our calibration target is specifically designed to enable algorithmic recovery of each resin&#x27;s optical properties through a series of one-dimensional and two-dimensional numerical optimizations, applied first on the white and black resins, and then on any remaining resins. The method supports both RGB and spectral calibration, depending on whether a camera or spectrometer is used to capture the calibration target. It also scales linearly with the number of resins, making it well-suited for modern multi-material printers. We validate our approach extensively, first on synthetic and then on real resins across 242 color mixtures, printed thin translucent samples, printed surface textures, and fully textured 3D models with complex geometry, including an eye model and a figurine.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1415">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/aodG9ERNTJc22jm6.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3763301" target="_blank" rel="noopener">Example-Based Feature Painting on Textures</a></h3>
                    <p class="authors">Andrei-Timotei Ardelean, Tim Weyrich</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3763301" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1415', 'url', 'https://dl.acm.org/doi/10.1145/3763301')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1415', 'abstract', 'In this work, we propose a system that covers the complete workflow for achieving controlled authoring and editing of textures that present distinctive local characteristics. These include various effects that change the surface appearance of materials, such as stains, tears, holes, abrasions, discoloration, and more. Such alterations are ubiquitous in nature, and including them in the synthesis process is crucial for generating realistic textures. We introduce a novel approach for creating textures with such blemishes, adopting a learning-based approach that leverages unlabeled examples. Our approach does not require manual annotations by the user; instead, it detects the appearance-altering features through unsupervised anomaly detection. The various textural features are then automatically clustered into semantically coherent groups, which are used to guide the conditional generation of images. Our pipeline as a whole goes from a small image collection to a versatile generative model that enables the user to interactively create and paint features on textures of arbitrary size. Notably, the algorithms we introduce for diffusion-based editing and infinite stationary texture generation are generic and should prove useful in other contexts as well. Project page: reality.tf.fau.de/pub/ardelean2025examplebased.html')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">In this work, we propose a system that covers the complete workflow for achieving controlled authoring and editing of textures that present distinctive local characteristics. These include various effects that change the surface appearance of materials, such as stains, tears, holes, abrasions, discoloration, and more. Such alterations are ubiquitous in nature, and including them in the synthesis process is crucial for generating realistic textures. We introduce a novel approach for creating textures with such blemishes, adopting a learning-based approach that leverages unlabeled examples. Our approach does not require manual annotations by the user; instead, it detects the appearance-altering features through unsupervised anomaly detection. The various textural features are then automatically clustered into semantically coherent groups, which are used to guide the conditional generation of images. Our pipeline as a whole goes from a small image collection to a versatile generative model that enables the user to interactively create and paint features on textures of arbitrary size. Notably, the algorithms we introduce for diffusion-based editing and infinite stationary texture generation are generic and should prove useful in other contexts as well. Project page: reality.tf.fau.de/pub/ardelean2025examplebased.html</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1568">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/Y3QTCRZLePb5zU14.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3763312" target="_blank" rel="noopener">DiffTex: Differentiable Texturing for Architectural Proxy Models</a></h3>
                    <p class="authors">Weidan Xiong, Yongli Wu, Bochuan Zeng, Jianwei Guo, Dani Lischinski, Daniel Cohen-Or</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3763312" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1568', 'url', 'https://dl.acm.org/doi/10.1145/3763312')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1568', 'abstract', 'Simplified proxy models are commonly used to represent architectural structures, reducing storage requirements and enabling real-time rendering. However, the geometric simplifications inherent in proxies result in a loss of fine color and geometric details, making it essential for textures to compensate for the loss. Preserving the rich texture information from the original dense architectural reconstructions remains a daunting task, particularly when working with unordered RGB photographs. We propose an automated method for generating realistic texture maps for architectural proxy models at the texel level from an unordered collection of registered photographs. Our approach establishes correspondences between texels on a UV map and pixels in the input images, with each texel\'s color computed as a weighted blend of associated pixel values. Using differentiable rendering, we optimize blending parameters to ensure photometric and perspective consistency, while maintaining seamless texture coherence. Experimental results demonstrate the effectiveness and robustness of our method across diverse architectural models and varying photographic conditions, enabling the creation of high-quality textures that preserve visual fidelity and structural detail.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">Simplified proxy models are commonly used to represent architectural structures, reducing storage requirements and enabling real-time rendering. However, the geometric simplifications inherent in proxies result in a loss of fine color and geometric details, making it essential for textures to compensate for the loss. Preserving the rich texture information from the original dense architectural reconstructions remains a daunting task, particularly when working with unordered RGB photographs. We propose an automated method for generating realistic texture maps for architectural proxy models at the texel level from an unordered collection of registered photographs. Our approach establishes correspondences between texels on a UV map and pixels in the input images, with each texel&#x27;s color computed as a weighted blend of associated pixel values. Using differentiable rendering, we optimize blending parameters to ensure photometric and perspective consistency, while maintaining seamless texture coherence. Experimental results demonstrate the effectiveness and robustness of our method across diverse architectural models and varying photographic conditions, enabling the creation of high-quality textures that preserve visual fidelity and structural detail.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1768">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/Fk9RdZHrQ5EKsWgu.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3763332" target="_blank" rel="noopener">Fine-Grained Spatially Varying Material Selection in Images</a></h3>
                    <p class="authors">Julia Guerrero-Viu, Michael Fischer, Iliyan Georgiev, Elena Garces, Diego Gutierrez, Belen Masia</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3763332" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1768', 'url', 'https://dl.acm.org/doi/10.1145/3763332')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1768', 'abstract', 'Selection is the first step in many image editing processes, enabling faster and simpler modifications of all pixels sharing a common modality. In this work, we present a method for material selection in images, robust to lighting and reflectance variations, which can be used for downstream editing tasks. We rely on vision transformer (ViT) models and leverage their features for selection, proposing a multi-resolution processing strategy that yields finer and more stable selection results than prior methods. Furthermore, we enable selection at two levels: texture and subtexture, leveraging a new two-level material selection (DuMaS) dataset which includes dense annotations for over 800,000 synthetic images, both on the texture and subtexture levels.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">Selection is the first step in many image editing processes, enabling faster and simpler modifications of all pixels sharing a common modality. In this work, we present a method for material selection in images, robust to lighting and reflectance variations, which can be used for downstream editing tasks. We rely on vision transformer (ViT) models and leverage their features for selection, proposing a multi-resolution processing strategy that yields finer and more stable selection results than prior methods. Furthermore, we enable selection at two levels: texture and subtexture, leveraging a new two-level material selection (DuMaS) dataset which includes dense annotations for over 800,000 synthetic images, both on the texture and subtexture levels.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1810">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/YE7czLAao3BzXasV.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3>S3 Imagery: Specular Shading from Scratch-Anisotropy</h3>
                    <p class="authors">Pengfei Shen, Feifan Qu, Li Liao, Ruizhen Hu, Yifan Peng</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <button class="edit-btn" onclick="openEditModal('papers_1810', 'url', '')" title="Add URL">‚úèÔ∏è Add link</button>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1810', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1364">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/2tPYySJE7mxdcT8M.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763863" target="_blank" rel="noopener">SeqTex: Generate Mesh Textures in Video Sequence</a></h3>
                    <p class="authors">Ze Yuan, Xin Yu, Yangtian Sun, Yuan-Chen Guo, Yan-Pei Cao, Ding Liang</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763863" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1364', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763863')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1364', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            </div>
        </section>
        
        <section class="session">
            <div class="session-header">
                <h2>Neural & Implicit Representations for Geometry and Physics</h2>
                <span class="session-count">6 papers</span>
            </div>
            <div class="papers-grid">
                
            <article class="paper-card" data-paper-id="papers_1003">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/CbRh6aCfKenHQwHD.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763810" target="_blank" rel="noopener">Precise Gradient Discontinuities in Neural Fields for Subspace Physics</a></h3>
                    <p class="authors">Mengfei Liu, Yue Chang, Zhecheng Wang, Peter Yichen Chen, Eitan Grinspun</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763810" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1003', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763810')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1003', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1641">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/k5tJn3caFFQqNJe2.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763900" target="_blank" rel="noopener">Variational Neural Surfacing of 3D Sketches</a></h3>
                    <p class="authors">Yutao Zhang, Stephanie Wang, Mikhail Bessmeltsev</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763900" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1641', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763900')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1641', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1755">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/UKRgaLGc1g8aS7qz.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3763331" target="_blank" rel="noopener">NeuVAS: Neural Implicit Surfaces for Variational Shape Modeling</a></h3>
                    <p class="authors">Pengfei Wang, Qiujie Dong, Fangtian Liang, Hao Pan, Lei Yang, Congyi Zhang</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3763331" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1755', 'url', 'https://dl.acm.org/doi/10.1145/3763331')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1755', 'abstract', 'Neural implicit shape representation has drawn significant attention in recent years due to its smoothness, differentiability, and topological flexibility. However, directly modeling the shape of a neural implicit surface, especially as the zero-level set of a neural signed distance function (SDF), with sparse geometric control is still a challenging task. Sparse input shape control typically includes 3D curve networks or, more generally, 3D curve sketches, which are unstructured and cannot be connected to form a curve network, and therefore more difficult to deal with. While 3D curve networks or curve sketches provide intuitive shape control, their sparsity and varied topology pose challenges in generating high-quality surfaces to meet such curve constraints. In this paper, we propose NeuVAS, a variational approach to shape modeling using neural implicit surfaces constrained under sparse input shape control, including unstructured 3D curve sketches as well as connected 3D curve networks. Specifically, we introduce a smoothness term based on a functional of surface curvatures to minimize shape variation of the zero-level set surface of a neural SDF. We also develop a new technique to faithfully model G 0 sharp feature curves as specified in the input curve sketches. Comprehensive comparisons with the state-of-the-art methods demonstrate the significant advantages of our method.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">Neural implicit shape representation has drawn significant attention in recent years due to its smoothness, differentiability, and topological flexibility. However, directly modeling the shape of a neural implicit surface, especially as the zero-level set of a neural signed distance function (SDF), with sparse geometric control is still a challenging task. Sparse input shape control typically includes 3D curve networks or, more generally, 3D curve sketches, which are unstructured and cannot be connected to form a curve network, and therefore more difficult to deal with. While 3D curve networks or curve sketches provide intuitive shape control, their sparsity and varied topology pose challenges in generating high-quality surfaces to meet such curve constraints. In this paper, we propose NeuVAS, a variational approach to shape modeling using neural implicit surfaces constrained under sparse input shape control, including unstructured 3D curve sketches as well as connected 3D curve networks. Specifically, we introduce a smoothness term based on a functional of surface curvatures to minimize shape variation of the zero-level set surface of a neural SDF. We also develop a new technique to faithfully model G 0 sharp feature curves as specified in the input curve sketches. Comprehensive comparisons with the state-of-the-art methods demonstrate the significant advantages of our method.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1795">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/iQz6YqZi4zchKDUi.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763925" target="_blank" rel="noopener">Neural Kinematic Bases for Fluids</a></h3>
                    <p class="authors">Yibo Liu, Zhixin Fang, Sune Darkner, Noam Aigerman, Kenny Erleben, Paul Kry</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763925" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1795', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763925')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1795', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_2228">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/tmJAr8ZfSwtdD2uK.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3763362" target="_blank" rel="noopener">Neural Octahedral Field: Octahedral Prior for Simultaneous Smoothing and Sharp Edge Regularization</a></h3>
                    <p class="authors">Ruichen Zheng, Tao Yu, Ruizhen Hu</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3763362" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_2228', 'url', 'https://dl.acm.org/doi/10.1145/3763362')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_2228', 'abstract', 'Neural implicit representation, the parameterization of a continuous distance function as a Multi-Layer Perceptron (MLP), has emerged as a promising lead in tackling surface reconstruction from unoriented point clouds. In the presence of noise, however, its lack of explicit neighborhood connectivity makes sharp edges identification particularly challenging, hence preventing the separation of smoothing and sharpening operations, as is achievable with its discrete counterparts. In this work, we propose to tackle this challenge with an auxiliary field, the octahedral field. We observe that both smoothness and sharp features in the distance field can be equivalently described by the smoothness in octahedral space. Therefore, by aligning and smoothing an octahedral field alongside the implicit geometry, our method behaves analogously to bilateral filtering, resulting in a smooth reconstruction while preserving sharp edges. Despite being operated purely pointwise, our method outperforms various traditional and neural implicit fitting approaches across extensive experiments, and is very competitive with methods that require normals and data priors. Code and data of our work are available at: https://github.com/Ankbzpx/frame-field.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">Neural implicit representation, the parameterization of a continuous distance function as a Multi-Layer Perceptron (MLP), has emerged as a promising lead in tackling surface reconstruction from unoriented point clouds. In the presence of noise, however, its lack of explicit neighborhood connectivity makes sharp edges identification particularly challenging, hence preventing the separation of smoothing and sharpening operations, as is achievable with its discrete counterparts. In this work, we propose to tackle this challenge with an auxiliary field, the octahedral field. We observe that both smoothness and sharp features in the distance field can be equivalently described by the smoothness in octahedral space. Therefore, by aligning and smoothing an octahedral field alongside the implicit geometry, our method behaves analogously to bilateral filtering, resulting in a smooth reconstruction while preserving sharp edges. Despite being operated purely pointwise, our method outperforms various traditional and neural implicit fitting approaches across extensive experiments, and is very competitive with methods that require normals and data priors. Code and data of our work are available at: https://github.com/Ankbzpx/frame-field.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1744">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/2C8DDPvpfu1xRKrP.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3763329" target="_blank" rel="noopener">Practical Gaussian Process Implicit Surfaces with Sparse Convolutions</a></h3>
                    <p class="authors">Kehan Xu, Benedikt Bitterli, Eugene d&#x27;Eon, Wojciech Jarosz</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3763329" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1744', 'url', 'https://dl.acm.org/doi/10.1145/3763329')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1744', 'abstract', 'A fundamental challenge in rendering has been the dichotomy between surface and volume models. Gaussian Process Implicit Surfaces (GPISes) recently provided a unified approach for surfaces, volumes, and the spectrum in between. However, this representation remains impractical due to its high computational cost and mathematical complexity. We address these limitations by reformulating GPISes as procedural noise, eliminating expensive linear system solves while maintaining control over spatial correlations. Our method enables efficient sampling of stochastic realizations and supports flexible conditioning of values and derivatives through pathwise updates. To further enable practical rendering, we derive analytic distributions for surface normals, allowing for variance-reduced light transport via next-event estimation and multiple importance sampling. Our framework achieves efficient, high-quality rendering of stochastic surfaces and volumes with significantly simplified implementations on both CPU and GPU, while preserving the generality of the original GPIS representation.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">A fundamental challenge in rendering has been the dichotomy between surface and volume models. Gaussian Process Implicit Surfaces (GPISes) recently provided a unified approach for surfaces, volumes, and the spectrum in between. However, this representation remains impractical due to its high computational cost and mathematical complexity. We address these limitations by reformulating GPISes as procedural noise, eliminating expensive linear system solves while maintaining control over spatial correlations. Our method enables efficient sampling of stochastic realizations and supports flexible conditioning of values and derivatives through pathwise updates. To further enable practical rendering, we derive analytic distributions for surface normals, allowing for variance-reduced light transport via next-event estimation and multiple importance sampling. Our framework achieves efficient, high-quality rendering of stochastic surfaces and volumes with significantly simplified implementations on both CPU and GPU, while preserving the generality of the original GPIS representation.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            </div>
        </section>
        
        <section class="session">
            <div class="session-header">
                <h2>Creating Digital Humans</h2>
                <span class="session-count">6 papers</span>
            </div>
            <div class="papers-grid">
                
            <article class="paper-card" data-paper-id="papers_1374">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/1S4eu3ydep2W4JmF.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3763300" target="_blank" rel="noopener">Generative Head-Mounted Camera Captures for Photorealistic Avatars</a></h3>
                    <p class="authors">Shaojie Bai, Seunghyeon Seo, Yida Wang, Chenghui Li, Owen Wang, Te-Li Wang</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3763300" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1374', 'url', 'https://dl.acm.org/doi/10.1145/3763300')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1374', 'abstract', 'Enabling photorealistic avatar animations in virtual and augmented reality (VR/AR) has been challenging because of the difficulty of obtaining ground truth state of faces. It is physically impossible to obtain synchronized images from head-mounted cameras (HMC) sensing input, which has partial observations in infrared (IR), and an array of outside-in dome cameras, which have full observations that match avatars\' appearance. Prior works relying on analysis-by-synthesis methods could generate accurate ground truth, but suffer from imperfect disentanglement between expression and style in their personalized training. The reliance of extensive paired captures (HMC and dome) for the same subject makes it operationally expensive to collect large-scale datasets, which cannot be reused for different HMC viewpoints and lighting. In this work, we propose a novel generative approach, Generative HMC (GenHMC), that leverages large unpaired HMC captures , which are much easier to collect, to directly generate high-quality synthetic HMC images given any conditioning avatar state from dome captures. We show that our method is able to properly disentangle the input conditioning signal that specifies facial expression and viewpoint, from facial appearance, leading to more accurate ground truth. Furthermore, our method can generalize to unseen identities, removing the reliance on the paired captures. We demonstrate these breakthroughs by both evaluating synthetic HMC images and universal face encoders trained from these new HMC-avatar correspondences, which achieve better data efficiency and state-of-the-art accuracy.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">Enabling photorealistic avatar animations in virtual and augmented reality (VR/AR) has been challenging because of the difficulty of obtaining ground truth state of faces. It is physically impossible to obtain synchronized images from head-mounted cameras (HMC) sensing input, which has partial observations in infrared (IR), and an array of outside-in dome cameras, which have full observations that match avatars&#x27; appearance. Prior works relying on analysis-by-synthesis methods could generate accurate ground truth, but suffer from imperfect disentanglement between expression and style in their personalized training. The reliance of extensive paired captures (HMC and dome) for the same subject makes it operationally expensive to collect large-scale datasets, which cannot be reused for different HMC viewpoints and lighting. In this work, we propose a novel generative approach, Generative HMC (GenHMC), that leverages large unpaired HMC captures , which are much easier to collect, to directly generate high-quality synthetic HMC images given any conditioning avatar state from dome captures. We show that our method is able to properly disentangle the input conditioning signal that specifies facial expression and viewpoint, from facial appearance, leading to more accurate ground truth. Furthermore, our method can generalize to unseen identities, removing the reliance on the paired captures. We demonstrate these breakthroughs by both evaluating synthetic HMC images and universal face encoders trained from these new HMC-avatar correspondences, which achieve better data efficiency and state-of-the-art accuracy.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1018">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/wMdDkEWwbmqTZoPE.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763815" target="_blank" rel="noopener">InfiniHuman: Realistic 3D Human Creation with Precise Control</a></h3>
                    <p class="authors">Yuxuan Xue, Xianghui Xie, Margaret Kostyrko, Gerard Pons-Moll</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763815" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1018', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763815')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1018', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1586">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/QD8WtbAa1AUHo6Qr.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763894" target="_blank" rel="noopener">HRM^2Avatar: High-Fidelity Real-Time Mobile Avatars from Monocular Phone Scans</a></h3>
                    <p class="authors">Chao Shi, Shenghao Jia, Jinhui Liu, Yong Zhang, Liangchao Zhu, Zhonglei Yang</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763894" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1586', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763894')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1586', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_2119">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/9eXWpL8CMEkrjUNQ.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763978" target="_blank" rel="noopener">PriorAvatar: Efficient and Robust Avatar Creation from Monocular Video Using Learned Priors</a></h3>
                    <p class="authors">Tianjian Jiang, Hsuan-I Ho, Manuel Kaufmann, Jie Song</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763978" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_2119', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763978')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_2119', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1965">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/A6xC43L4sMfaWoxQ.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763953" target="_blank" rel="noopener">Constructing Diffusion Avatar with Learnable Embeddings</a></h3>
                    <p class="authors">Xuan Gao, Jingtao Zhou, Dongyu Liu, Yuqi Zhou, Juyong Zhang</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763953" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1965', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763953')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1965', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1178">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/6c39VySDGAAiDtDx.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763839" target="_blank" rel="noopener">HumanLift: Single-Image 3D Human Reconstruction with 3D-Aware Diffusion Priors and Facial Enhancement</a></h3>
                    <p class="authors">Jie Yang, Bo-Tao Zhang, Feng-Lin Liu, Hongbo Fu, Yu-Kun Lai, Lin Gao</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763839" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1178', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763839')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1178', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            </div>
        </section>
        
        <section class="session">
            <div class="session-header">
                <h2>Smart Process Planning for Manufacturing</h2>
                <span class="session-count">6 papers</span>
            </div>
            <div class="papers-grid">
                
            <article class="paper-card" data-paper-id="papers_1570">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/xr2VjxjpzGpiJqmb.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3763313" target="_blank" rel="noopener">Waste-to-Value: Reutilized Material Maximization for Additive and Subtractive Hybrid Remanufacturing</a></h3>
                    <p class="authors">Fanchao Zhong, Zhenmin Zhang, Liyuan Wang, Xin Yan, Jikai Liu, Lin Lu</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3763313" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1570', 'url', 'https://dl.acm.org/doi/10.1145/3763313')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1570', 'abstract', 'Remanufacturing effectively extends component lifespans by restoring used or end-of-life parts to like-new or even superior conditions, with an emphasis on maximizing reutilized material, especially for high-cost materials. Hybrid manufacturing technology combines the capabilities of additive and subtractive manufacturing, with the ability to add and remove material, enabling it to remanufacture complex shapes and is increasingly being applied in remanufacturing. How to effectively plan the process of additive and subtractive hybrid remanufacturing (ASHRM) to maximize material reutilization has become a key focus of attention. However, current ASHRM process planning methods lack strict consideration of collision-free constraints, hindering practical application. This paper introduces a computational framework to tackle ASHRM process planning for general shapes with strictly considering these constraints. We separate global and local collision-free constraints, employing clipping planes and graph to tackle them respectively, ultimately maximizing the reutilized volume while ensuring these constraints are satisfied. Additionally, we also optimize the setup of the target model that is conducive to maximizing the reutilized volume. Extensive experiments and physical validations on a 5-axis hybrid manufacturing platform demonstrate the effectiveness of our method across various 3D shapes, achieving an average material reutilization of 69% across 12 cases. Code is publicly available at https://github.com/fanchao98/Waste-to-Value.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">Remanufacturing effectively extends component lifespans by restoring used or end-of-life parts to like-new or even superior conditions, with an emphasis on maximizing reutilized material, especially for high-cost materials. Hybrid manufacturing technology combines the capabilities of additive and subtractive manufacturing, with the ability to add and remove material, enabling it to remanufacture complex shapes and is increasingly being applied in remanufacturing. How to effectively plan the process of additive and subtractive hybrid remanufacturing (ASHRM) to maximize material reutilization has become a key focus of attention. However, current ASHRM process planning methods lack strict consideration of collision-free constraints, hindering practical application. This paper introduces a computational framework to tackle ASHRM process planning for general shapes with strictly considering these constraints. We separate global and local collision-free constraints, employing clipping planes and graph to tackle them respectively, ultimately maximizing the reutilized volume while ensuring these constraints are satisfied. Additionally, we also optimize the setup of the target model that is conducive to maximizing the reutilized volume. Extensive experiments and physical validations on a 5-axis hybrid manufacturing platform demonstrate the effectiveness of our method across various 3D shapes, achieving an average material reutilization of 69% across 12 cases. Code is publicly available at https://github.com/fanchao98/Waste-to-Value.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_2154">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/R8Tr7eJuNGrjf829.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3763355" target="_blank" rel="noopener">Can Any Model Be Fabricated? Inverse Operation Based Planning for Hybrid Additive‚ÄìSubtractive Manufacturing</a></h3>
                    <p class="authors">Yongxue Chen, Tao Liu, Yuming Huang, Weiming Wang, Tianyu Zhang, Kun Qian</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3763355" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_2154', 'url', 'https://dl.acm.org/doi/10.1145/3763355')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_2154', 'abstract', 'This paper presents a method for computing interleaved additive and subtractive manufacturing operations to fabricate models of arbitrary shapes. We solve the manufacturing planning problem by searching a sequence of inverse operations that progressively transform a target model into a null shape. Each inverse operation corresponds to either an additive or a subtractive step, ensuring both manufacturability and structural stability of intermediate shapes throughout the process. We theoretically prove that any model can be fabricated exactly using a sequence generated by our approach. To demonstrate the effectiveness of this method, we adopt a voxel-based implementation and develop a scalable algorithm that works on models represented by a large number of voxels. Our approach has been tested across a range of digital models and further validated through physical fabrication on a hybrid manufacturing system with automatic tool switching.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">This paper presents a method for computing interleaved additive and subtractive manufacturing operations to fabricate models of arbitrary shapes. We solve the manufacturing planning problem by searching a sequence of inverse operations that progressively transform a target model into a null shape. Each inverse operation corresponds to either an additive or a subtractive step, ensuring both manufacturability and structural stability of intermediate shapes throughout the process. We theoretically prove that any model can be fabricated exactly using a sequence generated by our approach. To demonstrate the effectiveness of this method, we adopt a voxel-based implementation and develop a scalable algorithm that works on models represented by a large number of voxels. Our approach has been tested across a range of digital models and further validated through physical fabrication on a hybrid manufacturing system with automatic tool switching.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1590">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/1LwgPoyhK9ahGWQd.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3763314" target="_blank" rel="noopener">Chapper: Carvable Hull-and-Pack for Subtractive Manufacturing</a></h3>
                    <p class="authors">Zhenmin Zhang, Shuai Feng, Hao Xu, Lujiaoyang Fu, Lin Lu, Jianwei Guo</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3763314" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1590', 'url', 'https://dl.acm.org/doi/10.1145/3763314')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1590', 'abstract', 'Tightly cutting raw materials into a set of carvable objects, known as the stock cutting problem, is a necessary step in subtractive manufacturing. This problem can be framed as a 3D irregular object packing task, aiming to fit as many objects as possible within a predefined container. While previous packing algorithms can generate dense, non-overlapping, and even disassemblable configurations, they cannot satisfy carvable constraints. This paper introduces the carvable hull-and-pack problem, which integrates irregular object packing with subtractive manufacturing. This problem is more challenging than general 3D packing, as it requires ensuring the carvability of each object and generate the disassembly sequence. To address this, we first define a novel geometric hull, called carving hull , which accounts for both the object\'s shape and the cutter accessibility, constrained by the real-time distribution of surrounding objects. Then we present Chapper , an effective solution to co-optimize carving hull packing and the planning of disassembly sequence to maximize space utilization while preserving the carvable constraints. Given a raw material and a list of generic 3D objects, our algorithm starts with densely packing each object into the material with a pre-computed placement order, while simultaneously maintaining a valid disassembly sequence. We solve the complex object-to-object and cutter-to-object collisions by leveraging a discrete voxel representation. The carvability of each object is also guaranteed in the packing process, where we define a novel carvable metric to determine whether each object is carvable or not. Based on the packing result and the disassembly sequence, we propose a clipped Voronoi-based volume decomposition method to generate the actual carving hull for each object and finally create feasible cutting tool paths on the carving hulls. Our approach effectively packs CAD and freeform datasets, exhibiting a unique space utilization rate performance compared to the alternative baseline.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">Tightly cutting raw materials into a set of carvable objects, known as the stock cutting problem, is a necessary step in subtractive manufacturing. This problem can be framed as a 3D irregular object packing task, aiming to fit as many objects as possible within a predefined container. While previous packing algorithms can generate dense, non-overlapping, and even disassemblable configurations, they cannot satisfy carvable constraints. This paper introduces the carvable hull-and-pack problem, which integrates irregular object packing with subtractive manufacturing. This problem is more challenging than general 3D packing, as it requires ensuring the carvability of each object and generate the disassembly sequence. To address this, we first define a novel geometric hull, called carving hull , which accounts for both the object&#x27;s shape and the cutter accessibility, constrained by the real-time distribution of surrounding objects. Then we present Chapper , an effective solution to co-optimize carving hull packing and the planning of disassembly sequence to maximize space utilization while preserving the carvable constraints. Given a raw material and a list of generic 3D objects, our algorithm starts with densely packing each object into the material with a pre-computed placement order, while simultaneously maintaining a valid disassembly sequence. We solve the complex object-to-object and cutter-to-object collisions by leveraging a discrete voxel representation. The carvability of each object is also guaranteed in the packing process, where we define a novel carvable metric to determine whether each object is carvable or not. Based on the packing result and the disassembly sequence, we propose a clipped Voronoi-based volume decomposition method to generate the actual carving hull for each object and finally create feasible cutting tool paths on the carving hulls. Our approach effectively packs CAD and freeform datasets, exhibiting a unique space utilization rate performance compared to the alternative baseline.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1426">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/Dp1KA2KZEzteLzre.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3763304" target="_blank" rel="noopener">MiGumi: Making Tightly Coupled Integral Joints Millable</a></h3>
                    <p class="authors">Aditya Ganeshan, Kurt Fleischer, Wenzel Jakob, Ariel Shamir, Daniel Ritchie, Takeo Igarashi</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3763304" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1426', 'url', 'https://dl.acm.org/doi/10.1145/3763304')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1426', 'abstract', 'Traditional integral wood joints, despite their strength, durability, and elegance, remain rare in modern workflows due to the cost and difficulty of manual fabrication. CNC milling offers a scalable alternative, but directly milling traditional joints often fails to produce functional results because milling induces geometric deviations‚Äîsuch as rounded inner corners‚Äîthat alter the target geometries of the parts. Since joints rely on tightly fitting surfaces, such deviations introduce gaps or overlaps that undermine fit or block assembly. We propose to overcome this problem by (1) designing a language that represent millable geometry, and (2) co-optimizing part geometries to restore coupling. We introduce Millable Extrusion Geometry (MXG), a language for representing geometry as the outcome of milling operations performed with flat-end drill bits. MXG represents each operation as a subtractive extrusion volume defined by a tool direction and drill radius. This parameterization enables the modeling of artifact-free geometry under an idealized zero-radius drill bit, matching traditional joint designs. Increasing the radius then reveals milling-induced deviations, which compromise the integrity of the joint. To restore coupling, we formalize tight coupling in terms of both surface proximity and proximity constraints on the mill-bit paths associated with mating surfaces. We then derive two tractable, differentiable losses that enable efficient optimization of joint geometry. We evaluate our method on 30 traditional joint designs, demonstrating that it produces CNC-compatible, tightly fitting joints that approximates the original geometry. By reinterpreting traditional joints for CNC workflows, we continue the evolution of this heritage craft and help ensure its relevance in future making practices.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">Traditional integral wood joints, despite their strength, durability, and elegance, remain rare in modern workflows due to the cost and difficulty of manual fabrication. CNC milling offers a scalable alternative, but directly milling traditional joints often fails to produce functional results because milling induces geometric deviations‚Äîsuch as rounded inner corners‚Äîthat alter the target geometries of the parts. Since joints rely on tightly fitting surfaces, such deviations introduce gaps or overlaps that undermine fit or block assembly. We propose to overcome this problem by (1) designing a language that represent millable geometry, and (2) co-optimizing part geometries to restore coupling. We introduce Millable Extrusion Geometry (MXG), a language for representing geometry as the outcome of milling operations performed with flat-end drill bits. MXG represents each operation as a subtractive extrusion volume defined by a tool direction and drill radius. This parameterization enables the modeling of artifact-free geometry under an idealized zero-radius drill bit, matching traditional joint designs. Increasing the radius then reveals milling-induced deviations, which compromise the integrity of the joint. To restore coupling, we formalize tight coupling in terms of both surface proximity and proximity constraints on the mill-bit paths associated with mating surfaces. We then derive two tractable, differentiable losses that enable efficient optimization of joint geometry. We evaluate our method on 30 traditional joint designs, demonstrating that it produces CNC-compatible, tightly fitting joints that approximates the original geometry. By reinterpreting traditional joints for CNC workflows, we continue the evolution of this heritage craft and help ensure its relevance in future making practices.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_2095">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/KK6gbxt2Q8Gmm2et.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3763352" target="_blank" rel="noopener">Curve-Based Slicer for Multi-Axis DLP 3D Printing</a></h3>
                    <p class="authors">Chengkai Dai, Liu Tao, Dezhao Guo, Binzhi Sun, Guoxin Fang, Yeung Yam</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3763352" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_2095', 'url', 'https://dl.acm.org/doi/10.1145/3763352')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_2095', 'abstract', 'This paper introduces a novel curve-based slicing method for generating planar layers with dynamically varying orientations in digital light processing (DLP) 3D printing. Our approach effectively addresses key challenges in DLP printing, such as regions with large overhangs and staircase artifacts, while preserving its intrinsic advantages of high resolution and fast printing speeds. We formulate the slicing problem as an optimization task, in which parametric curves are computed to define both the slicing layers and the model partitioning through their tangent planes. These curves inherently define motion trajectories for the build platform and can be optimized to meet critical manufacturing objectives, including collision-free motion and floating-free deposition. We validate our method through physical experiments on a robotic multi-axis DLP printing setup, demonstrating that the optimized curves can robustly guide smooth, high-quality fabrication of complex geometries.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">This paper introduces a novel curve-based slicing method for generating planar layers with dynamically varying orientations in digital light processing (DLP) 3D printing. Our approach effectively addresses key challenges in DLP printing, such as regions with large overhangs and staircase artifacts, while preserving its intrinsic advantages of high resolution and fast printing speeds. We formulate the slicing problem as an optimization task, in which parametric curves are computed to define both the slicing layers and the model partitioning through their tangent planes. These curves inherently define motion trajectories for the build platform and can be optimized to meet critical manufacturing objectives, including collision-free motion and floating-free deposition. We validate our method through physical experiments on a robotic multi-axis DLP printing setup, demonstrating that the optimized curves can robustly guide smooth, high-quality fabrication of complex geometries.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_2105">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/zFysqZ3jbS12F5Le.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3763354" target="_blank" rel="noopener">INF-3DP: Implicit Neural Fields for Collision-Free Multi-Axis 3D Printing</a></h3>
                    <p class="authors">Jiasheng Qu, Zhuo Huang, Dezhao Guo, Hailin Sun, Aoran Lyu, Chengkai Dai</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3763354" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_2105', 'url', 'https://dl.acm.org/doi/10.1145/3763354')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_2105', 'abstract', 'We introduce a general, scalable computational framework for multi-axis 3D printing based on implicit neural fields (INFs) that unifies all stages of tool-path generation and global collision-free motion planning. In our pipeline, input models are represented as signed distance fields, with fabrication objectives‚Äîsuch as support-free printing, surface finish quality, and extrusion control‚Äîdirectly encoded in the optimization of an implicit guidance field. This unified approach enables toolpath optimization across both surface and interior domains, allowing shell and infill paths to be generated via implicit field interpolation. The printing sequence and multi-axis motion are then jointly optimized over a continuous quaternion field. Our continuous formulation constructs the evolving printing object as a time-varying SDF, supporting differentiable global collision handling throughout INF-based motion planning. Compared to explicit-representation-based methods, INF-3DP achieves up to two orders of magnitude speedup and significantly reduces waypoint-to-surface error. We validate our framework on diverse, complex models and demonstrate its efficiency with physical fabrication experiments using a robot-assisted multi-axis system.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">We introduce a general, scalable computational framework for multi-axis 3D printing based on implicit neural fields (INFs) that unifies all stages of tool-path generation and global collision-free motion planning. In our pipeline, input models are represented as signed distance fields, with fabrication objectives‚Äîsuch as support-free printing, surface finish quality, and extrusion control‚Äîdirectly encoded in the optimization of an implicit guidance field. This unified approach enables toolpath optimization across both surface and interior domains, allowing shell and infill paths to be generated via implicit field interpolation. The printing sequence and multi-axis motion are then jointly optimized over a continuous quaternion field. Our continuous formulation constructs the evolving printing object as a time-varying SDF, supporting differentiable global collision handling throughout INF-based motion planning. Compared to explicit-representation-based methods, INF-3DP achieves up to two orders of magnitude speedup and significantly reduces waypoint-to-surface error. We validate our framework on diverse, complex models and demonstrate its efficiency with physical fabrication experiments using a robot-assisted multi-axis system.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            </div>
        </section>
        
        <section class="session">
            <div class="session-header">
                <h2>Visibility & Real-Time Rendering</h2>
                <span class="session-count">5 papers</span>
            </div>
            <div class="papers-grid">
                
            <article class="paper-card" data-paper-id="papers_1060">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/NR6oKmXkokJu76AQ.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763822" target="_blank" rel="noopener">NeuralPVS: Learned Estimation of Potentially Visible Sets</a></h3>
                    <p class="authors">Xiangyu Wang, Thomas K√∂hler, Jun Lin Qiu, Shohei Mori, Markus Steinberger, Dieter Schmalstieg</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763822" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1060', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763822')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1060', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1388">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/DNn9bgrTKfbpy88T.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763869" target="_blank" rel="noopener">Neural Visibility of Point Sets</a></h3>
                    <p class="authors">Jun-Hao Wang, Yi-Yang Tian, Baoquan Chen, Peng-Shuai Wang</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763869" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1388', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763869')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1388', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_2150">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/F1qZBTrwVWjF1f6y.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763981" target="_blank" rel="noopener">Potentially Visible Set Generation with the Disocclusion Buffer</a></h3>
                    <p class="authors">Sebastian K√ºnzel, Sergej Geringer, Quynh Quang Ngo, Philip Voglreiter, Daniel Weiskopf, Dieter Schmalstieg</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763981" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_2150', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763981')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_2150', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_2067">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/8jaNcgfSVVy3Aqzc.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3763348" target="_blank" rel="noopener">Lightweight, Edge-Aware, and Temporally Consistent Supersampling for Mobile Real-Time Rendering</a></h3>
                    <p class="authors">Sipeng Yang, Jiayu Ji, Junhao Zhuge, Jinzhe Zhao, Qiang Qiu, Chen Li</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3763348" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_2067', 'url', 'https://dl.acm.org/doi/10.1145/3763348')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_2067', 'abstract', 'Supersampling has proven highly effective in enhancing visual fidelity by reducing aliasing, increasing resolution, and generating interpolated frames. It has become a standard component of modern real-time rendering pipelines. However, on mobile platforms, deep learning-based supersampling methods remain impractical due to stringent hardware constraints, while non-neural supersampling techniques often fall short in delivering perceptually high-quality results. In particular, producing visually pleasing reconstructions and temporally coherent interpolations is still a significant challenge in mobile settings. In this work, we present a novel, lightweight supersampling framework tailored for mobile devices. Our approach substantially improves both image reconstruction quality and temporal consistency while maintaining real-time performance. For super-resolution, we propose an intra-pixel object coverage estimation method for reconstructing high-quality anti-aliased pixels in edge regions, a gradient-guided strategy for non-edge areas, and a temporal sample accumulation approach to improve overall image quality. For frame interpolation, we develop an efficient motion estimation module coupled with a lightweight fusion scheme that integrates both estimated optical flow and rendered motion vectors, enabling temporally coherent interpolation of object dynamics and lighting variations. Extensive experiments demonstrate that our method consistently outperforms existing baselines in both perceptual image quality and temporal smoothness, while maintaining real-time performance on mobile GPUs. A demo application and supplementary materials are available on the project page.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">Supersampling has proven highly effective in enhancing visual fidelity by reducing aliasing, increasing resolution, and generating interpolated frames. It has become a standard component of modern real-time rendering pipelines. However, on mobile platforms, deep learning-based supersampling methods remain impractical due to stringent hardware constraints, while non-neural supersampling techniques often fall short in delivering perceptually high-quality results. In particular, producing visually pleasing reconstructions and temporally coherent interpolations is still a significant challenge in mobile settings. In this work, we present a novel, lightweight supersampling framework tailored for mobile devices. Our approach substantially improves both image reconstruction quality and temporal consistency while maintaining real-time performance. For super-resolution, we propose an intra-pixel object coverage estimation method for reconstructing high-quality anti-aliased pixels in edge regions, a gradient-guided strategy for non-edge areas, and a temporal sample accumulation approach to improve overall image quality. For frame interpolation, we develop an efficient motion estimation module coupled with a lightweight fusion scheme that integrates both estimated optical flow and rendered motion vectors, enabling temporally coherent interpolation of object dynamics and lighting variations. Extensive experiments demonstrate that our method consistently outperforms existing baselines in both perceptual image quality and temporal smoothness, while maintaining real-time performance on mobile GPUs. A demo application and supplementary materials are available on the project page.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_2231">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/XXMKoPG5CWtH9UpM.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3763363" target="_blank" rel="noopener">Consecutive Frame Extrapolation with Predictive Sparse Shading</a></h3>
                    <p class="authors">Zhizhen Wu, Zhe Cao, Yazhen Yuan, Rui Wang, Yuchi Huo</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3763363" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_2231', 'url', 'https://dl.acm.org/doi/10.1145/3763363')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_2231', 'abstract', 'The demand for high-frame-rate rendering keeps increasing in modern displays. Existing frame generation and super-resolution techniques accelerate rendering by reducing rendering samples across space or time. However, they rely on a uniform sampling reduction strategy, which undersamples areas with complex details or dynamic shading. To address this, we propose to sparsely shade critical areas while reusing generated pixels in low-variation areas for neural extrapolation. Specifically, we introduce the Predictive Error-Flow-eXtrapolation Network (EFXNet)-an architecture that predicts extrapolation errors, estimates flows, and extrapolates frames at once. Firstly, EFXNet leverages temporal coherence to predict extrapolation error and guide the sparse shading of dynamic areas. In addition, EFXNet employs a target-grid correlation module to estimate robust optical flows from pixel correlations rather than pixel values. Finally, EFXNet uses dedicated motion representations for the historical geometric and lighting components, respectively, to extrapolate temporally stable frames. Extensive experimental results show that, compared with state-of-the-art methods, our frame extrapolation method exhibits superior visual quality and temporal stability under a low rendering budget.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">The demand for high-frame-rate rendering keeps increasing in modern displays. Existing frame generation and super-resolution techniques accelerate rendering by reducing rendering samples across space or time. However, they rely on a uniform sampling reduction strategy, which undersamples areas with complex details or dynamic shading. To address this, we propose to sparsely shade critical areas while reusing generated pixels in low-variation areas for neural extrapolation. Specifically, we introduce the Predictive Error-Flow-eXtrapolation Network (EFXNet)-an architecture that predicts extrapolation errors, estimates flows, and extrapolates frames at once. Firstly, EFXNet leverages temporal coherence to predict extrapolation error and guide the sparse shading of dynamic areas. In addition, EFXNet employs a target-grid correlation module to estimate robust optical flows from pixel correlations rather than pixel values. Finally, EFXNet uses dedicated motion representations for the historical geometric and lighting components, respectively, to extrapolate temporally stable frames. Extensive experimental results show that, compared with state-of-the-art methods, our frame extrapolation method exhibits superior visual quality and temporal stability under a low rendering budget.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            </div>
        </section>
        
        <section class="session">
            <div class="session-header">
                <h2>Physically Based Simulation & Dynamic Environments</h2>
                <span class="session-count">6 papers</span>
            </div>
            <div class="papers-grid">
                
            <article class="paper-card" data-paper-id="papers_1133">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/ms5TLsWAdUPu6bYt.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3763283" target="_blank" rel="noopener">A Highly-Efficient Hybrid Simulation System for Flight Controller Design and Evaluation of Unmanned Aerial Vehicles</a></h3>
                    <p class="authors">Jiwei Wang, Wenbin Song, Yicheng Fan, Yang Wang, Xiaopei Liu</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3763283" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1133', 'url', 'https://dl.acm.org/doi/10.1145/3763283')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1133', 'abstract', 'Unmanned aerial vehicles (UAVs) have demonstrated remarkable efficacy across diverse fields. Nevertheless, developing flight controllers tailored to a specific UAV design, particularly in environments with strong fluid-interactive dynamics, remains challenging. Conventional controller design experiences often fall short in such cases, rendering it infeasible to apply time-tested practices. Consequently, a simulation test bed becomes indispensable for controller design and evaluation prior to its actual implementation on the physical UAV. This platform should allow for meticulous adjustment of controllers and should be able to transfer to real-world systems without significant performance degradation. Existing simulators predominantly hinge on empirical models due to high efficiency, often overlooking the dynamic interplay between the UAV and the surrounding airflow. This makes it difficult to mimic more complex flight maneuvers, such as an abrupt midair halt inside narrow channels, in which the UAV may experience strong fluid-structure interactions. On the other hand, simulators considering the complex surrounding airflow are extremely slow and inadequate to support the design and evaluation of flight controllers. In this paper, we present a novel remedy for highly-efficient UAV flight simulations, which entails a hybrid modeling that deftly combines our novel far-field adaptive block-based fluid simulator with parametric empirical models situated near the boundary of the UAV, with the model parameters automatically calibrated. With this newly devised simulator, a broader spectrum of flight scenarios can be explored for controller design and assessment, encompassing those influenced by potent close-proximity effects, or situations where multiple UAVs operate in close quarters. The practical worth of our simulator has been authenticated through comparisons with actual UAV flight data. We further showcase its utility in designing flight controllers for fixed-wing, multi-rotor, and hybrid UAVs, and even exemplify its application when multiple UAVs are involved, underlining the unique value of our system for flight controllers.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">Unmanned aerial vehicles (UAVs) have demonstrated remarkable efficacy across diverse fields. Nevertheless, developing flight controllers tailored to a specific UAV design, particularly in environments with strong fluid-interactive dynamics, remains challenging. Conventional controller design experiences often fall short in such cases, rendering it infeasible to apply time-tested practices. Consequently, a simulation test bed becomes indispensable for controller design and evaluation prior to its actual implementation on the physical UAV. This platform should allow for meticulous adjustment of controllers and should be able to transfer to real-world systems without significant performance degradation. Existing simulators predominantly hinge on empirical models due to high efficiency, often overlooking the dynamic interplay between the UAV and the surrounding airflow. This makes it difficult to mimic more complex flight maneuvers, such as an abrupt midair halt inside narrow channels, in which the UAV may experience strong fluid-structure interactions. On the other hand, simulators considering the complex surrounding airflow are extremely slow and inadequate to support the design and evaluation of flight controllers. In this paper, we present a novel remedy for highly-efficient UAV flight simulations, which entails a hybrid modeling that deftly combines our novel far-field adaptive block-based fluid simulator with parametric empirical models situated near the boundary of the UAV, with the model parameters automatically calibrated. With this newly devised simulator, a broader spectrum of flight scenarios can be explored for controller design and assessment, encompassing those influenced by potent close-proximity effects, or situations where multiple UAVs operate in close quarters. The practical worth of our simulator has been authenticated through comparisons with actual UAV flight data. We further showcase its utility in designing flight controllers for fixed-wing, multi-rotor, and hybrid UAVs, and even exemplify its application when multiple UAVs are involved, underlining the unique value of our system for flight controllers.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1360">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/yqty5EqgcMoL8Mff.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763862" target="_blank" rel="noopener">PhySIC: Physically Plausible 3D Human-Scene Interaction and Contact from a Single Image</a></h3>
                    <p class="authors">Pradyumna Yalandur-Muralidhar, Yuxuan Xue, Xianghui Xie, Margaret Kostyrko, Gerard Pons-Moll</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763862" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1360', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763862')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1360', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_2447">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/SvAT4mxAqBNHyGeR.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3764002" target="_blank" rel="noopener">FreeMusco: Motion-Free Learning of Latent Control for Morphology-Adaptive Locomotion in Musculoskeletal Characters</a></h3>
                    <p class="authors">Minkwan Kim, Yoonsang Lee</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3764002" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_2447', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3764002')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_2447', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1613">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/co25mePRCVsmtioG.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3763318" target="_blank" rel="noopener">CFC: Simulating Character-Fluid Coupling using a Two-Level World Model</a></h3>
                    <p class="authors">Zhiyang Dou, Chen Peng, Xinyu Lu, Xiaohan Ye, Lixing Fang, Yuan Liu</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3763318" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1613', 'url', 'https://dl.acm.org/doi/10.1145/3763318')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1613', 'abstract', 'Humans possess the ability to master a wide range of motor skills, enabling them to quickly and flexibly adapt to the surrounding environment. Despite recent progress in replicating such versatile human motor skills, existing research often oversimplifies or inadequately captures the complex interplay between human body movements and highly dynamic environments, such as interactions with fluids. In this paper, we present a world model for Character-Fluid Coupling (CFC) for simulating human-fluid interactions via two-way coupling. We introduce a two-level world model which consists of a Physics-Informed Neural Network (PINN)-based model for fluid dynamics and a character world model capturing body dynamics under various external forces. This two-level world model adeptly predicts the dynamics of fluid and its influence on rigid bodies via force prediction, sidestepping the computational burden of fluid simulation and providing policy gradients for efficient policy training. Once trained, our system can control characters to complete high-level tasks while adaptively responding to environmental changes. We also present that the fluid initiates emergent behaviors of the characters, enhancing motion diversity and interactivity. Extensive experiments underscore the effectiveness of CFC, demonstrating its ability to produce high-quality, realistic human-fluid interaction animations.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">Humans possess the ability to master a wide range of motor skills, enabling them to quickly and flexibly adapt to the surrounding environment. Despite recent progress in replicating such versatile human motor skills, existing research often oversimplifies or inadequately captures the complex interplay between human body movements and highly dynamic environments, such as interactions with fluids. In this paper, we present a world model for Character-Fluid Coupling (CFC) for simulating human-fluid interactions via two-way coupling. We introduce a two-level world model which consists of a Physics-Informed Neural Network (PINN)-based model for fluid dynamics and a character world model capturing body dynamics under various external forces. This two-level world model adeptly predicts the dynamics of fluid and its influence on rigid bodies via force prediction, sidestepping the computational burden of fluid simulation and providing policy gradients for efficient policy training. Once trained, our system can control characters to complete high-level tasks while adaptively responding to environmental changes. We also present that the fluid initiates emergent behaviors of the characters, enhancing motion diversity and interactivity. Extensive experiments underscore the effectiveness of CFC, demonstrating its ability to produce high-quality, realistic human-fluid interaction animations.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_2360">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/xbvz2jvr3pi9iCLt.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763997" target="_blank" rel="noopener">Fast &amp; Stable Control of Coupled Solid-Fluid Dynamic Systems</a></h3>
                    <p class="authors">Jie Chen, Zherong Pan, Bo Ren</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763997" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_2360', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763997')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_2360', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1535">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/pnLH7CykemRTKzqW.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3763310" target="_blank" rel="noopener">Force-Dual Modes: Subspace Design from Stochastic Forces</a></h3>
                    <p class="authors">Otman Benchekroun, Eitan Grinspun, Maurizio Chiaramonte, Philip Allen Etter</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3763310" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1535', 'url', 'https://dl.acm.org/doi/10.1145/3763310')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1535', 'abstract', 'Designing subspaces for Reduced Order Modeling (ROM) is crucial for accelerating finite element simulations in graphics and engineering. Unfortunately, it\'s not always clear which subspace is optimal for arbitrary dynamic simulation. We propose to construct simulation subspaces from force distributions, allowing us to tailor such subspaces to common scene interactions involving constraint penalties, handles-based control, contact and musculoskeletal actuation. To achieve this we adopt a statistical perspective on Reduced Order Modelling, which allows us to push such user-designed force distributions through a linearized simulation to obtain a dual distribution on displacements. To construct our subspace, we then fit a low-rank Gaussian model to this displacement distribution, which we show generalizes Linear Modal Analysis subspaces for uncorrelated unit variance force distributions, as well as Green\'s Function subspaces for low rank force distributions. We show our framework allows for the construction of subspaces that are optimal both with respect to physical material properties, as well as arbitrary force distributions as observed in handle-based, contact, and musculoskeletal scene interactions.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">Designing subspaces for Reduced Order Modeling (ROM) is crucial for accelerating finite element simulations in graphics and engineering. Unfortunately, it&#x27;s not always clear which subspace is optimal for arbitrary dynamic simulation. We propose to construct simulation subspaces from force distributions, allowing us to tailor such subspaces to common scene interactions involving constraint penalties, handles-based control, contact and musculoskeletal actuation. To achieve this we adopt a statistical perspective on Reduced Order Modelling, which allows us to push such user-designed force distributions through a linearized simulation to obtain a dual distribution on displacements. To construct our subspace, we then fit a low-rank Gaussian model to this displacement distribution, which we show generalizes Linear Modal Analysis subspaces for uncorrelated unit variance force distributions, as well as Green&#x27;s Function subspaces for low rank force distributions. We show our framework allows for the construction of subspaces that are optimal both with respect to physical material properties, as well as arbitrary force distributions as observed in handle-based, contact, and musculoskeletal scene interactions.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            </div>
        </section>
        
        <section class="session">
            <div class="session-header">
                <h2>Audio-Driven Facial and Portrait Animation</h2>
                <span class="session-count">6 papers</span>
            </div>
            <div class="papers-grid">
                
            <article class="paper-card" data-paper-id="papers_2159">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/436cnuEoR9bbyrL4.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3763357" target="_blank" rel="noopener">One String to Pull Them All: Fast Assembly of Curved Structures from Flat Auxetic Linkages</a></h3>
                    <p class="authors">Akib Zaman, Jacqueline Aslarus, Jiaji Li, Stefanie Mueller, Mina Konakovic Lukovic</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3763357" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_2159', 'url', 'https://dl.acm.org/doi/10.1145/3763357')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_2159', 'abstract', 'We present a computational approach for designing freeform structures that can be rapidly assembled from initially flat configurations by a single string pull. The target structures are decomposed into rigid spatially varied quad tiles that are optimized to approximate the user-provided surface, forming a flat mechanical linkage. Our algorithm then uses a two-step method to find a physically realizable string path that controls only a subset of tiles to smoothly actuate the structure from flat to assembled configuration. We initially compute the minimal subset of tiles that are required to be controlled with the string considering the geometry of the structure and interaction among the tiles. We then find a valid string path through these tiles that minimizes friction, which will assemble the flat linkage into the target 3D structure upon tightening a single string. The resulting designs can be easily manufactured with computational fabrication techniques such as 3D printing, CNC milling, molding, etc. in flat configuration that, in addition to manufacturing, facilitates storage and transportation. We validate our approach by developing a series of physical prototypes and showcasing various application case studies, ranging from medical devices, space shelters, to architectural designs.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">We present a computational approach for designing freeform structures that can be rapidly assembled from initially flat configurations by a single string pull. The target structures are decomposed into rigid spatially varied quad tiles that are optimized to approximate the user-provided surface, forming a flat mechanical linkage. Our algorithm then uses a two-step method to find a physically realizable string path that controls only a subset of tiles to smoothly actuate the structure from flat to assembled configuration. We initially compute the minimal subset of tiles that are required to be controlled with the string considering the geometry of the structure and interaction among the tiles. We then find a valid string path through these tiles that minimizes friction, which will assemble the flat linkage into the target 3D structure upon tightening a single string. The resulting designs can be easily manufactured with computational fabrication techniques such as 3D printing, CNC milling, molding, etc. in flat configuration that, in addition to manufacturing, facilitates storage and transportation. We validate our approach by developing a series of physical prototypes and showcasing various application case studies, ranging from medical devices, space shelters, to architectural designs.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_2197">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/UXQMVhZX9UvC8jZc.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763983" target="_blank" rel="noopener">Discovering Folding Lines for Surface Compression</a></h3>
                    <p class="authors">Toshiki Aoki, Tomohiro Tachi, Mina Konakovic Lukovic</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763983" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_2197', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763983')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_2197', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1602">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/S2uTNsN18hZD1y2B.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763895" target="_blank" rel="noopener">Reconfigurable Hinged Kirigami Tessellations</a></h3>
                    <p class="authors">Aviv Segall, Jing Ren, Marcel Padilla, Olga Sorkine-Hornung</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763895" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1602', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763895')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1602', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1636">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/HrzRQSuChdMtMZsh.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3763808" target="_blank" rel="noopener">Snapping Deployable Toroids for Modular Gridshells</a></h3>
                    <p class="authors">Felix Dellinger, Martin Kilian, Munkyun Lee, Christian M√ºller, Georg Nawratil, Tomohiro Tachi</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3763808" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1636', 'url', 'https://dl.acm.org/doi/10.1145/3763808')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1636', 'abstract', 'We introduce a novel class of polyhedral tori (PQ-toroids) that snap between two stable configurations - a flat state and a deployed one separated by an energy barrier. Being able to create PQ-toroids from any set of given planar bottom and side faces opens the possibility to assemble the bistable blocks into a thick freeform curved shell structure to follow a planar quadrilateral (PQ) net with coplanar adjacent offset directions. A design pipeline is developed and presented for inversely computing PQ-toroid modules using conjugate net decompositions of a given surface. We analyze the snapping behavior and energy barriers through simulation and build physical prototypes to validate the feasibility of the proposed system. This work expands the geometric design space of multistable origami for lightweight modular structures and offers practical applications in architectural and deployable systems.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">We introduce a novel class of polyhedral tori (PQ-toroids) that snap between two stable configurations - a flat state and a deployed one separated by an energy barrier. Being able to create PQ-toroids from any set of given planar bottom and side faces opens the possibility to assemble the bistable blocks into a thick freeform curved shell structure to follow a planar quadrilateral (PQ) net with coplanar adjacent offset directions. A design pipeline is developed and presented for inversely computing PQ-toroid modules using conjugate net decompositions of a given surface. We analyze the snapping behavior and energy barriers through simulation and build physical prototypes to validate the feasibility of the proposed system. This work expands the geometric design space of multistable origami for lightweight modular structures and offers practical applications in architectural and deployable systems.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1302">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/DoyuUjiEL3WwKroa.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3763296" target="_blank" rel="noopener">Closed-Form Construction of Voronoi Diagrams with Star-Shaped Metrics</a></h3>
                    <p class="authors">Haoyang Zhou, Logan Numerow, Stelian Coros, Bernhard Thomaszewski</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3763296" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1302', 'url', 'https://dl.acm.org/doi/10.1145/3763296')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1302', 'abstract', 'Cellular patterns, from planar ornaments to architectural surfaces and mechanical metamaterials, blend aesthetics with functionality. Homogeneous patterns like isohedral tilings offer simplicity and symmetry but lack flexibility, particularly for heterogeneous designs. They cannot smoothly interpolate between tilings or adapt to double-curved surfaces without distortion. Voronoi diagrams provide a more adaptable patterning solution. They can be generalized to star-shaped metrics, enabling diverse cell shapes and continuous grading by interpolating metric parameters. Mart√≠nez et al. [2019] explored this idea in 2D using a rasterization-based algorithm to create compelling patterns. However, this discrete approach precludes gradient-based optimization, limiting control over pattern quality. We introduce a novel, closed-form, fully differentiable formulation for Voronoi diagrams with piecewise linear star-shaped metrics, enabling optimization of site positions and metric parameters to meet aesthetic and functional goals. It naturally extends to arbitrary dimensions, including curved 3D surfaces. For improved on-surface patterning, we propose a per-sector parameterization of star-shaped metrics, ensuring uniform cell shapes in non-regular neighborhoods. We demonstrate our approach by generating diverse patterns, from homogeneous to continuously graded designs, with applications in decorative surfaces and metamaterials.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">Cellular patterns, from planar ornaments to architectural surfaces and mechanical metamaterials, blend aesthetics with functionality. Homogeneous patterns like isohedral tilings offer simplicity and symmetry but lack flexibility, particularly for heterogeneous designs. They cannot smoothly interpolate between tilings or adapt to double-curved surfaces without distortion. Voronoi diagrams provide a more adaptable patterning solution. They can be generalized to star-shaped metrics, enabling diverse cell shapes and continuous grading by interpolating metric parameters. Mart√≠nez et al. [2019] explored this idea in 2D using a rasterization-based algorithm to create compelling patterns. However, this discrete approach precludes gradient-based optimization, limiting control over pattern quality. We introduce a novel, closed-form, fully differentiable formulation for Voronoi diagrams with piecewise linear star-shaped metrics, enabling optimization of site positions and metric parameters to meet aesthetic and functional goals. It naturally extends to arbitrary dimensions, including curved 3D surfaces. For improved on-surface patterning, we propose a per-sector parameterization of star-shaped metrics, ensuring uniform cell shapes in non-regular neighborhoods. We demonstrate our approach by generating diverse patterns, from homogeneous to continuously graded designs, with applications in decorative surfaces and metamaterials.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_2335">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/tjXQMxbnjbJh2puE.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763996" target="_blank" rel="noopener">Star-Shaped Distance Voronoi Diagrams for 3D Metamaterial Design</a></h3>
                    <p class="authors">Logan Numerow, Stelian Coros, Bernhard Thomaszewski</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763996" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_2335', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763996')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_2335', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            </div>
        </section>
        
        <section class="session">
            <div class="session-header">
                <h2>Computational Design & Fabricability</h2>
                <span class="session-count">6 papers</span>
            </div>
            <div class="papers-grid">
                
            <article class="paper-card" data-paper-id="papers_1077">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/yYF3nVHDrURfqoru.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763825" target="_blank" rel="noopener">CameraVDP: Perceptual Display Assessment with Uncertainty Estimation via Camera and Visual Difference Prediction</a></h3>
                    <p class="authors">Yancheng Cai, Robert Wanat, Rafal Mantiuk</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763825" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1077', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763825')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1077', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1081">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/j3qxe9GJwZTkVLVt.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763827" target="_blank" rel="noopener">DiffCamera: Arbitrary Refocusing on Images</a></h3>
                    <p class="authors">Yiyang Wang, Xi Chen, Xiaogang Xu, Yu Liu, Hengshuang Zhao</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763827" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1081', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763827')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1081', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1422">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/jga9zFHFkcEgj3Nq.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763873" target="_blank" rel="noopener">Learning to Refocus with Video Diffusion Models</a></h3>
                    <p class="authors">SaiKiran Tedla, Zhoutong Zhang, Xuaner Zhang, Shumian Xin</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763873" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1422', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763873')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1422', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1460">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/9dRZKT443FUWeaFj.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3763306" target="_blank" rel="noopener">Generating the Past, Present and Future from a Motion-Blurred Image</a></h3>
                    <p class="authors">SaiKiran Tedla, Kelly Zhu, Trevor Canham, Felix Taubner, Michael S. Brown, Kiriakos N. Kutulakos</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3763306" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1460', 'url', 'https://dl.acm.org/doi/10.1145/3763306')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1460', 'abstract', 'We seek to answer the question: what can a motion-blurred image reveal about a scene\'s past, present, and future? Although motion blur obscures image details and degrades visual quality, it also encodes information about scene and camera motion during an exposure. Previous techniques leverage this information to estimate a sharp image from an input blurry one, or to predict a sequence of video frames showing what might have occurred at the moment of image capture. However, they rely on handcrafted priors or network architectures to resolve ambiguities in this inverse problem, and do not incorporate image and video priors on large-scale datasets. As such, existing methods struggle to reproduce complex scene dynamics and do not attempt to recover what occurred before or after an image was taken. Here, we introduce a new technique that repurposes a pre-trained video diffusion model trained on internet-scale datasets to recover videos revealing complex scene dynamics during the moment of capture and what might have occurred immediately into the past or future. Our approach is robust and versatile; it outperforms previous methods for this task, generalizes to challenging in-the-wild images, and supports downstream tasks such as recovering camera trajectories, object motion, and dynamic 3D scene structure. Code and data are available at blur2vid.github.io')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">We seek to answer the question: what can a motion-blurred image reveal about a scene&#x27;s past, present, and future? Although motion blur obscures image details and degrades visual quality, it also encodes information about scene and camera motion during an exposure. Previous techniques leverage this information to estimate a sharp image from an input blurry one, or to predict a sequence of video frames showing what might have occurred at the moment of image capture. However, they rely on handcrafted priors or network architectures to resolve ambiguities in this inverse problem, and do not incorporate image and video priors on large-scale datasets. As such, existing methods struggle to reproduce complex scene dynamics and do not attempt to recover what occurred before or after an image was taken. Here, we introduce a new technique that repurposes a pre-trained video diffusion model trained on internet-scale datasets to recover videos revealing complex scene dynamics during the moment of capture and what might have occurred immediately into the past or future. Our approach is robust and versatile; it outperforms previous methods for this task, generalizes to challenging in-the-wild images, and supports downstream tasks such as recovering camera trajectories, object motion, and dynamic 3D scene structure. Code and data are available at blur2vid.github.io</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1250">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/dDs61C6zoNmtArSW.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763850" target="_blank" rel="noopener">Automated Design of Compound Lenses with Discrete-Continuous Optimization</a></h3>
                    <p class="authors">Arjun Teh, Delio Vicini, Bernd Bickel, Ioannis Gkioulekas, Matthew O&#x27;Toole</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763850" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1250', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763850')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1250', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1761">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/hYRcf3yGb8syzSs5.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763919" target="_blank" rel="noopener">UltraZoom: Generating Gigapixel Images from Regular Photos</a></h3>
                    <p class="authors">Jingwei Ma, Vivek Jayaram, Brian Curless, Ira Kemelmacher-Shlizerman, Steven Seitz</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763919" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1761', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763919')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1761', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            </div>
        </section>
        
        <section class="session">
            <div class="session-header">
                <h2>Computational Photography & Cameras</h2>
                <span class="session-count">6 papers</span>
            </div>
            <div class="papers-grid">
                
            <article class="paper-card" data-paper-id="papers_1824">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/saa4GqDpEKAHgGhF.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763931" target="_blank" rel="noopener">Nonlinear Noise2Noise for Efficient Monte Carlo Denoiser Training</a></h3>
                    <p class="authors">Andrew Tinits, Stephen Mann</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763931" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1824', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763931')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1824', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_2326">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/BN8cVkGjULvTdSLX.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763995" target="_blank" rel="noopener">Statistical Error Reduction for Monte Carlo Rendering</a></h3>
                    <p class="authors">Hiroyuki Sakai, Christian Freude, Michael Wimmer, David Hahn</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763995" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_2326', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763995')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_2326', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1592">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/PuzuDwWxaRxXwt4W.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3763315" target="_blank" rel="noopener">DSCombiner: Double Shrinkage for Combining Biased and Unbiased Monte Carlo Renderings</a></h3>
                    <p class="authors">Chenxi Zhou, Keheng Xu, Mufan Guo, Xianhao Yu, Zhimin Fan, Guihuan Feng</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3763315" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1592', 'url', 'https://dl.acm.org/doi/10.1145/3763315')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1592', 'abstract', 'Monte Carlo rendering often faces a dilemma, namely, whether to choose an unbiased estimator or a biased one. Although different integrators have been developed to address various scenarios, no single method can effectively manage all situations. Thus, finding a good approach to combine different integrators has always been a topic that warrants exploration. This work proposes DSCombiner, a new shrinkage estimator that flexibly combines unbiased and biased estimators (typically generated by different integrators) in image space into a single estimating procedure, strategically utilizing the strengths of different integrators while minimizing their weaknesses. DSCombiner overcomes the limitation of single shrinkage combiners by introducing a two-step shrinkage towards a noise-free radiance prior. We derive optimal shrinkage factors for the two steps within a hierarchical Bayesian framework, and provide a deep learning-based method to improve the results. Comprehensive qualitative and quantitative validations across diverse scenes demonstrate visible improvements in image quality, as compared with previous image-space and path-space combiners.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">Monte Carlo rendering often faces a dilemma, namely, whether to choose an unbiased estimator or a biased one. Although different integrators have been developed to address various scenarios, no single method can effectively manage all situations. Thus, finding a good approach to combine different integrators has always been a topic that warrants exploration. This work proposes DSCombiner, a new shrinkage estimator that flexibly combines unbiased and biased estimators (typically generated by different integrators) in image space into a single estimating procedure, strategically utilizing the strengths of different integrators while minimizing their weaknesses. DSCombiner overcomes the limitation of single shrinkage combiners by introducing a two-step shrinkage towards a noise-free radiance prior. We derive optimal shrinkage factors for the two steps within a hierarchical Bayesian framework, and provide a deep learning-based method to improve the results. Comprehensive qualitative and quantitative validations across diverse scenes demonstrate visible improvements in image quality, as compared with previous image-space and path-space combiners.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1053">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/GTrtFsePghujzpx6.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3763273" target="_blank" rel="noopener">Jackknife Transmittance and MIS Weight Estimation</a></h3>
                    <p class="authors">Christoph Peters</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3763273" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1053', 'url', 'https://dl.acm.org/doi/10.1145/3763273')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1053', 'abstract', 'A core operation in Monte Carlo volume rendering is transmittance estimation: Given a segment along a ray, the goal is to estimate the fraction of light that will pass through this segment without encountering absorption or out-scattering. A naive approach is to estimate optical depth œÑ using unbiased ray marching and to then use exp(-œÑ) as transmittance estimate. However, this strategy systematically overestimates transmittance due to Jensen\'s inequality. On the other hand, existing unbiased transmittance estimators either suffer from high variance or have a cost governed by random decisions, which makes them less suitable for SIMD architectures. We propose a biased transmittance estimator with significantly reduced bias compared to the naive approach and a deterministic and low cost. We observe that ray marching with stratified jittered sampling results in estimates of optical depth that are nearly normal-distributed. We then apply the unique minimum variance unbiased (UMVU) estimator of exp(- œÑ ) based on two such estimates (using two different sets of random numbers). Bias only arises from violations of the assumption of normal-distributed inputs. We further reduce bias and variance using a variance-aware importance sampling scheme. The underlying theory can be used to estimate any analytic function of optical depth. We use this generalization to estimate multiple importance sampling (MIS) weights and introduce two integrators: Unbiased MIS with biased MIS weights and a more efficient but biased combination of MIS and transmittance estimation.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">A core operation in Monte Carlo volume rendering is transmittance estimation: Given a segment along a ray, the goal is to estimate the fraction of light that will pass through this segment without encountering absorption or out-scattering. A naive approach is to estimate optical depth œÑ using unbiased ray marching and to then use exp(-œÑ) as transmittance estimate. However, this strategy systematically overestimates transmittance due to Jensen&#x27;s inequality. On the other hand, existing unbiased transmittance estimators either suffer from high variance or have a cost governed by random decisions, which makes them less suitable for SIMD architectures. We propose a biased transmittance estimator with significantly reduced bias compared to the naive approach and a deterministic and low cost. We observe that ray marching with stratified jittered sampling results in estimates of optical depth that are nearly normal-distributed. We then apply the unique minimum variance unbiased (UMVU) estimator of exp(- œÑ ) based on two such estimates (using two different sets of random numbers). Bias only arises from violations of the assumption of normal-distributed inputs. We further reduce bias and variance using a variance-aware importance sampling scheme. The underlying theory can be used to estimate any analytic function of optical depth. We use this generalization to estimate multiple importance sampling (MIS) weights and introduce two integrators: Unbiased MIS with biased MIS weights and a more efficient but biased combination of MIS and transmittance estimation.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1797">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/28mY2rzDV8U3AmYs.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3763335" target="_blank" rel="noopener">Imperfect Image-Space Control Variates for Monte Carlo Rendering</a></h3>
                    <p class="authors">Chanu Yang, Bochang Moon</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3763335" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1797', 'url', 'https://dl.acm.org/doi/10.1145/3763335')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1797', 'abstract', 'We present an image-space control variate technique to improve Monte Carlo (MC) integration-based rendering. Our method selects spatially nearby pixel estimates as control variates to exploit spatial coherence among pixel estimates in a rendered image without requiring analytic modeling of the control variate functions. Employing control variates is a classical and well-established technique for variance reduction in MC integration, typically relying on the assumption that the expectations of control variates are readily obtainable. When this condition is met, control variate theory offers a principled framework for optimizing their use by adjusting coefficients that determine the relative contribution of each control variate. However, our image-space approach introduces a technical challenge, as the expectations of the pixel-based control variates are unknown and must be estimated from additional MC samples, which are unbiased but inherently noisy. In this paper, we propose a control variate estimator designed to optimally leverage such imperfect control variates by relaxing the traditional requirement that their expectations are known. We demonstrate that our approach, which estimates the optimal coefficients while explicitly accounting for uncertainty in the expectation estimates, effectively reduces the variance of MC rendering across various test scenes.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">We present an image-space control variate technique to improve Monte Carlo (MC) integration-based rendering. Our method selects spatially nearby pixel estimates as control variates to exploit spatial coherence among pixel estimates in a rendered image without requiring analytic modeling of the control variate functions. Employing control variates is a classical and well-established technique for variance reduction in MC integration, typically relying on the assumption that the expectations of control variates are readily obtainable. When this condition is met, control variate theory offers a principled framework for optimizing their use by adjusting coefficients that determine the relative contribution of each control variate. However, our image-space approach introduces a technical challenge, as the expectations of the pixel-based control variates are unknown and must be estimated from additional MC samples, which are unbiased but inherently noisy. In this paper, we propose a control variate estimator designed to optimally leverage such imperfect control variates by relaxing the traditional requirement that their expectations are known. We demonstrate that our approach, which estimates the optimal coefficients while explicitly accounting for uncertainty in the expectation estimates, effectively reduces the variance of MC rendering across various test scenes.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1042">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/2Utbq43UYikNLJki.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3>SZ Sequences: Binary-Constructed $(0, 2^q)$-Sequences</h3>
                    <p class="authors">Abdalla G. M. Ahmed, Matt Pharr, Victor Ostromoukhov, Hui Huang</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <button class="edit-btn" onclick="openEditModal('papers_1042', 'url', '')" title="Add URL">‚úèÔ∏è Add link</button>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1042', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            </div>
        </section>
        
        <section class="session">
            <div class="session-header">
                <h2>Sampling, Reconstruction & Variance Reduction</h2>
                <span class="session-count">6 papers</span>
            </div>
            <div class="papers-grid">
                
            <article class="paper-card" data-paper-id="papers_1264">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/WacCmtm7DFPEh5bA.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763854" target="_blank" rel="noopener">Audio Driven Real-Time Facial Animation for Social Telepresence</a></h3>
                    <p class="authors">Jiye Lee, Chenghui Li, Linh Tran, Shih-En Wei, Jason Saragih, Alexander Richard</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763854" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1264', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763854')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1264', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1520">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/bsqtiZJYtgmzR7wW.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763887" target="_blank" rel="noopener">LSF-Animation: Label-Free Speech-Driven Facial Animation via Implicit Feature Representation</a></h3>
                    <p class="authors">Xin Lu, Chuanqing Zhuang, Chenxi Jin, Zhengda Lu, Yiqun Wang, Wu Liu</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763887" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1520', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763887')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1520', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1730">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/hzRZgrMh663ERHKB.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763914" target="_blank" rel="noopener">High-Fidelity Dynamic Portrait Animation via Direct Preference Optimization and Temporal Motion Modulation</a></h3>
                    <p class="authors">Jiahao Cui, Baoyou Chen, Mingwang Xu, Hanlin Shang, Yuxuan Chen, Qinkun Su</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763914" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1730', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763914')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1730', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1860">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/7ZNa32bh5TUqqCy5.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763939" target="_blank" rel="noopener">Audio-Driven Universal Gaussian Head Avatars</a></h3>
                    <p class="authors">Kartik Teotia, Helge Rhodin, Mohit Mendiratta, Hyeongwoo Kim, Marc Habermann, Christian Theobalt</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763939" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1860', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763939')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1860', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1975">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/ggaiz9JgPjBgUkz1.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763955" target="_blank" rel="noopener">ARTalk: Speech-Driven 3D Head Animation via Autoregressive Model</a></h3>
                    <p class="authors">Xuangeng Chu, Nabarun Goswami, Ziteng Cui, Hanqin Wang, Tatsuya Harada</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763955" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1975', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763955')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1975', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_2207">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/3g1kFSjJCE5EyC3C.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3>X-Actor: Emotional and Expressive Long-Range Portrait Acting from Audio</h3>
                    <p class="authors">Chenxu Zhang, Zenan Li, Hongyi Xu, You Xie, Xiaochen Zhao, Tianpei Gu</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <button class="edit-btn" onclick="openEditModal('papers_2207', 'url', '')" title="Add URL">‚úèÔ∏è Add link</button>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_2207', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            </div>
        </section>
        
        <section class="session">
            <div class="session-header">
                <h2>Generative 3D Shape Synthesis</h2>
                <span class="session-count">5 papers</span>
            </div>
            <div class="papers-grid">
                
            <article class="paper-card" data-paper-id="papers_1005">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/eTDAjNSavJampH3z.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763812" target="_blank" rel="noopener">ShapeGen: Towards High-Quality 3D Shape Synthesis</a></h3>
                    <p class="authors">Yangguang Li, Xianglong He, Zi-Xin Zou, Zexiang Liu, Wanli Ouyang, Ding Liang</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763812" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1005', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763812')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1005', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1032">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/fBEeTyn7D1A3iWnH.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763818" target="_blank" rel="noopener">Autoregressive Generation of Static and Growing Trees</a></h3>
                    <p class="authors">Hanxiao Wang, Biao Zhang, Jonathan Klein, Dominik L. Michels, Dongming Yan, Peter Wonka</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763818" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1032', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763818')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1032', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1412">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/s6LPGSkMbcpeqaJm.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763872" target="_blank" rel="noopener">OmniPart: Part-Aware 3D Generation with Semantic Decoupling and Structural Cohesion</a></h3>
                    <p class="authors">Yunhan Yang, Yufan Zhou, Yuan-Chen Guo, Zi-Xin Zou, Yukun Huang, Ying-Tian Liu</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763872" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1412', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763872')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1412', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1446">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/QYxwVxgYCcyGgjBM.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763877" target="_blank" rel="noopener">ART-DECO: Arbitrary Text Guidance for 3D Detailizer Construction</a></h3>
                    <p class="authors">Qimin Chen, Yuezhi Yang, Yifan Wang, Vladimir Kim, Siddhartha Chaudhuri, Hao Zhang</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763877" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1446', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763877')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1446', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1992">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/H3NAF3CYv6pNox6M.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763959" target="_blank" rel="noopener">SPGen: Spherical Projection as Consistent and Flexible Representation for Single Image 3D Shape Generation</a></h3>
                    <p class="authors">Jingdong Zhang, Weikai Chen, Yuan Liu, Jionghao Wang, Zhengming Yu, Zhuowen Shen</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763959" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1992', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763959')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1992', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            </div>
        </section>
        
        <section class="session">
            <div class="session-header">
                <h2>Image Restoration, Editing & Enhancement</h2>
                <span class="session-count">5 papers</span>
            </div>
            <div class="papers-grid">
                
            <article class="paper-card" data-paper-id="papers_1719">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/qKhWUj2q65uZMCZ8.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763913" target="_blank" rel="noopener">DvD: Unleashing a Generative Paradigm for Document Dewarping via Coordinates-based Diffusion Model</a></h3>
                    <p class="authors">Weiguang Zhang, Huangcheng Lu, Maizhen Ning, Xiaowei Huang, Wei Wang, Kaizhu Huang</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763913" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1719', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763913')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1719', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1848">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/N8ea4w5LCXKd27Mf.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3763337" target="_blank" rel="noopener">HRC-Net: Learning Visual Hypothesis, Representative, and Collaboration for Multi-Domain Image Inpainting</a></h3>
                    <p class="authors">Xin Wang, Di Lin, Wanchao Su, Ji Du, Renjie Zhang, Jie Zhang</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3763337" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1848', 'url', 'https://dl.acm.org/doi/10.1145/3763337')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1848', 'abstract', 'Multi-domain image inpainting utilizes complementary contextual information from auxiliary domain images to restore corrupted regions. While existing methods reconstruct auxiliary images to provide additional guidance, they face fundamental limitations: recovered pixels with complex patterns often lack representative details, while oversimplified patterns offer insufficient contextual information. To address these challenges, we propose HRC-Net, a novel framework incorporating three generative sub-networks for the comprehensive image inpainting task. Our architecture consists of: (1) A Hypothesis Sub-network that enables robust samplings of pixel-wise hypotheses from multi-domain inputs; (2) A Representative Sub-network that learns to score hypothesis quality based on contextual relevance; and (3) a Collaboration Sub-network that optimizes adaptive fusion kernels to integrate the most pertinent details. Together, these components model the joint distribution of representative scores and convolutional kernels, fostering a precise interaction between auxiliary hypotheses and target image corruption to meticulously repair the target image. Extensive evaluations across multiple benchmark datasets demonstrate HRC-Net\'s superior performance, significantly outperforming state-of-the-art methods in both quantitative metrics and visual quality.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">Multi-domain image inpainting utilizes complementary contextual information from auxiliary domain images to restore corrupted regions. While existing methods reconstruct auxiliary images to provide additional guidance, they face fundamental limitations: recovered pixels with complex patterns often lack representative details, while oversimplified patterns offer insufficient contextual information. To address these challenges, we propose HRC-Net, a novel framework incorporating three generative sub-networks for the comprehensive image inpainting task. Our architecture consists of: (1) A Hypothesis Sub-network that enables robust samplings of pixel-wise hypotheses from multi-domain inputs; (2) A Representative Sub-network that learns to score hypothesis quality based on contextual relevance; and (3) a Collaboration Sub-network that optimizes adaptive fusion kernels to integrate the most pertinent details. Together, these components model the joint distribution of representative scores and convolutional kernels, fostering a precise interaction between auxiliary hypotheses and target image corruption to meticulously repair the target image. Extensive evaluations across multiple benchmark datasets demonstrate HRC-Net&#x27;s superior performance, significantly outperforming state-of-the-art methods in both quantitative metrics and visual quality.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_2027">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/jzB5H8u1XWW3XrD8.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3763346" target="_blank" rel="noopener">Harnessing Diffusion-Yielded Score Priors for Image Restoration</a></h3>
                    <p class="authors">Xinqi Lin, Fanghua Yu, Jinfan Hu, Zhiyuan You, Wu Shi, Jimmy S. Ren</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3763346" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_2027', 'url', 'https://dl.acm.org/doi/10.1145/3763346')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_2027', 'abstract', 'Deep image restoration models aim to learn a mapping from degraded image space to natural image space. However, they face several critical challenges: removing degradation, generating realistic details, and ensuring pixel-level consistency. Over time, three major classes of methods have emerged, including MSE-based, GAN-based, and diffusion-based methods. However, they fail to achieve a good balance between restoration quality, fidelity, and speed. We propose a novel method, HYPIR, to address these challenges. Our solution pipeline is straightforward: it involves initializing the image restoration model with a pre-trained diffusion model and then fine-tuning it with adversarial training. This approach does not rely on diffusion loss, iterative sampling, or additional adapters. We theoretically demonstrate that initializing adversarial training from a pre-trained diffusion model positions the initial restoration model very close to the natural image distribution. Consequently, this initialization improves numerical stability, avoids mode collapse, and substantially accelerates the convergence of adversarial training. Moreover, HYPIR inherits the capabilities of diffusion models with rich user control, enabling text-guided restoration and adjustable texture richness. Requiring only a single forward pass, it achieves faster convergence and inference speed than diffusion-based methods. Extensive experiments show that HYPIR outperforms previous state-of-the-art methods, achieving efficient and high-quality image restoration.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">Deep image restoration models aim to learn a mapping from degraded image space to natural image space. However, they face several critical challenges: removing degradation, generating realistic details, and ensuring pixel-level consistency. Over time, three major classes of methods have emerged, including MSE-based, GAN-based, and diffusion-based methods. However, they fail to achieve a good balance between restoration quality, fidelity, and speed. We propose a novel method, HYPIR, to address these challenges. Our solution pipeline is straightforward: it involves initializing the image restoration model with a pre-trained diffusion model and then fine-tuning it with adversarial training. This approach does not rely on diffusion loss, iterative sampling, or additional adapters. We theoretically demonstrate that initializing adversarial training from a pre-trained diffusion model positions the initial restoration model very close to the natural image distribution. Consequently, this initialization improves numerical stability, avoids mode collapse, and substantially accelerates the convergence of adversarial training. Moreover, HYPIR inherits the capabilities of diffusion models with rich user control, enabling text-guided restoration and adjustable texture richness. Requiring only a single forward pass, it achieves faster convergence and inference speed than diffusion-based methods. Extensive experiments show that HYPIR outperforms previous state-of-the-art methods, achieving efficient and high-quality image restoration.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_2056">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/UwnkDsHPtJjXHH1v.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763969" target="_blank" rel="noopener">ELAD: Blind Face Restoration using Expectation-based Likelihood Approximation and Diffusion Prior</a></h3>
                    <p class="authors">Sean Man, Guy Ohayon, Ron Raphaeli, Matan Kleiner, Michael Elad</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763969" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_2056', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763969')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_2056', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_2401">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/xu2rUcPyUgqtHKXr.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763999" target="_blank" rel="noopener">Self-supervised Underwater Color Restoration via Wavelet-Diffusion Model with Filtered Multi-Scale Feature Distillation</a></h3>
                    <p class="authors">Xin Zhang, zhuang Zhou, Yixiao Yang, Haijun Xie, Haowen Yan, Hexiang Zhai</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763999" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_2401', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763999')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_2401', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            </div>
        </section>
        
        <section class="session">
            <div class="session-header">
                <h2>Differentiable Rendering & Applications</h2>
                <span class="session-count">5 papers</span>
            </div>
            <div class="papers-grid">
                
            <article class="paper-card" data-paper-id="papers_1252">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/fobVnj5ptYC8Yt5Y.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3763291" target="_blank" rel="noopener">Automatic Sampling for Discontinuities in Differentiable Shaders</a></h3>
                    <p class="authors">Yash Belhe, Ishit Mehta, Wesley Chang, Iliyan Georgiev, Michael Gharbi, Ravi Ramamoorthi</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3763291" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1252', 'url', 'https://dl.acm.org/doi/10.1145/3763291')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1252', 'abstract', 'We present a novel method to differentiate integrals of discontinuous functions, which are common in inverse graphics, computer vision, and machine learning applications. Previous methods either require specialized routines to sample the discontinuous boundaries of predetermined primitives, or use reparameterization techniques that suffer from high variance. In contrast, our method handles general discontinuous functions, expressed as shader programs, without requiring manually specified boundary sampling routines. We achieve this through a program transformation that converts discontinuous functions into piecewise constant ones, enabling efficient boundary sampling through a novel segment snapping technique, and accurate derivatives at the boundary by simply comparing values on both sides of the discontinuity. Our method handles both explicit boundaries (polygons, ellipses, B√©zier curves) and implicit ones (neural networks, noise-based functions, swept surfaces). We demonstrate that our system supports a wide range of applications, including painterly rendering, raster image fitting, constructive solid geometry, swept surfaces, mosaicing, and ray marching.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">We present a novel method to differentiate integrals of discontinuous functions, which are common in inverse graphics, computer vision, and machine learning applications. Previous methods either require specialized routines to sample the discontinuous boundaries of predetermined primitives, or use reparameterization techniques that suffer from high variance. In contrast, our method handles general discontinuous functions, expressed as shader programs, without requiring manually specified boundary sampling routines. We achieve this through a program transformation that converts discontinuous functions into piecewise constant ones, enabling efficient boundary sampling through a novel segment snapping technique, and accurate derivatives at the boundary by simply comparing values on both sides of the discontinuity. Our method handles both explicit boundaries (polygons, ellipses, B√©zier curves) and implicit ones (neural networks, noise-based functions, swept surfaces). We demonstrate that our system supports a wide range of applications, including painterly rendering, raster image fitting, constructive solid geometry, swept surfaces, mosaicing, and ray marching.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1435">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/3ntGXc4BbDfMAB1D.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3763305" target="_blank" rel="noopener">Differentiable Light Transport with Gaussian Surfels via Adapted Radiosity for Efficient Relighting and Geometry Reconstruction</a></h3>
                    <p class="authors">Kaiwen Jiang, Jia-Mu Sun, Zilu Li, Dan Wang, Tzu-Mao Li, Ravi Ramamoorthi</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3763305" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1435', 'url', 'https://dl.acm.org/doi/10.1145/3763305')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1435', 'abstract', 'Radiance fields have gained tremendous success with applications ranging from novel view synthesis to geometry reconstruction, especially with the advent of Gaussian splatting. However, they sacrifice modeling of material reflective properties and lighting conditions, leading to significant geometric ambiguities and the inability to easily perform relighting. One way to address these limitations is to incorporate physically-based rendering, but it has been prohibitively expensive to include full global illumination within the inner loop of the optimization. Therefore, previous works adopt simplifications that make the whole optimization with global illumination effects efficient but less accurate. In this work, we adopt Gaussian surfels as the primitives and build an efficient framework for differentiable light transport, inspired from the classic radiosity theory. The whole framework operates in the coefficient space of spherical harmonics, enabling both diffuse and specular materials. We extend the classic radiosity into non-binary visibility and semi-opaque primitives, propose novel solvers to efficiently solve the light transport, and derive the backward pass for gradient optimizations, which is more efficient than auto-differentiation. During inference, we achieve view-independent rendering where light transport need not be recomputed under viewpoint changes, enabling hundreds of FPS for global illumination effects, including view-dependent reflections using a spherical harmonics representation. Through extensive qualitative and quantitative experiments, we demonstrate superior geometry reconstruction, view synthesis and relighting than previous inverse rendering baselines, or data-driven baselines given relatively sparse datasets with known or unknown lighting conditions.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">Radiance fields have gained tremendous success with applications ranging from novel view synthesis to geometry reconstruction, especially with the advent of Gaussian splatting. However, they sacrifice modeling of material reflective properties and lighting conditions, leading to significant geometric ambiguities and the inability to easily perform relighting. One way to address these limitations is to incorporate physically-based rendering, but it has been prohibitively expensive to include full global illumination within the inner loop of the optimization. Therefore, previous works adopt simplifications that make the whole optimization with global illumination effects efficient but less accurate. In this work, we adopt Gaussian surfels as the primitives and build an efficient framework for differentiable light transport, inspired from the classic radiosity theory. The whole framework operates in the coefficient space of spherical harmonics, enabling both diffuse and specular materials. We extend the classic radiosity into non-binary visibility and semi-opaque primitives, propose novel solvers to efficiently solve the light transport, and derive the backward pass for gradient optimizations, which is more efficient than auto-differentiation. During inference, we achieve view-independent rendering where light transport need not be recomputed under viewpoint changes, enabling hundreds of FPS for global illumination effects, including view-dependent reflections using a spherical harmonics representation. Through extensive qualitative and quantitative experiments, we demonstrate superior geometry reconstruction, view synthesis and relighting than previous inverse rendering baselines, or data-driven baselines given relatively sparse datasets with known or unknown lighting conditions.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1757">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/zx4g4rU4E9cyvxwM.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763918" target="_blank" rel="noopener">Spectral Reconstruction with Uncertainty Quantification via Differentiable Rendering and Null-Space Sampling</a></h3>
                    <p class="authors">Mengqi Xia, Bai Xue, Rachel Liang, Holly Rushmeier</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763918" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1757', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763918')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1757', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1766">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/ZpAfxvBuXtrehpqk.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763920" target="_blank" rel="noopener">Adaptive Neural Kernels for Gradient-domain Rendering</a></h3>
                    <p class="authors">Matthieu Josse, Joey Litalien, Adrien Gruson</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763920" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1766', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763920')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1766', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1309">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/2o96B6rEP58bLNnv.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3763297" target="_blank" rel="noopener">Generalized Unbiased Reconstruction for Gradient-Domain Rendering</a></h3>
                    <p class="authors">Difei Yan, Zengyu Li, Lifan Wu, Kun Xu</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3763297" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1309', 'url', 'https://dl.acm.org/doi/10.1145/3763297')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1309', 'abstract', 'Gradient-domain rendering estimates image-space gradients using correlated sampling, which can be combined with color information to reconstruct smoother and less noisy images. While simple ‚Ñí 2 reconstruction is unbiased, it often leads to visible artifacts. In contrast, most recent reconstruction methods based on learned or handcrafted techniques improve visual quality but introduce bias, leaving the development of practically unbiased reconstruction approaches relatively underexplored. In this work, we propose a generalized framework for unbiased reconstruction in gradient-domain rendering. We first derive the unbiasedness condition under a general formulation that linearly combines pixel colors and gradients. Based on this unbiasedness condition, we design a practical algorithm 1 that minimizes image variance while strictly satisfying unbiasedness. Experimental results demonstrate that our method not only guarantees unbiasedness but also achieves superior quality compared to existing unbiased and slightly biased reconstruction methods.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">Gradient-domain rendering estimates image-space gradients using correlated sampling, which can be combined with color information to reconstruct smoother and less noisy images. While simple ‚Ñí 2 reconstruction is unbiased, it often leads to visible artifacts. In contrast, most recent reconstruction methods based on learned or handcrafted techniques improve visual quality but introduce bias, leaving the development of practically unbiased reconstruction approaches relatively underexplored. In this work, we propose a generalized framework for unbiased reconstruction in gradient-domain rendering. We first derive the unbiasedness condition under a general formulation that linearly combines pixel colors and gradients. Based on this unbiasedness condition, we design a practical algorithm 1 that minimizes image variance while strictly satisfying unbiasedness. Experimental results demonstrate that our method not only guarantees unbiasedness but also achieves superior quality compared to existing unbiased and slightly biased reconstruction methods.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            </div>
        </section>
        
        <section class="session">
            <div class="session-header">
                <h2>Perception and Performance in AR/VR Systems</h2>
                <span class="session-count">6 papers</span>
            </div>
            <div class="papers-grid">
                
            <article class="paper-card" data-paper-id="papers_1073">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/PLTcPDwGQidgBMn1.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763824" target="_blank" rel="noopener">Supra-threshold Contrast Perception in Augmented Reality</a></h3>
                    <p class="authors">Dongyeon Kim, Maliha Ashraf, Alexandre Chapiro, Rafa≈Ç K. Mantiuk</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763824" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1073', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763824')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1073', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_2458">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/6qxchM4PLSYp9RqE.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3764003" target="_blank" rel="noopener">Vertical Binocular Misalignment in AR Impairs Reading Performance</a></h3>
                    <p class="authors">Daniel Gurman, Daniel P. Spiegel, Kevin W. Rio</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3764003" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_2458', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3764003')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_2458', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1287">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/WB8qFsEs6ypb7C7w.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3763294" target="_blank" rel="noopener">Modeling and Exploiting the Time Course of Chromatic Adaptation for Display Power Optimizations in Virtual Reality</a></h3>
                    <p class="authors">Ethan Chen, Sushant Kondguli, Carl Marshall, Yuhao Zhu</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3763294" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1287', 'url', 'https://dl.acm.org/doi/10.1145/3763294')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1287', 'abstract', 'We introduce a gaze-tracking-free method to reduce OLED display power consumption in VR with minimal perceptual impact. This technique exploits the time course of chromatic adaptation, the human visual system\'s ability to maintain stable color perception under changing illumination. To that end, we propose a novel psychophysical paradigm that models how human adaptation state changes with the scene illuminant. We exploit this model to compute an optimal illuminant shift trajectory, controlling the rate and extent of illumination change, to reduce display power under a given perceptual loss budget. Our technique significantly improves the perceptual quality over prior work that applies illumination shifts instantaneously. Our technique can also be combined with prior work on luminance dimming to reduce display power by 31% with no statistical loss of perceptual quality.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">We introduce a gaze-tracking-free method to reduce OLED display power consumption in VR with minimal perceptual impact. This technique exploits the time course of chromatic adaptation, the human visual system&#x27;s ability to maintain stable color perception under changing illumination. To that end, we propose a novel psychophysical paradigm that models how human adaptation state changes with the scene illuminant. We exploit this model to compute an optimal illuminant shift trajectory, controlling the rate and extent of illumination change, to reduce display power under a given perceptual loss budget. Our technique significantly improves the perceptual quality over prior work that applies illumination shifts instantaneously. Our technique can also be combined with prior work on luminance dimming to reduce display power by 31% with no statistical loss of perceptual quality.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1173">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/iGp1eWw5Yx66iprz.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763837" target="_blank" rel="noopener">Performance Analysis of Catch-Up Eye Movements in Visual Tracking</a></h3>
                    <p class="authors">Jenna Kang, Budmonde Duinkharjav, Niall Williams, Qi Sun</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763837" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1173', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763837')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1173', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_2156">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/myfsUVtaUwbPXibx.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3763356" target="_blank" rel="noopener">Glare Pattern Depiction: High-Fidelity Physical Computation and Physiologically-Inspired Visual Response</a></h3>
                    <p class="authors">Yuxiang Sun, Gladimir Baranoski</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3763356" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_2156', 'url', 'https://dl.acm.org/doi/10.1145/3763356')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_2156', 'abstract', 'When observing an intense light source, humans perceive dense radiating spikes known as glare/starburst patterns. These patterns are frequently used in computer graphics applications to enhance the perception of brightness (e.g., in games and films). Previous works have computed the physical energy distribution of glare patterns under daytime conditions using approximations like Fresnel diffraction. These techniques are capable of producing visually believable results, particularly when the pupil remains small. However, they are insufficient under nighttime conditions, when the pupil is significantly dilated and the assumptions behind the approximations no longer hold. To address this, we employ the Rayleigh-Sommerfeld diffraction solution, from which Fresnel diffraction is derived as an approximation, as our baseline reference. In pursuit of performance and visual quality, we also employ Ochoa\'s approximation and the Chirp Z transform to efficiently generate high-resolution results for computer graphics applications. By also taking into account background illumination and certain physiological characteristics of the human photoreceptor cells, particularly the visual threshold of light stimulus, we propose a framework capable of producing plausible visual depictions of glare patterns for both daytime and nighttime scenes.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">When observing an intense light source, humans perceive dense radiating spikes known as glare/starburst patterns. These patterns are frequently used in computer graphics applications to enhance the perception of brightness (e.g., in games and films). Previous works have computed the physical energy distribution of glare patterns under daytime conditions using approximations like Fresnel diffraction. These techniques are capable of producing visually believable results, particularly when the pupil remains small. However, they are insufficient under nighttime conditions, when the pupil is significantly dilated and the assumptions behind the approximations no longer hold. To address this, we employ the Rayleigh-Sommerfeld diffraction solution, from which Fresnel diffraction is derived as an approximation, as our baseline reference. In pursuit of performance and visual quality, we also employ Ochoa&#x27;s approximation and the Chirp Z transform to efficiently generate high-resolution results for computer graphics applications. By also taking into account background illumination and certain physiological characteristics of the human photoreceptor cells, particularly the visual threshold of light stimulus, we propose a framework capable of producing plausible visual depictions of glare patterns for both daytime and nighttime scenes.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1905">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/f5wg9eynndBQALEN.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3763340" target="_blank" rel="noopener">MILO: A Lightweight Perceptual Quality Metric for Image and Latent-Space Optimization</a></h3>
                    <p class="authors">Ugur Cogalan, Mojtaba Bemana, Karol Myszkowski, Hans-Peter Seidel, Colin Groth</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3763340" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1905', 'url', 'https://dl.acm.org/doi/10.1145/3763340')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1905', 'abstract', 'We present MILO (Metric for Image- and Latent-space Optimization), a lightweight, multiscale, perceptual metric for full-reference image quality assessment (FR-IQA). MILO is trained using pseudo-MOS (Mean Opinion Score) supervision, in which reproducible distortions are applied to diverse images and scored via an ensemble of recent quality metrics that account for visual masking effects. This approach enables accurate learning without requiring large-scale human-labeled datasets. Despite its compact architecture, MILO outperforms existing metrics across standard FR-IQA benchmarks and offers fast inference suitable for real-time applications. Beyond quality prediction, we demonstrate the utility of MILO as a perceptual loss in both image and latent domains. In particular, we show that spatial masking modeled by MILO, when applied to latent representations from a VAE encoder within Stable Diffusion, enables efficient and perceptually aligned optimization. By combining spatial masking with a curriculum learning strategy, we first process perceptually less relevant regions before progressively shifting the optimization to more visually distorted areas. This strategy leads to significantly improved performance in tasks like denoising, super-resolution, and face restoration, while also reducing computational overhead. MILO thus functions as both a state-of-the-art image quality metric and as a practical tool for perceptual optimization in generative pipelines.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">We present MILO (Metric for Image- and Latent-space Optimization), a lightweight, multiscale, perceptual metric for full-reference image quality assessment (FR-IQA). MILO is trained using pseudo-MOS (Mean Opinion Score) supervision, in which reproducible distortions are applied to diverse images and scored via an ensemble of recent quality metrics that account for visual masking effects. This approach enables accurate learning without requiring large-scale human-labeled datasets. Despite its compact architecture, MILO outperforms existing metrics across standard FR-IQA benchmarks and offers fast inference suitable for real-time applications. Beyond quality prediction, we demonstrate the utility of MILO as a perceptual loss in both image and latent domains. In particular, we show that spatial masking modeled by MILO, when applied to latent representations from a VAE encoder within Stable Diffusion, enables efficient and perceptually aligned optimization. By combining spatial masking with a curriculum learning strategy, we first process perceptually less relevant regions before progressively shifting the optimization to more visually distorted areas. This strategy leads to significantly improved performance in tasks like denoising, super-resolution, and face restoration, while also reducing computational overhead. MILO thus functions as both a state-of-the-art image quality metric and as a practical tool for perceptual optimization in generative pipelines.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            </div>
        </section>
        
        <section class="session">
            <div class="session-header">
                <h2>4D Gaussian Splatting for Dynamic Scene Reconstruction</h2>
                <span class="session-count">6 papers</span>
            </div>
            <div class="papers-grid">
                
            <article class="paper-card" data-paper-id="papers_1094">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/uAeFxVi91vUwCPQ6.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763829" target="_blank" rel="noopener">TrackerSplat: Exploiting Point Tracking for Fast and Robust Dynamic 3D Gaussians Reconstruction</a></h3>
                    <p class="authors">Daheng Yin, Isaac Ding, Yili Jin, Jianxin Shi, Jiangchuan Liu</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763829" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1094', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763829')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1094', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1336">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/7bPx1gd4FtQ6KNDA.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763858" target="_blank" rel="noopener">Clustered Error Correction with Grouped 4D Gaussian Splatting</a></h3>
                    <p class="authors">Taeho Kang, Jaeyeon Park, Kyungjin Lee, Youngki Lee</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763858" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1336', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763858')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1336', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1616">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/3fWTfcwEpNK2MZ9Q.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763898" target="_blank" rel="noopener">Anchored 4D Gaussian Splatting for Dynamic Novel View Synthesis</a></h3>
                    <p class="authors">Yilong Li, Bo Pang, Yisong Chen, Guoping Wang</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763898" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1616', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763898')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1616', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1687">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/Gyymrf9gJVSDxpdM.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763910" target="_blank" rel="noopener">Prior-Enhanced Gaussian Splatting for Dynamic Scene Reconstruction from Casual Video</a></h3>
                    <p class="authors">Meng-Li Shih, Ying-Huan Chen, Yu-Lun Liu, Brian Curless</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763910" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1687', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763910')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1687', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1682">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/g84GM8vUj3SjUj8T.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763908" target="_blank" rel="noopener">4DSloMo: 4D Reconstruction for High Speed Scene with Asynchronous Capture</a></h3>
                    <p class="authors">Yutian Chen, Shi Guo, Tianshuo Yang, Lihe Ding, Xiuyuan Yu, Jinwei Gu</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763908" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1682', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763908')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1682', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1986">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/DfdkqdMXBZs834hH.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3763343" target="_blank" rel="noopener">Split4D: Decomposed 4D Scene Reconstruction Without Video Segmentation</a></h3>
                    <p class="authors">Yongzhen Hu, Yihui Yang, Haotong Lin, Yifan Wang, Junting Dong, Yifu Deng</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3763343" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1986', 'url', 'https://dl.acm.org/doi/10.1145/3763343')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1986', 'abstract', 'This paper addresses the problem of decomposed 4D scene reconstruction from multi-view videos. Recent methods achieve this by lifting video segmentation results to a 4D representation through differentiable rendering techniques. Therefore, they heavily rely on the quality of video segmentation maps, which are often unstable, leading to unreliable reconstruction results. To overcome this challenge, our key idea is to represent the decomposed 4D scene with the Freetime FeatureGS and design a streaming feature learning strategy to accurately recover it from per-image segmentation maps, eliminating the need for video segmentation. Freetime FeatureGS models the dynamic scene as a set of Gaussian primitives with learnable features and linear motion ability, allowing them to move to neighboring regions over time. We apply a contrastive loss to Freetime FeatureGS, forcing primitive features to be close or far apart based on whether their projections belong to the same instance in the 2D segmentation map. As our Gaussian primitives can move across time, it naturally extends the feature learning to the temporal dimension, achieving 4D segmentation. Furthermore, we sample observations for training in a temporally ordered manner, enabling the streaming propagation of features over time and effectively avoiding local minima during the optimization process. Experimental results on several datasets show that the reconstruction quality of our method outperforms recent methods by a large margin.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">This paper addresses the problem of decomposed 4D scene reconstruction from multi-view videos. Recent methods achieve this by lifting video segmentation results to a 4D representation through differentiable rendering techniques. Therefore, they heavily rely on the quality of video segmentation maps, which are often unstable, leading to unreliable reconstruction results. To overcome this challenge, our key idea is to represent the decomposed 4D scene with the Freetime FeatureGS and design a streaming feature learning strategy to accurately recover it from per-image segmentation maps, eliminating the need for video segmentation. Freetime FeatureGS models the dynamic scene as a set of Gaussian primitives with learnable features and linear motion ability, allowing them to move to neighboring regions over time. We apply a contrastive loss to Freetime FeatureGS, forcing primitive features to be close or far apart based on whether their projections belong to the same instance in the 2D segmentation map. As our Gaussian primitives can move across time, it naturally extends the feature learning to the temporal dimension, achieving 4D segmentation. Furthermore, we sample observations for training in a temporally ordered manner, enabling the streaming propagation of features over time and effectively avoiding local minima during the optimization process. Experimental results on several datasets show that the reconstruction quality of our method outperforms recent methods by a large margin.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            </div>
        </section>
        
        <section class="session">
            <div class="session-header">
                <h2>Garment & Cloth Modeling, Simulation and Rendering</h2>
                <span class="session-count">5 papers</span>
            </div>
            <div class="papers-grid">
                
            <article class="paper-card" data-paper-id="papers_1028">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/4HTXfpbBhz8uja72.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3763271" target="_blank" rel="noopener">GarmageNet: A Multimodal Generative Framework for Sewing Pattern Design and Generic Garment Modeling</a></h3>
                    <p class="authors">Siran Li, Ruiyang Liu, Chen Liu, Zhendong Wang, Gaofeng He, Yong-Lu Li</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3763271" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1028', 'url', 'https://dl.acm.org/doi/10.1145/3763271')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1028', 'abstract', 'Realistic digital garment modeling remains a labor-intensive task due to the intricate process of translating 2D sewing patterns into high-fidelity, simulation-ready 3D garments. We introduce GarmageNet , a unified generative framework that automates the creation of 2D sewing patterns, the construction of sewing relationships, and the synthesis of 3D garment initializations compatible with physics-based simulation. Central to our approach is Garmage , a novel garment representation that encodes each panel as a structured geometry image, effectively bridging the semantic and geometric gap between 2D structural patterns and 3D garment geometries. Followed by GarmageNet , a latent diffusion transformer to synthesize panel-wise geometry images and GarmageJigsaw , a neural module for predicting point-to-point sewing connections along panel contours. To support training and evaluation, we build GarmageSet , a large-scale dataset comprising 14,801 professionally designed garments with detailed structural and style annotations. Our method demonstrates versatility and efficacy across multiple application scenarios, including scalable garment generation from multi-modal design concepts (text prompts, sketches, photographs), automatic modeling from raw flat sewing patterns, pattern recovery from unstructured point clouds, and progressive garment editing using conventional instructions, laying the foundation for fully automated, production-ready pipelines in digital fashion. Refer to our project page for open-sourced code and dataset.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">Realistic digital garment modeling remains a labor-intensive task due to the intricate process of translating 2D sewing patterns into high-fidelity, simulation-ready 3D garments. We introduce GarmageNet , a unified generative framework that automates the creation of 2D sewing patterns, the construction of sewing relationships, and the synthesis of 3D garment initializations compatible with physics-based simulation. Central to our approach is Garmage , a novel garment representation that encodes each panel as a structured geometry image, effectively bridging the semantic and geometric gap between 2D structural patterns and 3D garment geometries. Followed by GarmageNet , a latent diffusion transformer to synthesize panel-wise geometry images and GarmageJigsaw , a neural module for predicting point-to-point sewing connections along panel contours. To support training and evaluation, we build GarmageSet , a large-scale dataset comprising 14,801 professionally designed garments with detailed structural and style annotations. Our method demonstrates versatility and efficacy across multiple application scenarios, including scalable garment generation from multi-modal design concepts (text prompts, sketches, photographs), automatic modeling from raw flat sewing patterns, pattern recovery from unstructured point clouds, and progressive garment editing using conventional instructions, laying the foundation for fully automated, production-ready pipelines in digital fashion. Refer to our project page for open-sourced code and dataset.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_2192">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/AuzX7uYW7Q7KgDh1.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3763359" target="_blank" rel="noopener">Realistic Cloth Rendering with a Ray-Wave Hybrid Shading Model</a></h3>
                    <p class="authors">Yunchen Yu, Bruce Walter, Steve Marschner, Andrea Weidlich</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3763359" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_2192', 'url', 'https://dl.acm.org/doi/10.1145/3763359')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_2192', 'abstract', 'Realistic fabric rendering is still a significant challenge due to their complex structures and varying fiber properties. We present a new fabric shading technique, which models both reflection and transmission using a hybrid of ray and wave optics methods, grounded in simulation data. We target fabrics woven from yarns, each formed by twisting together one or more plies, which further contain twisted fibers. Our model is based on simulations that predict the scattering of a narrow Gaussian beam by a single ply. Comparing results from full-wave simulations and path tracing, we found that ray optics can accurately simulate the average far field scattering from an ensemble of plies, but not the variation among individual ply instances, and ray tracing overlooks important diffraction effects. Following these observations, our model is built from ray simulations performed for many ply instances, with simulation data fitted by Gaussian mixtures to be used during rendering. Wave simulations are used to calibrate noise functions that account for instance-to-instance variation, and an aperture diffraction model is used to handle light passing between plies and yarns. The result is a hybrid model capable of producing realistic appearance and highlight structure in fabrics, while capturing spatial break-ups and irregularities and simulating the subtle color shifts and blurriness that occur in transmission. We validate our results by comparing rendered images with photographs, demonstrating the effectiveness of our approach in achieving realistic cloth rendering.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">Realistic fabric rendering is still a significant challenge due to their complex structures and varying fiber properties. We present a new fabric shading technique, which models both reflection and transmission using a hybrid of ray and wave optics methods, grounded in simulation data. We target fabrics woven from yarns, each formed by twisting together one or more plies, which further contain twisted fibers. Our model is based on simulations that predict the scattering of a narrow Gaussian beam by a single ply. Comparing results from full-wave simulations and path tracing, we found that ray optics can accurately simulate the average far field scattering from an ensemble of plies, but not the variation among individual ply instances, and ray tracing overlooks important diffraction effects. Following these observations, our model is built from ray simulations performed for many ply instances, with simulation data fitted by Gaussian mixtures to be used during rendering. Wave simulations are used to calibrate noise functions that account for instance-to-instance variation, and an aperture diffraction model is used to handle light passing between plies and yarns. The result is a hybrid model capable of producing realistic appearance and highlight structure in fabrics, while capturing spatial break-ups and irregularities and simulating the subtle color shifts and blurriness that occur in transmission. We validate our results by comparing rendered images with photographs, demonstrating the effectiveness of our approach in achieving realistic cloth rendering.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1386">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/VHkcvLmfAci3wERP.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763868" target="_blank" rel="noopener">Progressive Outfit Assembly and Instantaneous Pose Transfer</a></h3>
                    <p class="authors">Dewen Guo, Zhendong Wang, Zegao Liu, Sheng Li, Guoping Wang, Yin Yang</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763868" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1386', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763868')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1386', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1533">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/chTxWyJKSRQzHVZC.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763890" target="_blank" rel="noopener">Neighbor-Aware Data-Driven Relaxation of Stitch Mesh Models for Knits</a></h3>
                    <p class="authors">Yura Hwang, Jenny Han Lin, Jerry Hsu, Benjamin Mastripolito, James McCann, Cem Yuksel</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763890" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1533', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763890')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1533', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_2212">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/7wuBuREhdgwAccUC.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763989" target="_blank" rel="noopener">A Nonconforming Formulation of Cloth</a></h3>
                    <p class="authors">Elias Gueidon, Maurizio M. Chiaramonte</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763989" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_2212', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763989')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_2212', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            </div>
        </section>
        
        <section class="session">
            <div class="session-header">
                <h2>3D Reconstruction & Rendering</h2>
                <span class="session-count">6 papers</span>
            </div>
            <div class="papers-grid">
                
            <article class="paper-card" data-paper-id="papers_1453">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/CL4PhrunUAevLwmb.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763878" target="_blank" rel="noopener">EGG-Fusion: Efficient 3D Reconstruction with Geometry-aware Gaussian Surfel on the Fly</a></h3>
                    <p class="authors">Xiaokun Pan, Zhenzhe Li, Zhichao Ye, Hongjia Zhai, Guofeng Zhang</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763878" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1453', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763878')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1453', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1849">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/WzXhabmCMnUjiqq6.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763938" target="_blank" rel="noopener">Inverse Radiative Transport for Infrared Scenes with Gaussian Primitives</a></h3>
                    <p class="authors">Zhenyuan Desmond Liu, Bharath Seshadri, George Kopanas, Bernd Bickel</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763938" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1849', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763938')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1849', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1375">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/fKVzpoXEDd9JkyGh.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763865" target="_blank" rel="noopener">Efficient Object Reconstruction with Differentiable Area Light Shading</a></h3>
                    <p class="authors">Yaoan Gao, Jiamin Xu, James Tompkin, Qi Wang, Zheng Dong, Hujun Bao</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763865" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1375', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763865')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1375', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1611">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/L4LjSTnfGpcMkmPa.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3763317" target="_blank" rel="noopener">WATER: Watertight Tessellation for Real-Time Pixel-Accurate Rendering of Large-Scale Surfaces</a></h3>
                    <p class="authors">Yajun Zeng, Yang Lu, Cong Chen, Ligang Liu</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3763317" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1611', 'url', 'https://dl.acm.org/doi/10.1145/3763317')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1611', 'abstract', 'Watertight tessellation is essential for real-time rendering of large-scale surfaces, particularly for Non-Uniform Rational B-Splines (NURBS) and Catmull-Clark Subdivision (CCS) surfaces. We present WATER, a software-based framework that delivers watertight, non-uniform tessellation with pixel-level accuracy at real-time frame rates. Unlike fixed-function hardware tessellation, WATER adopts a fully GPU-driven pipeline with cache-friendly design and novel algorithms, offering greater flexibility, scalability, and performance. Under our framework, a 2√ó‚Äì3√ó speedup over hardware tessellation is achieved for bi-3 B√©zier surfaces, while bi-7 B√©zier surfaces exhibit a 7√ó‚Äì11√ó improvement. Compared to ETER [Xiong et al. 2023], our method achieves 1.3√ó‚Äì2.1√ó faster rendering and 52%‚Äì72% lower memory usage under the same non-watertight uniform pattern. When enforcing watertightness, a moderate overhead of 23%‚Äì51% is incurred. With its advantages in quality, efficiency, and adaptability, WATER provides a compelling alternative for industrial-scale rendering tasks requiring watertightness.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">Watertight tessellation is essential for real-time rendering of large-scale surfaces, particularly for Non-Uniform Rational B-Splines (NURBS) and Catmull-Clark Subdivision (CCS) surfaces. We present WATER, a software-based framework that delivers watertight, non-uniform tessellation with pixel-level accuracy at real-time frame rates. Unlike fixed-function hardware tessellation, WATER adopts a fully GPU-driven pipeline with cache-friendly design and novel algorithms, offering greater flexibility, scalability, and performance. Under our framework, a 2√ó‚Äì3√ó speedup over hardware tessellation is achieved for bi-3 B√©zier surfaces, while bi-7 B√©zier surfaces exhibit a 7√ó‚Äì11√ó improvement. Compared to ETER [Xiong et al. 2023], our method achieves 1.3√ó‚Äì2.1√ó faster rendering and 52%‚Äì72% lower memory usage under the same non-watertight uniform pattern. When enforcing watertightness, a moderate overhead of 23%‚Äì51% is incurred. With its advantages in quality, efficiency, and adaptability, WATER provides a compelling alternative for industrial-scale rendering tasks requiring watertightness.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1827">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/UwChC8TJ9HLdn86j.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763932" target="_blank" rel="noopener">GigaSLAM: Large-Scale Monocular SLAM with Hierarchical Gaussian Splats</a></h3>
                    <p class="authors">Kai Deng, Yigong Zhang, Jian Yang, Jin Xie</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763932" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1827', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763932')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1827', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_2227">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/b8cxsCVy4PUNQyLx.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3763361" target="_blank" rel="noopener">Artifact-Resilient Real-Time Holography</a></h3>
                    <p class="authors">Victor Chu, Oscar Pueyo-Ciutad, Ethan Tseng, Florian Schiffers, Grace Kuo, Nathan Matsuda</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3763361" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_2227', 'url', 'https://dl.acm.org/doi/10.1145/3763361')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_2227', 'abstract', 'Holographic near-eye displays promise unparalleled depth cues, high-resolution imagery, and realistic three-dimensional parallax at a compact form factor, making them promising candidates for emerging augmented and virtual reality systems. However, existing holographic display methods often assume ideal viewing conditions and overlook real-world factors such as eye floaters and eyelashes‚Äîobstructions that can severely degrade perceived image quality. In this work, we propose a new metric that quantifies hologram resilience to artifacts and apply it to computer generated holography (CGH) optimization. We call this Artifact Resilient Holography (ARH). We begin by introducing a simulation method that models the effects of pre- and post-pupil obstructions on holographic displays. Our analysis reveals that eyebox regions dominated by low frequencies‚Äîproduced especially by the smooth-phase holograms broadly adopted in recent holography work‚Äîare vulnerable to visual degradation from dynamic obstructions such as floaters and eyelashes. In contrast, random phase holograms spread energy more uniformly across the eyebox spectrum, enabling them to diffract around obstructions without producing prominent artifacts. By characterizing a random phase eyebox using the Rayleigh Distribution, we derive a differentiable metric in the eyebox domain. We then apply this metric to train a real-time neural network-based phase generator, enabling it to produce artifact-resilient 3D holograms that preserve visual fidelity across a range of practical viewing conditions‚Äîenhancing both robustness and user interactivity.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">Holographic near-eye displays promise unparalleled depth cues, high-resolution imagery, and realistic three-dimensional parallax at a compact form factor, making them promising candidates for emerging augmented and virtual reality systems. However, existing holographic display methods often assume ideal viewing conditions and overlook real-world factors such as eye floaters and eyelashes‚Äîobstructions that can severely degrade perceived image quality. In this work, we propose a new metric that quantifies hologram resilience to artifacts and apply it to computer generated holography (CGH) optimization. We call this Artifact Resilient Holography (ARH). We begin by introducing a simulation method that models the effects of pre- and post-pupil obstructions on holographic displays. Our analysis reveals that eyebox regions dominated by low frequencies‚Äîproduced especially by the smooth-phase holograms broadly adopted in recent holography work‚Äîare vulnerable to visual degradation from dynamic obstructions such as floaters and eyelashes. In contrast, random phase holograms spread energy more uniformly across the eyebox spectrum, enabling them to diffract around obstructions without producing prominent artifacts. By characterizing a random phase eyebox using the Rayleigh Distribution, we derive a differentiable metric in the eyebox domain. We then apply this metric to train a real-time neural network-based phase generator, enabling it to produce artifact-resilient 3D holograms that preserve visual fidelity across a range of practical viewing conditions‚Äîenhancing both robustness and user interactivity.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            </div>
        </section>
        
        <section class="session">
            <div class="session-header">
                <h2>Animation, Simulation & Deformation</h2>
                <span class="session-count">6 papers</span>
            </div>
            <div class="papers-grid">
                
            <article class="paper-card" data-paper-id="papers_2018">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/oLaeUH3xHe1XZvCN.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3763344" target="_blank" rel="noopener">Numerical Homogenization of Sand from Grain-level Simulations</a></h3>
                    <p class="authors">Yi-Lu Chen, Micka√´l Ly, Chris Wojtan</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3763344" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_2018', 'url', 'https://dl.acm.org/doi/10.1145/3763344')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_2018', 'abstract', 'The realistic simulation of sand, soil, powders, rubble piles, and large collections of rigid bodies is a common and important problem in the fields of computer graphics, computational physics, and engineering. Direct simulation of these individual bodies quickly becomes expensive, so we often approximate the entire group as a continuum material that can be more easily computed using tools for solving partial differential equations, like the material point method (MPM). In this paper, we present a method for automatically extracting continuum material properties from a collection of rigid bodies. We use numerical homogenization with periodic boundary conditions to simulate an effectively infinite number of rigid bodies in contact. We then record the effective stress-strain relationships from these simulations and convert them into elastic properties and yield criteria for the continuum simulations. Our experiments validate existing theoretical models like the Mohr-Coulomb yield surface by extracting material behaviors from a collection of spheres in contact. We further generalize these existing models to more exotic materials derived from diverse and non-convex shapes. We observe complicated jamming behaviors from non-convex grains, and we introduce a new material model for materials with extremely high levels of internal friction and cohesion. We simulate these new continuum models using MPM with an improved return mapping technique. The end result is a complete system for turning an input rigid body simulation into an efficient continuum simulation with the same effective mechanical properties.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">The realistic simulation of sand, soil, powders, rubble piles, and large collections of rigid bodies is a common and important problem in the fields of computer graphics, computational physics, and engineering. Direct simulation of these individual bodies quickly becomes expensive, so we often approximate the entire group as a continuum material that can be more easily computed using tools for solving partial differential equations, like the material point method (MPM). In this paper, we present a method for automatically extracting continuum material properties from a collection of rigid bodies. We use numerical homogenization with periodic boundary conditions to simulate an effectively infinite number of rigid bodies in contact. We then record the effective stress-strain relationships from these simulations and convert them into elastic properties and yield criteria for the continuum simulations. Our experiments validate existing theoretical models like the Mohr-Coulomb yield surface by extracting material behaviors from a collection of spheres in contact. We further generalize these existing models to more exotic materials derived from diverse and non-convex shapes. We observe complicated jamming behaviors from non-convex grains, and we introduce a new material model for materials with extremely high levels of internal friction and cohesion. We simulate these new continuum models using MPM with an improved return mapping technique. The end result is a complete system for turning an input rigid body simulation into an efficient continuum simulation with the same effective mechanical properties.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_2138">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/XYGbWkEUsMGq5Thn.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763980" target="_blank" rel="noopener">Improving Curl Noise</a></h3>
                    <p class="authors">J. Andreas B√¶rentzen, Jon√†s Mart√≠nez, Jeppe Revall Frisvad, Sylvain Lefebvre</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763980" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_2138', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763980')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_2138', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_2309">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/pHZQpJRhm2WGZTNr.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763992" target="_blank" rel="noopener">Gaussian See, Gaussian Do: Semantic 3D Motion Transfer from Multiview Video</a></h3>
                    <p class="authors">Yarin Bekor, Gal Michael Harari, Or Perel, Or Litany</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763992" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_2309', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763992')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_2309', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1996">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/K2KgVm1Sktz1xqTh.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763961" target="_blank" rel="noopener">QMF-Blend: Quantized Matrix Factorization for Efficient Blendshape Compression</a></h3>
                    <p class="authors">Roman Fedotov, Brian Budge, Ladislav Kavan</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763961" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1996', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763961')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1996', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_2559">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/izmh9AYx5Pr5DT5n.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3764009" target="_blank" rel="noopener">AniMaker: Multi-Agent Animated Storytelling with MCTS-Driven Clip Generation</a></h3>
                    <p class="authors">Haoyuan Shi, Yunxin Li, Xinyu Chen, Longyue Wang, Baotian Hu, Min Zhang</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3764009" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_2559', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3764009')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_2559', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1537">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/HjE44PKCRDBS7vA1.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3763311" target="_blank" rel="noopener">Shape-aware Inertial Poser: Motion Tracking for Humans with Diverse Shapes Using Sparse Inertial Sensors</a></h3>
                    <p class="authors">Lu Yin, Ziying Shi, Yinghao Wu, Xinyu Yi, Feng Xu, Shihui Guo</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3763311" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1537', 'url', 'https://dl.acm.org/doi/10.1145/3763311')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1537', 'abstract', 'Human motion capture with sparse inertial sensors has gained significant attention recently. However, existing methods almost exclusively rely on a template adult body shape to model the training data, which poses challenges when generalizing to individuals with largely different body shapes (such as a child). This is primarily due to the variation in IMU-measured acceleration caused by changes in body shape. To fill this gap, we propose Shape-aware Inertial Poser (SAIP), the first solution considering body shape differences in sparse inertial-based motion capture. Specifically, we decompose the sensor measurements related to shape and pose in order to effectively model their joint correlations. Firstly, we train a regression model to transfer the IMU-measured accelerations of a real body to match the template adult body model, compensating for the shape-related sensor measurements. Then, we can easily follow the state-of-the-art methods to estimate the full body motions of the template-shaped body. Finally, we utilize a second regression model to map the joint velocities back to the real body, combined with a shape-aware physical optimization strategy to calculate global motions on the subject. Furthermore, our method relies on body shape awareness, introducing the first inertial shape estimation scheme. This is accomplished by modeling the shape-conditioned IMU-pose correlation using an MLP-based network. To validate the effectiveness of SAIP, we also present the first IMU motion capture dataset containing individuals of different body sizes. This dataset features 10 children and 10 adults, with heights ranging from 110 cm to 190 cm, and a total of 400 minutes of paired IMU-Motion samples. Extensive experimental results demonstrate that SAIP can effectively handle motion capture tasks for diverse body shapes. The code and dataset are available at https://github.com/yinlu5942/SAIP .')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">Human motion capture with sparse inertial sensors has gained significant attention recently. However, existing methods almost exclusively rely on a template adult body shape to model the training data, which poses challenges when generalizing to individuals with largely different body shapes (such as a child). This is primarily due to the variation in IMU-measured acceleration caused by changes in body shape. To fill this gap, we propose Shape-aware Inertial Poser (SAIP), the first solution considering body shape differences in sparse inertial-based motion capture. Specifically, we decompose the sensor measurements related to shape and pose in order to effectively model their joint correlations. Firstly, we train a regression model to transfer the IMU-measured accelerations of a real body to match the template adult body model, compensating for the shape-related sensor measurements. Then, we can easily follow the state-of-the-art methods to estimate the full body motions of the template-shaped body. Finally, we utilize a second regression model to map the joint velocities back to the real body, combined with a shape-aware physical optimization strategy to calculate global motions on the subject. Furthermore, our method relies on body shape awareness, introducing the first inertial shape estimation scheme. This is accomplished by modeling the shape-conditioned IMU-pose correlation using an MLP-based network. To validate the effectiveness of SAIP, we also present the first IMU motion capture dataset containing individuals of different body sizes. This dataset features 10 children and 10 adults, with heights ranging from 110 cm to 190 cm, and a total of 400 minutes of paired IMU-Motion samples. Extensive experimental results demonstrate that SAIP can effectively handle motion capture tasks for diverse body shapes. The code and dataset are available at https://github.com/yinlu5942/SAIP .</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            </div>
        </section>
        
        <section class="session">
            <div class="session-header">
                <h2>Neural Fields and Surface Reconstruction</h2>
                <span class="session-count">6 papers</span>
            </div>
            <div class="papers-grid">
                
            <article class="paper-card" data-paper-id="papers_1479">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/9Y7rVz1njzUxG5Jw.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763882" target="_blank" rel="noopener">Low-Rank Adaptation of Neural Fields</a></h3>
                    <p class="authors">Anh Truong, Ahmed H. Mahmoud, Mina Konakoviƒá Lukoviƒá, Justin Solomon</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763882" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1479', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763882')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1479', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1650">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/1Z8Y8afcadRzEkkV.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763901" target="_blank" rel="noopener">Spectral Prefiltering of Neural Fields</a></h3>
                    <p class="authors">Mustafa B. YALDIZ, Ishit Mehta, Nithin Raghavan, Andreas Meuleman, Tzu-Mao Li, Ravi Ramamoorthi</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763901" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1650', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763901')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1650', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1740">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/TKH91wvTz8Akm7Ss.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3763328" target="_blank" rel="noopener">Marching Neurons: Accurate Surface Extraction for Neural Implicit Shapes</a></h3>
                    <p class="authors">Christian Stippel, Felix Mujkanovic, Thomas Leimk√ºhler, Pedro Hermosilla</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3763328" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1740', 'url', 'https://dl.acm.org/doi/10.1145/3763328')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1740', 'abstract', 'Accurate surface geometry representation is crucial in 3D visual computing. Explicit representations, such as polygonal meshes, and implicit representations, like signed distance functions, each have distinct advantages, making efficient conversions between them increasingly important. Conventional surface extraction methods for implicit representations, such as the widely used Marching Cubes algorithm, rely on spatial decomposition and sampling, leading to inaccuracies due to fixed and limited resolution. We introduce a novel approach for analytically extracting surfaces from neural implicit functions. Our method operates natively in parallel and can navigate large neural architectures. By leveraging the fact that each neuron partitions the domain, we develop a depth-first traversal strategy to efficiently track the encoded surface. The resulting meshes faithfully capture the full geometric information from the network without ad-hoc spatial discretization, achieving unprecedented accuracy across diverse shapes and network architectures while maintaining competitive speed.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">Accurate surface geometry representation is crucial in 3D visual computing. Explicit representations, such as polygonal meshes, and implicit representations, like signed distance functions, each have distinct advantages, making efficient conversions between them increasingly important. Conventional surface extraction methods for implicit representations, such as the widely used Marching Cubes algorithm, rely on spatial decomposition and sampling, leading to inaccuracies due to fixed and limited resolution. We introduce a novel approach for analytically extracting surfaces from neural implicit functions. Our method operates natively in parallel and can navigate large neural architectures. By leveraging the fact that each neuron partitions the domain, we develop a depth-first traversal strategy to efficiently track the encoded surface. The resulting meshes faithfully capture the full geometric information from the network without ad-hoc spatial discretization, achieving unprecedented accuracy across diverse shapes and network architectures while maintaining competitive speed.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1830">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/TnQdYEfavtR4FjTu.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763933" target="_blank" rel="noopener">SOF: Sorted Opacity Fields for Fast Unbounded Surface Reconstruction</a></h3>
                    <p class="authors">Lukas Radl, Felix Windisch, Thomas Deixelberger, Jozef Hladky, Michael Steiner, Dieter Schmalstieg</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763933" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1830', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763933')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1830', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1884">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/Rw63mgX5m8v6ZoXS.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3763339" target="_blank" rel="noopener">MILo: Mesh-In-the-Loop Gaussian Splatting for Detailed and Efficient Surface Reconstruction</a></h3>
                    <p class="authors">Antoine Gu√©don, Diego Gomez, Nissim Maruani, Bingchen Gong, George Drettakis, Maks Ovsjanikov</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3763339" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1884', 'url', 'https://dl.acm.org/doi/10.1145/3763339')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1884', 'abstract', 'While recent advances in Gaussian Splatting have enabled fast reconstruction of high-quality 3D scenes from images, extracting accurate surface meshes remains a challenge. Current approaches extract the surface through costly post-processing steps, resulting in the loss of fine geometric details or requiring significant time and leading to very dense meshes with millions of vertices. More fundamentally, the a posteriori conversion from a volumetric to a surface representation limits the ability of the final mesh to preserve all geometric structures captured during training. We present MILo, a novel Gaussian Splatting framework that bridges the gap between volumetric and surface representations by differentiably extracting a mesh from the 3D Gaussians. We design a fully differentiable procedure that constructs the mesh‚Äîincluding both vertex locations and connectivity‚Äîat every iteration directly from the parameters of the Gaussians, which are the only quantities optimized during training. Our method introduces three key technical contributions: (1) a bidirectional consistency framework ensuring both representations‚ÄîGaussians and the extracted mesh‚Äîcapture the same underlying geometry during training; (2) an adaptive mesh extraction process performed at each training iteration, which uses Gaussians as differentiable pivots for Delaunay triangulation; (3) a novel method for computing signed distance values from the 3D Gaussians that enables precise surface extraction while avoiding geometric erosion. Our approach can reconstruct complete scenes, including backgrounds, with state-of-the-art quality while requiring an order of magnitude fewer mesh vertices than previous methods. Due to their light weight and empty interior, our meshes are well suited for downstream applications such as physics simulations and animation. The code for our approach and an online gallery are available at https://anttwo.github.io/milo/.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">While recent advances in Gaussian Splatting have enabled fast reconstruction of high-quality 3D scenes from images, extracting accurate surface meshes remains a challenge. Current approaches extract the surface through costly post-processing steps, resulting in the loss of fine geometric details or requiring significant time and leading to very dense meshes with millions of vertices. More fundamentally, the a posteriori conversion from a volumetric to a surface representation limits the ability of the final mesh to preserve all geometric structures captured during training. We present MILo, a novel Gaussian Splatting framework that bridges the gap between volumetric and surface representations by differentiably extracting a mesh from the 3D Gaussians. We design a fully differentiable procedure that constructs the mesh‚Äîincluding both vertex locations and connectivity‚Äîat every iteration directly from the parameters of the Gaussians, which are the only quantities optimized during training. Our method introduces three key technical contributions: (1) a bidirectional consistency framework ensuring both representations‚ÄîGaussians and the extracted mesh‚Äîcapture the same underlying geometry during training; (2) an adaptive mesh extraction process performed at each training iteration, which uses Gaussians as differentiable pivots for Delaunay triangulation; (3) a novel method for computing signed distance values from the 3D Gaussians that enables precise surface extraction while avoiding geometric erosion. Our approach can reconstruct complete scenes, including backgrounds, with state-of-the-art quality while requiring an order of magnitude fewer mesh vertices than previous methods. Due to their light weight and empty interior, our meshes are well suited for downstream applications such as physics simulations and animation. The code for our approach and an online gallery are available at https://anttwo.github.io/milo/.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_2168">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/EWydXcFeRxrD9Ze3.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763982" target="_blank" rel="noopener">RaRa Clipper: A Clipper for Gaussian Splatting Based on Ray Tracer and Rasterizer</a></h3>
                    <p class="authors">Da Li, Donggang Jia, Yousef Rajeh, Dominik Engel, Ivan Viola</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763982" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_2168', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763982')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_2168', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            </div>
        </section>
        
        <section class="session">
            <div class="session-header">
                <h2>Vector Graphics & Sketches</h2>
                <span class="session-count">6 papers</span>
            </div>
            <div class="papers-grid">
                
            <article class="paper-card" data-paper-id="papers_1654">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/RDyDUVuagP2GCj2Y.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763902" target="_blank" rel="noopener">Capturing Non-Linear Human Perspective in Line Drawings</a></h3>
                    <p class="authors">Jinfan Yang, Leo Foord-Kelcey, Suzuran Takikawa, Nicholas Vining, Niloy Mitra, Alla Sheffer</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763902" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1654', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763902')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1654', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1086">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/RQCJucoacvzDdEff.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763828" target="_blank" rel="noopener">AutoSketch: VLM-assisted Style-Aware Vector Sketch Completion</a></h3>
                    <p class="authors">Hsiao-Yuan Chin, I-Chao Shen, Yi-Ting Chiu, Ariel Shamir, Bing-Yu Chen</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763828" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1086', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763828')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1086', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1500">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/QJuYiiXPcXQUU32H.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3763307" target="_blank" rel="noopener">KISSColor: Kinetic and Intuitive Stroke Stretching for Vector Drawing Colorization</a></h3>
                    <p class="authors">Yiming Dong, Hongxu Xin, Zhiyang Dou, Rui Xu, Yuan Liu, Shuangmin Chen</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3763307" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1500', 'url', 'https://dl.acm.org/doi/10.1145/3763307')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1500', 'abstract', 'Hand-drawn vector sketches often contain implied lines, imprecise intersections, and unintended gaps, making it challenging to identify closed regions for colorization. These challenges become more pronounced as the number of strokes increases. In this paper, we present KISSColor, a novel method for inferring users\' intended closed regions. Specifically, we propose intuitive stroke stretching by extending open strokes along tangent isolines of winding-number fields, which provably form geometrically aligned closed regions. Extending all open strokes can lead to overly fragmented regions due to redundant intersections. While a Mixed Integer Programming (MIP) formulation helps reduce redundancy, it is computationally expensive. To improve efficiency, we introduce kinetic stroke stretching, which grows all strokes simultaneously and prioritizes early intersections using a kinetic data structure. This approach preserves stylistic ambiguity for lines requiring long extensions. Based on the growth results, redundant regions are suppressed to minimize fragmentation. We conduct extensive experiments demonstrating the effectiveness of KISSColor, which generates more intuitive partitions, especially for imprecise sketches (see teaser figure). Our code and data will be released upon publication.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">Hand-drawn vector sketches often contain implied lines, imprecise intersections, and unintended gaps, making it challenging to identify closed regions for colorization. These challenges become more pronounced as the number of strokes increases. In this paper, we present KISSColor, a novel method for inferring users&#x27; intended closed regions. Specifically, we propose intuitive stroke stretching by extending open strokes along tangent isolines of winding-number fields, which provably form geometrically aligned closed regions. Extending all open strokes can lead to overly fragmented regions due to redundant intersections. While a Mixed Integer Programming (MIP) formulation helps reduce redundancy, it is computationally expensive. To improve efficiency, we introduce kinetic stroke stretching, which grows all strokes simultaneously and prioritizes early intersections using a kinetic data structure. This approach preserves stylistic ambiguity for lines requiring long extensions. Based on the growth results, redundant regions are suppressed to minimize fragmentation. We conduct extensive experiments demonstrating the effectiveness of KISSColor, which generates more intuitive partitions, especially for imprecise sketches (see teaser figure). Our code and data will be released upon publication.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_2008">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/kgYQXUeemDQiCvGE.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763964" target="_blank" rel="noopener">LayerPeeler: Autoregressive Peeling for Layer-wise Image Vectorization</a></h3>
                    <p class="authors">Ronghuan Wu, Wanchao Su, Jing Liao</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763964" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_2008', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763964')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_2008', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_2019">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/5anzov4gRSJiQbnY.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3763345" target="_blank" rel="noopener">Neural Image abstraction using long smoothing B-splines</a></h3>
                    <p class="authors">Daniel Berio, Michael Stroh, Sylvain Calinon, Frederic Fol Leymarie, Oliver Deussen, Ariel Shamir</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3763345" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_2019', 'url', 'https://dl.acm.org/doi/10.1145/3763345')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_2019', 'abstract', 'We integrate smoothing B-splines into a standard differentiable vector graphics (DiffVG) pipeline through linear mapping, and show how this can be used to generate smooth and arbitrarily long paths within image-based deep learning systems. We take advantage of derivative-based smoothing costs for parametric control of fidelity vs. simplicity tradeoffs, while also enabling stylization control in geometric and image spaces. The proposed pipeline is compatible with recent vector graphics generation and vectorization methods. We demonstrate the versatility of our approach with four applications aimed at the generation of stylized vector graphics: stylized space-filling path generation, stroke-based image abstraction, closed-area image abstraction, and stylized text generation.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">We integrate smoothing B-splines into a standard differentiable vector graphics (DiffVG) pipeline through linear mapping, and show how this can be used to generate smooth and arbitrarily long paths within image-based deep learning systems. We take advantage of derivative-based smoothing costs for parametric control of fidelity vs. simplicity tradeoffs, while also enabling stylization control in geometric and image spaces. The proposed pipeline is compatible with recent vector graphics generation and vectorization methods. We demonstrate the versatility of our approach with four applications aimed at the generation of stylized vector graphics: stylized space-filling path generation, stroke-based image abstraction, closed-area image abstraction, and stylized text generation.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1793">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/a4vy9oaigM849bHo.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763923" target="_blank" rel="noopener">UTDesign: A Unified Framework for Stylized Text Editing and Generation in Graphic Design Images</a></h3>
                    <p class="authors">Yiming Zhao, Yuanpeng Gao, Yuxuan Luo, Jiwei Duan, Shisong Lin, Longfei Xiong</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763923" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1793', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763923')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1793', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            </div>
        </section>
        
        <section class="session">
            <div class="session-header">
                <h2>Intelligent CAD: B-Reps, NURBs & Splines</h2>
                <span class="session-count">6 papers</span>
            </div>
            <div class="papers-grid">
                
            <article class="paper-card" data-paper-id="papers_1016">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/EPnQf4SDwCfeF5os.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763814" target="_blank" rel="noopener">AutoBrep: Autoregressive B-Rep Generation with Unified Topology and Geometry</a></h3>
                    <p class="authors">Xiang Xu, Pradeep Jayaraman, Joseph Lambourne, Yilin Liu, Durvesh Malpure, Pete Meltzer</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763814" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1016', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763814')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1016', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1666">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/wFQR9G7tY28fof6W.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3763323" target="_blank" rel="noopener">BrepGPT: Autoregressive B-rep Generation with Voronoi Half-Patch</a></h3>
                    <p class="authors">Pu Li, Wenhao Zhang, Weize Quan, Biao Zhang, Peter Wonka, Dongming Yan</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3763323" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1666', 'url', 'https://dl.acm.org/doi/10.1145/3763323')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1666', 'abstract', 'Boundary representation (B-rep) is the de facto standard for CAD model representation in modern industrial design. The intricate coupling between geometric and topological elements in B-rep structures has forced existing generative methods to rely on cascaded multi-stage networks, resulting in error accumulation and computational inefficiency. We present BrepGPT, a single-stage autoregressive framework for B-rep generation. Our key innovation lies in the Voronoi Half-Patch (VHP) representation, which decomposes B-reps into unified local units by assigning geometry to nearest half-edges and sampling their next pointers. Unlike hierarchical representations that require multiple distinct encodings for different structural levels, our VHP representation facilitates unifying geometric attributes and topological relations in a single, coherent format. We further leverage dual VQ-VAEs to encode both vertex topology and Voronoi Half-Patches into vertex-based tokens, achieving a more compact sequential encoding. A decoder-only Transformer is then trained to autoregressively predict these tokens, which are subsequently mapped to vertex-based features and decoded into complete B-rep models. Experiments demonstrate that BrepGPT achieves state-of-the-art performance in unconditional B-rep generation. The framework also exhibits versatility in various applications, including conditional generation from category labels, point clouds, text descriptions, and images, as well as B-rep autocompletion and interpolation.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">Boundary representation (B-rep) is the de facto standard for CAD model representation in modern industrial design. The intricate coupling between geometric and topological elements in B-rep structures has forced existing generative methods to rely on cascaded multi-stage networks, resulting in error accumulation and computational inefficiency. We present BrepGPT, a single-stage autoregressive framework for B-rep generation. Our key innovation lies in the Voronoi Half-Patch (VHP) representation, which decomposes B-reps into unified local units by assigning geometry to nearest half-edges and sampling their next pointers. Unlike hierarchical representations that require multiple distinct encodings for different structural levels, our VHP representation facilitates unifying geometric attributes and topological relations in a single, coherent format. We further leverage dual VQ-VAEs to encode both vertex topology and Voronoi Half-Patches into vertex-based tokens, achieving a more compact sequential encoding. A decoder-only Transformer is then trained to autoregressively predict these tokens, which are subsequently mapped to vertex-based features and decoded into complete B-rep models. Experiments demonstrate that BrepGPT achieves state-of-the-art performance in unconditional B-rep generation. The framework also exhibits versatility in various applications, including conditional generation from category labels, point clouds, text descriptions, and images, as well as B-rep autocompletion and interpolation.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1147">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/7QesMgYvRpCAqREk.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3763284" target="_blank" rel="noopener">NURBS-Based Grid Shell Form Finding on Domains with Topologically Arbitrary Boundaries</a></h3>
                    <p class="authors">Masaaki Miki, Toby Mitchell</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3763284" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1147', 'url', 'https://dl.acm.org/doi/10.1145/3763284')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1147', 'abstract', 'In architecture, special attention is paid to the shapes of thin curved surface structures known as shells. Ideally, shells should have shapes that can support their self-weight without bending. These shapes rely solely on in-plane stresses flowing along the surface, resulting in highly efficient thin structures. The process of finding the shape of a shell is called form finding. In the context of form finding of shells, the computation of another surface, called the Airy stress function, often plays a key role. An Airy stress function is a smooth and continuous surface whose horizontal projection matches the shell, with stress distribution information encoded in its curvatures. However, some form-finding problems, particularly those involving topologically complex boundary curves, cannot be easily solved due to a limitation of the Airy stress function. By construction, it cannot represent stress distributions that transmit net forces between disjoint domain boundaries. Formally, the Airy stress function is incomplete: It cannot represent all valid stress fields. It requires extension to capture the case of interacting boundaries. In this paper, we address the limitation of the Airy stress function by reintroducing a previously overlooked additional stress function originally presented by Schaefer [1953] and Gurtin[1963]. In combination with the Airy stress function, this formulation was shown by Gurtin[1972] to represent all possible stress states, regardless of the topological complexity of the domain boundary. Using several examples, we demonstrate that topologically complex boundaries with interacting forces can be solved using this stress function inserted in combination with the Airy stress function.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">In architecture, special attention is paid to the shapes of thin curved surface structures known as shells. Ideally, shells should have shapes that can support their self-weight without bending. These shapes rely solely on in-plane stresses flowing along the surface, resulting in highly efficient thin structures. The process of finding the shape of a shell is called form finding. In the context of form finding of shells, the computation of another surface, called the Airy stress function, often plays a key role. An Airy stress function is a smooth and continuous surface whose horizontal projection matches the shell, with stress distribution information encoded in its curvatures. However, some form-finding problems, particularly those involving topologically complex boundary curves, cannot be easily solved due to a limitation of the Airy stress function. By construction, it cannot represent stress distributions that transmit net forces between disjoint domain boundaries. Formally, the Airy stress function is incomplete: It cannot represent all valid stress fields. It requires extension to capture the case of interacting boundaries. In this paper, we address the limitation of the Airy stress function by reintroducing a previously overlooked additional stress function originally presented by Schaefer [1953] and Gurtin[1963]. In combination with the Airy stress function, this formulation was shown by Gurtin[1972] to represent all possible stress states, regardless of the topological complexity of the domain boundary. Using several examples, we demonstrate that topologically complex boundaries with interacting forces can be solved using this stress function inserted in combination with the Airy stress function.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1507">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/gziYhVPgvB3WTioW.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3763308" target="_blank" rel="noopener">Overlap Region Extraction of Two NURBS Surfaces</a></h3>
                    <p class="authors">Jieyin Yang, Xiaohong Jia</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3763308" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1507', 'url', 'https://dl.acm.org/doi/10.1145/3763308')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1507', 'abstract', 'The detection and computation of the overlap region between two NURBS surfaces, as a special case of the intersection problem, are essential components of CAD systems, directly influencing the robustness of the entire system. Despite their importance, efficient, topologically correct, and numerically robust algorithms for detecting overlap regions remain lacking. To address this issue, we propose an optimization approach for computing the overlap region between two NURBS surfaces within a given error threshold. Based on a bilevel optimization framework, our algorithm first employs cubic B√©zier simplices to approximate the boundary of the overlap region. The boundary points of the overlap region are computed iteratively, followed by a Delaunay triangulation to establish the boundary topology. Additional refinement of the boundary edge is applied to ensure the topological correctness and maintain the precision of the overlap region within the specified error threshold. Our main contribution lies in the development of a novel and robust algorithm to calculate the boundary of the overlap region. This approach differs from previous overlap computation methods, which seldom account for error thresholds and are difficult to implement in floating-point arithmetic in CAD systems. We demonstrate the robustness and topological accuracy of our method through extensive experiments on a diverse set of complex examples with varying error thresholds.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">The detection and computation of the overlap region between two NURBS surfaces, as a special case of the intersection problem, are essential components of CAD systems, directly influencing the robustness of the entire system. Despite their importance, efficient, topologically correct, and numerically robust algorithms for detecting overlap regions remain lacking. To address this issue, we propose an optimization approach for computing the overlap region between two NURBS surfaces within a given error threshold. Based on a bilevel optimization framework, our algorithm first employs cubic B√©zier simplices to approximate the boundary of the overlap region. The boundary points of the overlap region are computed iteratively, followed by a Delaunay triangulation to establish the boundary topology. Additional refinement of the boundary edge is applied to ensure the topological correctness and maintain the precision of the overlap region within the specified error threshold. Our main contribution lies in the development of a novel and robust algorithm to calculate the boundary of the overlap region. This approach differs from previous overlap computation methods, which seldom account for error thresholds and are difficult to implement in floating-point arithmetic in CAD systems. We demonstrate the robustness and topological accuracy of our method through extensive experiments on a diverse set of complex examples with varying error thresholds.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1596">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/qqGRds8qxm4Sm4vi.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3763316" target="_blank" rel="noopener">$G^2$ Interpolating Spline with Local Maximum Curvature</a></h3>
                    <p class="authors">Bowen Jiang, Renjie Chen</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3763316" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1596', 'url', 'https://dl.acm.org/doi/10.1145/3763316')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1596', 'abstract', 'We introduce a novel class of G 2 continuous splines constructed using an innovative blending method, which guarantees precise interpolation of given control points. These splines are designed to achieve local curvature maxima specifically at these control points and possess compact local support, thereby eliminating the need for global optimization processes. The formulation ensures the splines are free from cusps and self-intersections and, notably, prevents adjacent segments from intersecting‚Äîa significant improvement over prior blending-based curve techniques. This framework utilizes quadratic B√©zier splines in conjunction with quartic B√©zier blending functions. A constructive algorithm is presented that generates these curvature-controlled curves without relying on global optimization. Through parametric adjustments of curvatures, the curve\'s geometry near control points can be tuned to create features ranging from smooth to sharp, thus broadening the design possibilities. Rigorous mathematical proofs and visual demonstrations validate all claimed properties of the framework.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">We introduce a novel class of G 2 continuous splines constructed using an innovative blending method, which guarantees precise interpolation of given control points. These splines are designed to achieve local curvature maxima specifically at these control points and possess compact local support, thereby eliminating the need for global optimization processes. The formulation ensures the splines are free from cusps and self-intersections and, notably, prevents adjacent segments from intersecting‚Äîa significant improvement over prior blending-based curve techniques. This framework utilizes quadratic B√©zier splines in conjunction with quartic B√©zier blending functions. A constructive algorithm is presented that generates these curvature-controlled curves without relying on global optimization. Through parametric adjustments of curvatures, the curve&#x27;s geometry near control points can be tuned to create features ranging from smooth to sharp, thus broadening the design possibilities. Rigorous mathematical proofs and visual demonstrations validate all claimed properties of the framework.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1539">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/C3j9JxnG6VgAx4CV.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763891" target="_blank" rel="noopener">Img2CAD: Reverse Engineering 3D CAD Models from Images through VLM-Assisted Conditional Factorization</a></h3>
                    <p class="authors">Yang You, Mikaela Uy, Jiaqi Han, Rahul Thomas, Haotong Zhang, Yi Du</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763891" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1539', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763891')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1539', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            </div>
        </section>
        
        <section class="session">
            <div class="session-header">
                <h2>It's All About the Motion</h2>
                <span class="session-count">6 papers</span>
            </div>
            <div class="papers-grid">
                
            <article class="paper-card" data-paper-id="papers_2420">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/RW5mCZoft8et5Zck.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3763366" target="_blank" rel="noopener">Curvature Enthusiasm: Correspondence-Free Interpolation and Matching of Articulated 3D Shapes using Compressed Normal Cycles</a></h3>
                    <p class="authors">Adam Hartshorne, Allen Paul, Tony Shardlow, Neill D. F. Campbell</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3763366" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_2420', 'url', 'https://dl.acm.org/doi/10.1145/3763366')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_2420', 'abstract', 'We present an unsupervised framework for physically plausible shape interpolation and dense correspondence estimation between 3D articulated shapes. Our approach intentionally focuses upon pose variation within the same identity, which we believe is a meaningful and challenging problem in its own right. Our method uses Neural Ordinary Differential Equations (NODEs) to generate smooth flow fields that define diffeomorphic transformations, ensuring topological consistency and preventing self-intersections while accommodating hard constraints, such as volume preservation. By incorporating a lightweight skeletal structure, we impose kinematic constraints that resolve symmetries without requiring manual skinning or predefined poses. We enhance physical realism by interpolating skeletal motion with dual quaternions and applying constrained optimisation to align the flow field with the skeleton, preserving local rigidity. Additionally, we employ an efficient formulation of Normal Cycles, a metric from geometric measure theory, to capture higher-order surface details like curvature, enabling precise alignment between complex articulated structures and recovery of accurate dense correspondence mapping. Evaluations on multiple benchmarks show notable improvements over state-of-the-art methods in both interpolation quality and correspondence accuracy, with consistent performance across different skeletal configurations, demonstrating broad utility for shape matching and animation tasks.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">We present an unsupervised framework for physically plausible shape interpolation and dense correspondence estimation between 3D articulated shapes. Our approach intentionally focuses upon pose variation within the same identity, which we believe is a meaningful and challenging problem in its own right. Our method uses Neural Ordinary Differential Equations (NODEs) to generate smooth flow fields that define diffeomorphic transformations, ensuring topological consistency and preventing self-intersections while accommodating hard constraints, such as volume preservation. By incorporating a lightweight skeletal structure, we impose kinematic constraints that resolve symmetries without requiring manual skinning or predefined poses. We enhance physical realism by interpolating skeletal motion with dual quaternions and applying constrained optimisation to align the flow field with the skeleton, preserving local rigidity. Additionally, we employ an efficient formulation of Normal Cycles, a metric from geometric measure theory, to capture higher-order surface details like curvature, enabling precise alignment between complex articulated structures and recovery of accurate dense correspondence mapping. Evaluations on multiple benchmarks show notable improvements over state-of-the-art methods in both interpolation quality and correspondence accuracy, with consistent performance across different skeletal configurations, demonstrating broad utility for shape matching and animation tasks.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1934">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/wERrKudfM7GoG5tw.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763950" target="_blank" rel="noopener">Motion In-Betweening for Densely Interacting Characters</a></h3>
                    <p class="authors">Xiaotang Zhang, Ziyi Chang, Qianhui Men, Hubert Shum</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763950" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1934', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763950')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1934', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1632">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/GPWTR8mT8div2zjm.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3763319" target="_blank" rel="noopener">Control Operators for Interactive Character Animation</a></h3>
                    <p class="authors">Ruiyu Gou, Michiel van de Panne, Daniel Holden</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3763319" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1632', 'url', 'https://dl.acm.org/doi/10.1145/3763319')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1632', 'abstract', 'Neural-network-based character controllers are increasingly common and capable. However, the integration of desired control inputs such as joystick movement, motion paths, and objects in the environment, remains challenging. This is because these inputs often require custom feature engineering, specific neural network architectures, and training procedures. This renders these methods largely inaccessible to non-technical designers. To address this challenge, we introduce Control Operators , a powerful and flexible framework for specifying the control mechanisms of interactive character controllers. By breaking down the control problem into a set of simple operators, each with a semantic meaning for designers, and a corresponding neural network structure, we allow non-technical users to design control mechanisms in a way that is intuitive and can be composed together to train models that have multiple skills and control modes. We demonstrate their potential with two current state-of-the-art interactive character controllers - a Flow-Matching-based auto-regressive model, and a variation of Learned Motion Matching. We validate the approach via a user study wherein industry practitioners with varying degrees of ML and technical expertise explore the use of our system.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">Neural-network-based character controllers are increasingly common and capable. However, the integration of desired control inputs such as joystick movement, motion paths, and objects in the environment, remains challenging. This is because these inputs often require custom feature engineering, specific neural network architectures, and training procedures. This renders these methods largely inaccessible to non-technical designers. To address this challenge, we introduce Control Operators , a powerful and flexible framework for specifying the control mechanisms of interactive character controllers. By breaking down the control problem into a set of simple operators, each with a semantic meaning for designers, and a corresponding neural network structure, we allow non-technical users to design control mechanisms in a way that is intuitive and can be composed together to train models that have multiple skills and control modes. We demonstrate their potential with two current state-of-the-art interactive character controllers - a Flow-Matching-based auto-regressive model, and a variation of Learned Motion Matching. We validate the approach via a user study wherein industry practitioners with varying degrees of ML and technical expertise explore the use of our system.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1784">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/6rz6zMo9hNABsB8H.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3763334" target="_blank" rel="noopener">Environment-aware Motion Matching</a></h3>
                    <p class="authors">Jose Luis Ponton, Sheldon Andrews, Carlos Andujar, Nuria Pelechano</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3763334" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1784', 'url', 'https://dl.acm.org/doi/10.1145/3763334')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1784', 'abstract', 'Interactive applications demand believable characters that respond naturally to dynamic environments. Traditional character animation techniques often struggle to handle arbitrary situations, leading to a growing trend of dynamically selecting motion-captured animations based on predefined features. While Motion Matching has proven effective for locomotion by aligning to target trajectories, animating environment interactions and crowd behaviors remains challenging due to the need to consider surrounding elements. Existing approaches often involve manual setup or lack the naturalism of motion capture. Furthermore, in crowd animation, body animation is frequently treated as a separate process from trajectory planning, leading to inconsistencies between body pose and root motion. To address these limitations, we present Environment-aware Motion Matching , a novel real-time system for full-body character animation that dynamically adapts to obstacles and other agents, emphasizing the bidirectional relationship between pose and trajectory. In a preprocessing step, we extract shape, pose, and trajectory features from a motion capture database. At runtime, we perform an efficient search that matches user input and current pose while penalizing collisions with a dynamic environment. Our method allows characters to naturally adjust their pose and trajectory to navigate crowded scenes.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">Interactive applications demand believable characters that respond naturally to dynamic environments. Traditional character animation techniques often struggle to handle arbitrary situations, leading to a growing trend of dynamically selecting motion-captured animations based on predefined features. While Motion Matching has proven effective for locomotion by aligning to target trajectories, animating environment interactions and crowd behaviors remains challenging due to the need to consider surrounding elements. Existing approaches often involve manual setup or lack the naturalism of motion capture. Furthermore, in crowd animation, body animation is frequently treated as a separate process from trajectory planning, leading to inconsistencies between body pose and root motion. To address these limitations, we present Environment-aware Motion Matching , a novel real-time system for full-body character animation that dynamically adapts to obstacles and other agents, emphasizing the bidirectional relationship between pose and trajectory. In a preprocessing step, we extract shape, pose, and trajectory features from a motion capture database. At runtime, we perform an efficient search that matches user input and current pose while penalizing collisions with a dynamic environment. Our method allows characters to naturally adjust their pose and trajectory to navigate crowded scenes.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1029">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/WqttHKqFwcX2g6SQ.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763817" target="_blank" rel="noopener">StableMotion: Training Motion Cleanup Models with Unpaired Corrupted Data</a></h3>
                    <p class="authors">Yuxuan Mu, Hung Yu Ling, Yi Shi, Ismael Baira Ojeda, Pengcheng Xi, Chang Shu</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763817" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1029', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763817')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1029', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_2494">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/xaTqsVhqmWeV6ArQ.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3763367" target="_blank" rel="noopener">Learning to Ball: Composing Policies for Long-Horizon Basketball Moves</a></h3>
                    <p class="authors">Pei Xu, Zhen Wu, Ruocheng Wang, Vishnu Sarukkai, Kayvon Fatahalian, Ioannis Karamouzas</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3763367" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_2494', 'url', 'https://dl.acm.org/doi/10.1145/3763367')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_2494', 'abstract', 'Learning a control policy for a multi-phase, long-horizon task, such as basketball maneuvers, remains challenging for reinforcement learning approaches due to the need for seamless policy composition and transitions between skills. A long-horizon task typically consists of distinct subtasks with well-defined goals, separated by transitional subtasks with unclear goals but critical to the success of the entire task. Existing methods like the mixture of experts and skill chaining struggle with tasks where individual policies do not share significant commonly explored states or lack well-defined initial and terminal states between different phases. In this paper, we introduce a novel policy integration framework to enable the composition of drastically different motor skills in multi-phase long-horizon tasks with ill-defined intermediate states. Based on that, we further introduce a high-level soft router to enable seamless and robust transitions between the subtasks. We evaluate our framework on a set of fundamental basketball skills and challenging transitions. Policies trained by our approach can effectively control the simulated character to interact with the ball and accomplish the long-horizon task specified by real-time user commands, without relying on ball trajectory references.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">Learning a control policy for a multi-phase, long-horizon task, such as basketball maneuvers, remains challenging for reinforcement learning approaches due to the need for seamless policy composition and transitions between skills. A long-horizon task typically consists of distinct subtasks with well-defined goals, separated by transitional subtasks with unclear goals but critical to the success of the entire task. Existing methods like the mixture of experts and skill chaining struggle with tasks where individual policies do not share significant commonly explored states or lack well-defined initial and terminal states between different phases. In this paper, we introduce a novel policy integration framework to enable the composition of drastically different motor skills in multi-phase long-horizon tasks with ill-defined intermediate states. Based on that, we further introduce a high-level soft router to enable seamless and robust transitions between the subtasks. We evaluate our framework on a set of fundamental basketball skills and challenging transitions. Policies trained by our approach can effectively control the simulated character to interact with the ball and accomplish the long-horizon task specified by real-time user commands, without relying on ball trajectory references.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            </div>
        </section>
        
        <section class="session">
            <div class="session-header">
                <h2>Compositional and Layout-Guided Image Synthesis</h2>
                <span class="session-count">5 papers</span>
            </div>
            <div class="papers-grid">
                
            <article class="paper-card" data-paper-id="papers_1074">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/vBqpCvA5dnRvAApH.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3763274" target="_blank" rel="noopener">Computational Modeling and Design of Capacitive Stretch Sensors</a></h3>
                    <p class="authors">Arvi Gjoka, Yongkang Sun, Roi Poranne, Daniele Panozzo</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3763274" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1074', 'url', 'https://dl.acm.org/doi/10.1145/3763274')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1074', 'abstract', 'A stretch sensor is a device that attaches to objects and measures the amount by which they deform. These sensors have shown great promise as an alternative to vision-based motion-capture systems, and for robotic sensing. Currently, they are generally limited to linear designs, and require a somewhat challenging calibration process. Our goal is to enable inverse design of such sensors, and to largely eliminate the calibration process. To this end, we introduce an accurate, differentiable simulator for capacitive stretch sensors, that treats both the elasto- and electro -static parts of the system. Differentiability allows optimizing the geometry of the sensor in order to improve its design for specific applications. We demonstrate the accuracy of our simulator and the effectiveness of our sensor optimization process for various use cases, such as human interfaces and robotics.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">A stretch sensor is a device that attaches to objects and measures the amount by which they deform. These sensors have shown great promise as an alternative to vision-based motion-capture systems, and for robotic sensing. Currently, they are generally limited to linear designs, and require a somewhat challenging calibration process. Our goal is to enable inverse design of such sensors, and to largely eliminate the calibration process. To this end, we introduce an accurate, differentiable simulator for capacitive stretch sensors, that treats both the elasto- and electro -static parts of the system. Differentiability allows optimizing the geometry of the sensor in order to improve its design for specific applications. We demonstrate the accuracy of our simulator and the effectiveness of our sensor optimization process for various use cases, such as human interfaces and robotics.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1082">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/ytcSrmtiNMGBDVd6.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3763275" target="_blank" rel="noopener">Designing and Fabricating Color BRDFs with Differentiable Wave Optics</a></h3>
                    <p class="authors">Yixin Zeng, Kiseok Choi, Hadi Amata, Kaizhang Kang, Wolfgang Heidrich, Hongzhi Wu</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3763275" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1082', 'url', 'https://dl.acm.org/doi/10.1145/3763275')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1082', 'abstract', 'Modeling surface reflectance is central to connecting optical theory with real-world rendering and fabrication. While analytic BRDFs remain standard in rendering, recent advances in geometric and wave optics have expanded the design space for complex reflectance effects. However, existing wave-optics-based methods are limited to controlling reflectance intensity only, lacking the ability to design full-spectrum, color-dependent BRDFs. In this work, we present the first method for designing and fabricating color BRDFs using a fully differentiable wave optics framework. Our differentiable and memory-efficient simulation framework supports end-to-end optimization of microstructured surfaces under scalar diffraction theory, enabling joint control over both angular intensity and spectral color of reflectance. We leverage grayscale lithography with a feature size of 1.5‚Äì2.0 Œº m to fabricate 15 BRDFs spanning four representative categories: anti-mirrors, pictorial reflections, structural colors, and iridescences. Compared to prior work, our approach achieves significantly higher fidelity and broader design flexibility, producing physically accurate and visually compelling results. By providing a practical and extensible solution for full-color BRDF design and fabrication, our method opens up new opportunities in structural coloration, product design, security printing, and advanced manufacturing.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">Modeling surface reflectance is central to connecting optical theory with real-world rendering and fabrication. While analytic BRDFs remain standard in rendering, recent advances in geometric and wave optics have expanded the design space for complex reflectance effects. However, existing wave-optics-based methods are limited to controlling reflectance intensity only, lacking the ability to design full-spectrum, color-dependent BRDFs. In this work, we present the first method for designing and fabricating color BRDFs using a fully differentiable wave optics framework. Our differentiable and memory-efficient simulation framework supports end-to-end optimization of microstructured surfaces under scalar diffraction theory, enabling joint control over both angular intensity and spectral color of reflectance. We leverage grayscale lithography with a feature size of 1.5‚Äì2.0 Œº m to fabricate 15 BRDFs spanning four representative categories: anti-mirrors, pictorial reflections, structural colors, and iridescences. Compared to prior work, our approach achieves significantly higher fidelity and broader design flexibility, producing physically accurate and visually compelling results. By providing a practical and extensible solution for full-color BRDF design and fabrication, our method opens up new opportunities in structural coloration, product design, security printing, and advanced manufacturing.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1187">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/BJfyrfPQ6D9qkVGE.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3763287" target="_blank" rel="noopener">Acoustic Reliefs</a></h3>
                    <p class="authors">Jeremy Chew, Michal Piovarci, Kangrui Xue, Doug James, Bernd Bickel</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3763287" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1187', 'url', 'https://dl.acm.org/doi/10.1145/3763287')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1187', 'abstract', 'We present a framework to optimize and generate Acoustic Reliefs : acoustic diffusers that not only perform well acoustically in scattering sound uniformly in all directions, but are also visually interesting and can approximate user-provided images. To this end, we develop a differentiable acoustics simulator based on the boundary element method, and integrate it with a differentiable renderer coupled with a vision model to jointly optimize for acoustics, appearance, and fabrication constraints at the same time. We generate various examples and fabricate two room-scale reliefs. The result is a validated simulation and optimization scheme for generating acoustic reliefs whose appearances can be guided by a provided image.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">We present a framework to optimize and generate Acoustic Reliefs : acoustic diffusers that not only perform well acoustically in scattering sound uniformly in all directions, but are also visually interesting and can approximate user-provided images. To this end, we develop a differentiable acoustics simulator based on the boundary element method, and integrate it with a differentiable renderer coupled with a vision model to jointly optimize for acoustics, appearance, and fabrication constraints at the same time. We generate various examples and fabricate two room-scale reliefs. The result is a validated simulation and optimization scheme for generating acoustic reliefs whose appearances can be guided by a provided image.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1484">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/T3gJzQcEFVkLsr37.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763884" target="_blank" rel="noopener">PhysiOpt: Physics-Driven Shape Optimization for 3D Generative Models</a></h3>
                    <p class="authors">Xiao Zhan, Cl√©ment Jambon, Evan Thompson, Kenney Ng, Mina Konakoviƒá Lukoviƒá</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763884" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1484', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763884')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1484', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_2171">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/VKhko3sX4c64yjSJ.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3763358" target="_blank" rel="noopener">Large-Area Fabrication-aware Computational Diffractive Optics</a></h3>
                    <p class="authors">Kaixuan Wei, Hector Romero, Hadi Amata, Jipeng Sun, Qiang Fu, Felix Heide</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3763358" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_2171', 'url', 'https://dl.acm.org/doi/10.1145/3763358')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_2171', 'abstract', 'Differentiable optics, as an emerging paradigm that jointly optimizes optics and (optional) image processing algorithms, has made many innovative optical designs possible across a broad range of imaging and display applications. Many of these systems utilize diffractive optical components for holography, PSF engineering, or wavefront shaping. Existing approaches have, however, mostly remained limited to laboratory prototypes, owing to a large quality gap between simulation and manufactured devices. We aim at lifting the fundamental technical barriers to the practical use of learned diffractive optical systems. To this end, we propose a fabrication-aware design pipeline for diffractive optics fabricated by direct-write grayscale lithography followed by replication with nano-imprinting, which is directly suited for inexpensive mass-production of large area designs. We propose a super-resolved neural lithography model that can accurately predict the 3D geometry generated by the fabrication process. This model can be seamlessly integrated into existing differentiable optics frameworks, enabling fabrication-aware, end-to-end optimization of computational optical systems. To tackle the computational challenges, we also devise tensor-parallel compute framework centered on distributing large-scale FFT computation across many GPUs. As such, we demonstrate large scale diffractive optics designs up to 32.16 mm √ó 21.44 mm, simulated on grids of up to 128,640 by 85,760 feature points. We find adequate agreement between simulation and fabricated prototypes for applications such as holography and PSF engineering. We also achieve high image quality from an imaging system comprised only of a single diffractive optical element, with images processed only by a one-step inverse filter utilizing the simulation PSF. We believe our findings lift the fabrication limitations for real-world applications of diffractive optics and differentiable optical design.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">Differentiable optics, as an emerging paradigm that jointly optimizes optics and (optional) image processing algorithms, has made many innovative optical designs possible across a broad range of imaging and display applications. Many of these systems utilize diffractive optical components for holography, PSF engineering, or wavefront shaping. Existing approaches have, however, mostly remained limited to laboratory prototypes, owing to a large quality gap between simulation and manufactured devices. We aim at lifting the fundamental technical barriers to the practical use of learned diffractive optical systems. To this end, we propose a fabrication-aware design pipeline for diffractive optics fabricated by direct-write grayscale lithography followed by replication with nano-imprinting, which is directly suited for inexpensive mass-production of large area designs. We propose a super-resolved neural lithography model that can accurately predict the 3D geometry generated by the fabrication process. This model can be seamlessly integrated into existing differentiable optics frameworks, enabling fabrication-aware, end-to-end optimization of computational optical systems. To tackle the computational challenges, we also devise tensor-parallel compute framework centered on distributing large-scale FFT computation across many GPUs. As such, we demonstrate large scale diffractive optics designs up to 32.16 mm √ó 21.44 mm, simulated on grids of up to 128,640 by 85,760 feature points. We find adequate agreement between simulation and fabricated prototypes for applications such as holography and PSF engineering. We also achieve high image quality from an imaging system comprised only of a single diffractive optical element, with images processed only by a one-step inverse filter utilizing the simulation PSF. We believe our findings lift the fabrication limitations for real-world applications of diffractive optics and differentiable optical design.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            </div>
        </section>
        
        <section class="session">
            <div class="session-header">
                <h2>Computational Design & Geometry</h2>
                <span class="session-count">5 papers</span>
            </div>
            <div class="papers-grid">
                
            <article class="paper-card" data-paper-id="papers_1381">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/CjUhKTgY5KZKcF5G.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763867" target="_blank" rel="noopener">Object-level Visual Prompts for Compositional Image Generation</a></h3>
                    <p class="authors">Gaurav Parmar, Or Patashnik, Kuan-Chieh Wang, Daniil Ostashev, Srinivasa Narasimhan, Jun-Yan Zhu</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763867" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1381', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763867')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1381', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_2203">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/PwCNZ4vTX6MBXNa2.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763984" target="_blank" rel="noopener">ComposeMe: Attribute-Specific Image Prompts for Controllable Human Image Generation</a></h3>
                    <p class="authors">Guocheng Qian, Daniil Ostashev, Egor Nemchinov, Sergey Tulyakov, Kuan-Chieh Wang, Kfir Aberman</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763984" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_2203', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763984')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_2203', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1918">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/pDwRaW4hevaNEkGu.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763947" target="_blank" rel="noopener">PartComposer: Learning and Composing Part-Level Concepts from Single-Image Examples</a></h3>
                    <p class="authors">Junyu Liu, R. Kenny Jones, Daniel Ritchie</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763947" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1918', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763947')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1918', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1885">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/FAyJMhGLTvRrnD6z.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763944" target="_blank" rel="noopener">AGSwap: Overcoming Category Boundaries in Object Fusion via Adaptive Group Swapping</a></h3>
                    <p class="authors">Zedong Zhang, ying tai, Jianjun Qian, Jian Yang, Jun Li</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763944" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1885', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763944')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1885', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1917">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/Enz6apfYHXqtPwnt.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3763341" target="_blank" rel="noopener">MALeR: Improving Compositional Fidelity in Layout-Guided Generation</a></h3>
                    <p class="authors">Shivank Saxena, Dhruv Srivastava, Makarand Tapaswi</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3763341" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1917', 'url', 'https://dl.acm.org/doi/10.1145/3763341')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1917', 'abstract', 'Recent advances in text-to-image models have enabled a new era of creative and controllable image generation. However, generating compositional scenes with multiple subjects and attributes remains a significant challenge. To enhance user control over subject placement, several layout-guided methods have been proposed. However, these methods face numerous challenges, particularly in compositional scenes. Unintended subjects often appear outside the layouts, generated images can be out-of-distribution and contain unnatural artifacts, or attributes bleed across subjects, leading to incorrect visual outputs. In this work, we propose MALeR, a method that addresses each of these challenges. Given a text prompt and corresponding layouts, our method prevents subjects from appearing outside the given layouts while being in-distribution. Additionally, we propose a masked, attribute-aware binding mechanism that prevents attribute leakage, enabling accurate rendering of subjects with multiple attributes, even in complex compositional scenes. Qualitative and quantitative evaluation demonstrates that our method achieves superior performance in compositional accuracy, generation consistency, and attribute binding compared to previous work. MALeR is particularly adept at generating images of scenes with multiple subjects and multiple attributes per subject.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">Recent advances in text-to-image models have enabled a new era of creative and controllable image generation. However, generating compositional scenes with multiple subjects and attributes remains a significant challenge. To enhance user control over subject placement, several layout-guided methods have been proposed. However, these methods face numerous challenges, particularly in compositional scenes. Unintended subjects often appear outside the layouts, generated images can be out-of-distribution and contain unnatural artifacts, or attributes bleed across subjects, leading to incorrect visual outputs. In this work, we propose MALeR, a method that addresses each of these challenges. Given a text prompt and corresponding layouts, our method prevents subjects from appearing outside the given layouts while being in-distribution. Additionally, we propose a masked, attribute-aware binding mechanism that prevents attribute leakage, enabling accurate rendering of subjects with multiple attributes, even in complex compositional scenes. Qualitative and quantitative evaluation demonstrates that our method achieves superior performance in compositional accuracy, generation consistency, and attribute binding compared to previous work. MALeR is particularly adept at generating images of scenes with multiple subjects and multiple attributes per subject.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            </div>
        </section>
        
        <section class="session">
            <div class="session-header">
                <h2>Hair & Faces</h2>
                <span class="session-count">6 papers</span>
            </div>
            <div class="papers-grid">
                
            <article class="paper-card" data-paper-id="papers_1299">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/hTjhcGkCxWwEsbAj.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3763295" target="_blank" rel="noopener">Auto Hair Card Extraction for Smooth Hair with Differentiable Rendering</a></h3>
                    <p class="authors">Zhongtian Zheng, Tao Huang, Haozhe Su, Xueqi Ma, Yuefan Shen, Tongtong Wang</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3763295" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1299', 'url', 'https://dl.acm.org/doi/10.1145/3763295')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1299', 'abstract', 'Hair cards remain a widely used representation for hair modeling in real-time applications, offering a practical trade-off between visual fidelity, memory usage, and performance. However, generating high-quality hair card models remains a challenging and labor-intensive task. This work presents an automated pipeline for converting strand-based hair models into hair card models with a limited number of cards and textures while preserving the hairstyle appearance. Our key idea is a novel differentiable representation where each strand is encoded as a projected 2D curve in the texture space, which enables end-to-end optimization with differentiable rendering while respecting the structures of the hair geometry. Based on this representation, we develop a novel algorithm pipeline, where we first cluster hair strands into initial hair cards and project the strands into the texture space. We then conduct a two-stage optimization, where our first stage optimizes the orientation of each hair card separately, and after strand projection, our second stage conducts joint optimization over the entire hair card model for fine-tuning. Our method is evaluated on a range of hairstyles, including straight, wavy, curly, and coily hair. To capture the appearance of short or coily hair, our method comes with support for hair caps and cross-card.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">Hair cards remain a widely used representation for hair modeling in real-time applications, offering a practical trade-off between visual fidelity, memory usage, and performance. However, generating high-quality hair card models remains a challenging and labor-intensive task. This work presents an automated pipeline for converting strand-based hair models into hair card models with a limited number of cards and textures while preserving the hairstyle appearance. Our key idea is a novel differentiable representation where each strand is encoded as a projected 2D curve in the texture space, which enables end-to-end optimization with differentiable rendering while respecting the structures of the hair geometry. Based on this representation, we develop a novel algorithm pipeline, where we first cluster hair strands into initial hair cards and project the strands into the texture space. We then conduct a two-stage optimization, where our first stage optimizes the orientation of each hair card separately, and after strand projection, our second stage conducts joint optimization over the entire hair card model for fine-tuning. Our method is evaluated on a range of hairstyles, including straight, wavy, curly, and coily hair. To capture the appearance of short or coily hair, our method comes with support for hair caps and cross-card.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1368">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/8C3j8tkngyiGfXAo.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763864" target="_blank" rel="noopener">Strands2Cards: Automatic Generation of Hair Cards from Strands</a></h3>
                    <p class="authors">Kenji Tojo, Liwen Hu, Nobuyuki Umetani, Hao Li</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763864" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1368', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763864')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1368', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_2365">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/PteANe6LmFCnZBvS.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3763365" target="_blank" rel="noopener">Shaping Strands with Neural Style Transfer</a></h3>
                    <p class="authors">Beyzanur Coban, Pascal Chang, Guilherme Gomes Haetinger, Jingwei Tang, Vinicius C. Azevedo</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3763365" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_2365', 'url', 'https://dl.acm.org/doi/10.1145/3763365')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_2365', 'abstract', 'The intricate geometric complexity of knots, tangles, dreads and clumps require sophisticated grooming systems that allow artists to both realistically model and artistically control fur and hair systems. Recent volumetric and 3D neural style transfer techniques provided a new paradigm of art directability, allowing artists to modify assets drastically with the use of single style images. However, these previous 3D neural stylization approaches were limited to volumes and meshes. In this paper we propose the first stylization pipeline to support hair and fur. Through a carefully tailored fur/hair representation, our approach allows complex, 3D consistent and temporally coherent grooms that are stylized using style images.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">The intricate geometric complexity of knots, tangles, dreads and clumps require sophisticated grooming systems that allow artists to both realistically model and artistically control fur and hair systems. Recent volumetric and 3D neural style transfer techniques provided a new paradigm of art directability, allowing artists to modify assets drastically with the use of single style images. However, these previous 3D neural stylization approaches were limited to volumes and meshes. In this paper we propose the first stylization pipeline to support hair and fur. Through a carefully tailored fur/hair representation, our approach allows complex, 3D consistent and temporally coherent grooms that are stylized using style images.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1711">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/q6AV6Rrgu9a3w51q.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763912" target="_blank" rel="noopener">CHARM: Control-point-based 3D Anime Hairstyle Auto-Regressive Modeling</a></h3>
                    <p class="authors">Yuze He, Yanning Zhou, Wang Zhao, Jingwen Ye, Yushi Bai, Kaiwen Xiao</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763912" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1711', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763912')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1711', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1879">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/qyaxnF6vAEziQ9QW.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763942" target="_blank" rel="noopener">Single-Shot Facial Capture using Polarized RGB Sinusoidal Illumination</a></h3>
                    <p class="authors">Arvin Lin, Abhijeet Ghosh</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763942" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1879', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763942')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1879', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_2005">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/M3rLaNNeL8FCpKtr.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763962" target="_blank" rel="noopener">Single Image 3D Portrait Relighting with Generative Priors</a></h3>
                    <p class="authors">Pramod Rao, Xilong Zhou, Abhimitra Meka, Gereon Fox, Mallikarjun B R, Fangneng Zhan</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763962" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_2005', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763962')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_2005', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            </div>
        </section>
        
        <section class="session">
            <div class="session-header">
                <h2>Differentiable Physics and Fabrication-Aware Optimization</h2>
                <span class="session-count">6 papers</span>
            </div>
            <div class="papers-grid">
                
            <article class="paper-card" data-paper-id="papers_1405">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/hQcCdpWV6p7DipQu.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763871" target="_blank" rel="noopener">VideoFrom3D: 3D Scene Video Generation via Complementary Image and Video Diffusion Models</a></h3>
                    <p class="authors">Geonung Kim, Janghyeok Han, Sunghyun Cho</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763871" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1405', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763871')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1405', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1416">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/YakHeJ4BM71hdZdH.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3763302" target="_blank" rel="noopener">SS4D: Native 4D Generative Model via Structured Spacetime Latents</a></h3>
                    <p class="authors">Zhibing Li, Mengchen Zhang, Tong Wu, Jing Tan, Jiaqi Wang, Dahua Lin</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3763302" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1416', 'url', 'https://dl.acm.org/doi/10.1145/3763302')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1416', 'abstract', 'We present SS4D, a native 4D generative model that synthesizes dynamic 3D objects directly from monocular video. Unlike prior approaches that construct 4D representations by optimizing over 3D or video generative models, we train a generator directly on 4D data, achieving high fidelity, temporal coherence, and structural consistency. At the core of our method is a compressed set of structured spacetime latents. Specifically, (1) To address the scarcity of 4D training data, we build on a pre-trained single-image-to-3D model, preserving strong spatial consistency. (2) Temporal consistency is enforced by introducing dedicated temporal layers that reason across frames. (3) To support efficient training and inference over long video sequences, we compress the latent sequence along the temporal axis using factorized 4D convolutions and temporal downsampling blocks. In addition, we employ a carefully designed training strategy to enhance robustness against occlusion and motion blur, leading to high-quality generation. Extensive experiments show that SS4D produces spatio-temporally consistent 4D objects with superior quality and efficiency, significantly outperforming state-of-the-art methods on both synthetic and real-world datasets.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">We present SS4D, a native 4D generative model that synthesizes dynamic 3D objects directly from monocular video. Unlike prior approaches that construct 4D representations by optimizing over 3D or video generative models, we train a generator directly on 4D data, achieving high fidelity, temporal coherence, and structural consistency. At the core of our method is a compressed set of structured spacetime latents. Specifically, (1) To address the scarcity of 4D training data, we build on a pre-trained single-image-to-3D model, preserving strong spatial consistency. (2) Temporal consistency is enforced by introducing dedicated temporal layers that reason across frames. (3) To support efficient training and inference over long video sequences, we compress the latent sequence along the temporal axis using factorized 4D convolutions and temporal downsampling blocks. In addition, we employ a carefully designed training strategy to enhance robustness against occlusion and motion blur, leading to high-quality generation. Extensive experiments show that SS4D produces spatio-temporally consistent 4D objects with superior quality and efficiency, significantly outperforming state-of-the-art methods on both synthetic and real-world datasets.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1445">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/TVj77fcd9LC5iMRx.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763876" target="_blank" rel="noopener">Generating 360¬∞ Video is What You Need For a 3D Scene</a></h3>
                    <p class="authors">Zhaoyang Zhang, Yannick Hold-Geoffroy, Milo≈° Ha≈°an, Ziwen Chen, Fujun Luan, Julie Dorsey</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763876" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1445', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763876')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1445', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1483">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/yKQq5W4z23f7pQN5.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763883" target="_blank" rel="noopener">PanoDreamer: Optimization-Based Single Image to 360 3D Scene With Diffusion</a></h3>
                    <p class="authors">Avinash Paliwal, Xilong Zhou, Andrii Tsarov, Nima Kalantari</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763883" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1483', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763883')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1483', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1915">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/terGbe2QkLYN2sjJ.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763946" target="_blank" rel="noopener">WorldExplorer: Towards Generating Fully Navigable 3D Scenes</a></h3>
                    <p class="authors">Manuel-Andreas Schneider, Lukas H√∂llein, Matthias Niessner</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763946" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1915', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763946')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1915', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1747">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/ZZv14MTn1ivxkjsK.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3763330" target="_blank" rel="noopener">Voyager: Long-Range and World-Consistent Video Diffusion for Explorable 3D Scene Generation</a></h3>
                    <p class="authors">Tianyu Huang, Wangguandong Zheng, Tengfei Wang, Yuhao Liu, Zhenwei Wang, Junta Wu</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3763330" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1747', 'url', 'https://dl.acm.org/doi/10.1145/3763330')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1747', 'abstract', 'Real-world applications like video gaming and virtual reality often demand the ability to model 3D scenes that users can explore along custom camera trajectories. While significant progress has been made in generating 3D objects from text or images, creating long-range, 3D-consistent, explorable 3D scenes remains a complex and challenging problem. In this work, we present Voyager , a novel video diffusion framework that generates world-consistent 3D point-cloud sequences from a single image with user-defined camera path. Unlike existing approaches, Voyager achieves end-to-end scene generation and reconstruction with inherent consistency across frames, eliminating the need for 3D reconstruction pipelines (e.g., structure-from-motion or multi-view stereo). Our method integrates three key components: 1) World-Consistent Video Diffusion : A unified architecture that jointly generates aligned RGB and depth video sequences, conditioned on existing world observation to ensure global coherence 2) Long-Range World Exploration : An efficient world cache with point culling and an auto-regressive inference with smooth video sampling for iterative scene extension with context-aware consistency, and 3) Scalable Data Engine : A video reconstruction pipeline that automates camera pose estimation and metric depth prediction for arbitrary videos, enabling large-scale, diverse training data curation without manual 3D annotations. Collectively, these designs result in a clear improvement over existing methods in visual quality and geometric accuracy, with versatile applications. Code for this paper are at https://github.com/Tencent-Hunyuan/HunyuanWorld-Voyager.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">Real-world applications like video gaming and virtual reality often demand the ability to model 3D scenes that users can explore along custom camera trajectories. While significant progress has been made in generating 3D objects from text or images, creating long-range, 3D-consistent, explorable 3D scenes remains a complex and challenging problem. In this work, we present Voyager , a novel video diffusion framework that generates world-consistent 3D point-cloud sequences from a single image with user-defined camera path. Unlike existing approaches, Voyager achieves end-to-end scene generation and reconstruction with inherent consistency across frames, eliminating the need for 3D reconstruction pipelines (e.g., structure-from-motion or multi-view stereo). Our method integrates three key components: 1) World-Consistent Video Diffusion : A unified architecture that jointly generates aligned RGB and depth video sequences, conditioned on existing world observation to ensure global coherence 2) Long-Range World Exploration : An efficient world cache with point culling and an auto-regressive inference with smooth video sampling for iterative scene extension with context-aware consistency, and 3) Scalable Data Engine : A video reconstruction pipeline that automates camera pose estimation and metric depth prediction for arbitrary videos, enabling large-scale, diverse training data curation without manual 3D annotations. Collectively, these designs result in a clear improvement over existing methods in visual quality and geometric accuracy, with versatile applications. Code for this paper are at https://github.com/Tencent-Hunyuan/HunyuanWorld-Voyager.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            </div>
        </section>
        
        <section class="session">
            <div class="session-header">
                <h2>Generative Scenes & Panoramas</h2>
                <span class="session-count">6 papers</span>
            </div>
            <div class="papers-grid">
                
            <article class="paper-card" data-paper-id="papers_1469">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/N9vZyLEUaZNxnUyc.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763879" target="_blank" rel="noopener">Social Agent: Mastering Dyadic Nonverbal Behavior Generation via Conversational LLM Agents</a></h3>
                    <p class="authors">Zeyi Zhang, Yanju Zhou, Heyuan Yao, Tenglong Ao, Xiaohang Zhan, Libin Liu</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763879" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1469', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763879')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1469', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1244">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/sxVn2RNZGzJ6Tk7Q.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3>How Does a Virtual Agent Decide Where to Look? Symbolic Cognitive Reasoning for Embodied Head Rotation</h3>
                    <p class="authors">Juyeong Hwang, Seong-Eun Hong, JaeYoung Seon, HyeongYeop Kang</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <button class="edit-btn" onclick="openEditModal('papers_1244', 'url', '')" title="Add URL">‚úèÔ∏è Add link</button>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1244', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_2382">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/NE9qnC6jyAmspmcV.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763998" target="_blank" rel="noopener">Echo: Enhancing Conversational Behavior Generation via Hierarchical Semantic Comprehension with Large Language Models</a></h3>
                    <p class="authors">Haiwei Xue, Yanbo Fan, Xuan Wang, Zhiyong Wu</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763998" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_2382', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763998')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_2382', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1732">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/15KauRHKPUDuSRgn.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763915" target="_blank" rel="noopener">SymBridge: A Human-in-the-Loop Cyber-Physical Interactive System for Adaptive Human-Robot Symbiosis</a></h3>
                    <p class="authors">Haoran Chen, Yiteng Xu, Yiming Ren, Yaoqin Ye, Xinran Li, Ning Ding</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763915" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1732', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763915')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1732', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_2031">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/CCYKHqLvKMm3uoYU.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763966" target="_blank" rel="noopener">Unifying Latent Action and Latent State Pre-training for Policy Learning from Videos</a></h3>
                    <p class="authors">Guangyan Chen, Meiling Wang, Te Cui, Luojie Yang, Qi Shao, Lin Zhao</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763966" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_2031', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763966')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_2031', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_2493">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/rR49JFKEUsbUrinb.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3764006" target="_blank" rel="noopener">JoruriPuppet: Learning Tempo-Changing Mechanisms Beyond the Beat for Music-to-Motion Generation with Expressive Metrics</a></h3>
                    <p class="authors">Ran Dong, Shaowen Ni, Xi Yang</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3764006" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_2493', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3764006')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_2493', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            </div>
        </section>
        
        <section class="session">
            <div class="session-header">
                <h2>Human & Robot Animation & Behavior</h2>
                <span class="session-count">6 papers</span>
            </div>
            <div class="papers-grid">
                
            <article class="paper-card" data-paper-id="papers_1119">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/cee8tZ2JU2NASGjN.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3763281" target="_blank" rel="noopener">BSP-OT: Sparse transport plans between discrete measures in loglinear time</a></h3>
                    <p class="authors">Baptiste Genest, Nicolas Bonneel, Vincent Nivoliers, David Coeurjolly</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3763281" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1119', 'url', 'https://dl.acm.org/doi/10.1145/3763281')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1119', 'abstract', 'To solve the optimal transport problem between two uniform discrete measures of the same size, one seeks a bijective assignment that minimizes some matching cost. For this task, exact algorithms are intractable for large problems, while approximate ones may lose the bijectivity of the assignment. We address this issue and the more general cases of non-uniform discrete measures with different total masses, where partial transport may be desirable. The core of our algorithm is a variant of the Quicksort algorithm that provides an efficient strategy to randomly explore many relevant and easy-to-compute couplings, by matching BSP trees in loglinear time. The couplings we obtain are as sparse as possible, in the sense that they provide bijections, injective partial matchings or sparse couplings depending on the nature of the matched measures. To improve the transport cost, we propose efficient strategies to merge k sparse couplings into a higher quality one. For k = 64, we obtain transport plans with typically less than 1% of relative error in a matter of seconds between hundreds of thousands of points in 3D on the CPU. We demonstrate how these high-quality approximations can drastically speed-up usual pipelines involving optimal transport, such as shape interpolation, intrinsic manifold sampling, color transfer, topological data analysis, rigid partial registration of point clouds and image stippling.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">To solve the optimal transport problem between two uniform discrete measures of the same size, one seeks a bijective assignment that minimizes some matching cost. For this task, exact algorithms are intractable for large problems, while approximate ones may lose the bijectivity of the assignment. We address this issue and the more general cases of non-uniform discrete measures with different total masses, where partial transport may be desirable. The core of our algorithm is a variant of the Quicksort algorithm that provides an efficient strategy to randomly explore many relevant and easy-to-compute couplings, by matching BSP trees in loglinear time. The couplings we obtain are as sparse as possible, in the sense that they provide bijections, injective partial matchings or sparse couplings depending on the nature of the matched measures. To improve the transport cost, we propose efficient strategies to merge k sparse couplings into a higher quality one. For k = 64, we obtain transport plans with typically less than 1% of relative error in a matter of seconds between hundreds of thousands of points in 3D on the CPU. We demonstrate how these high-quality approximations can drastically speed-up usual pipelines involving optimal transport, such as shape interpolation, intrinsic manifold sampling, color transfer, topological data analysis, rigid partial registration of point clouds and image stippling.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_2099">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/4DZumctExhB7K9nG.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763976" target="_blank" rel="noopener">Efficient and Scalable Spatial Regularization of Optimal Transport</a></h3>
                    <p class="authors">Lucas Brifault, David Cohen-Steiner, Mathieu Desbrun</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763976" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_2099', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763976')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_2099', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1319">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/UBkTQZT9ZPXBTbKx.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763856" target="_blank" rel="noopener">Medial Sphere Preconditioning for Knot Untangling and Volume-Filling Curves</a></h3>
                    <p class="authors">Yuta Noma, Alec Jacobson, Karan Singh</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763856" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1319', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763856')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1319', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1361">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/dqoH36eUxiKqXu4B.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3763299" target="_blank" rel="noopener">CrossGen: Learning and Generating Cross Fields for Quad Meshing</a></h3>
                    <p class="authors">Qiujie Dong, Jiepeng Wang, Rui Xu, Cheng Lin, Yuan Liu, Shiqing Xin</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3763299" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1361', 'url', 'https://dl.acm.org/doi/10.1145/3763299')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1361', 'abstract', 'Cross fields play a critical role in various geometry processing tasks, especially for quad mesh generation. Existing methods for cross field generation often struggle to balance computational efficiency with generation quality, using slow per-shape optimization. We introduce CrossGen , a novel framework that supports both feed-forward prediction and latent generative modeling of cross fields for quad meshing by unifying geometry and cross field representations within a joint latent space. Our method enables extremely fast computation of high-quality cross fields of general input shapes, typically within one second without per-shape optimization. Our method assumes a point-sampled surface, also called a point-cloud surface , as input, so we can accommodate various surface representations by a straightforward point sampling process. Using an auto-encoder network architecture, we encode input point-cloud surfaces into a sparse voxel grid with fine-grained latent spaces, which are decoded into both SDF-based surface geometry and cross fields (see the teaser figure). We also contribute a dataset of models with both high-quality signed distance fields (SDFs) representations and their corresponding cross fields, and use it to train our network. Once trained, the network is capable of computing a cross field of an input surface in a feed-forward manner, ensuring high geometric fidelity, noise resilience, and rapid inference. Furthermore, leveraging the same unified latent representation, we incorporate a diffusion model for computing cross fields of new shapes generated from partial input, such as sketches. To demonstrate its practical applications, we validate CrossGen on the quad mesh generation task for a large variety of surface shapes. Experimental results demonstrate that CrossGen generalizes well across diverse shapes and consistently yields high-fidelity cross fields, thus facilitating the generation of high-quality quad meshes.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">Cross fields play a critical role in various geometry processing tasks, especially for quad mesh generation. Existing methods for cross field generation often struggle to balance computational efficiency with generation quality, using slow per-shape optimization. We introduce CrossGen , a novel framework that supports both feed-forward prediction and latent generative modeling of cross fields for quad meshing by unifying geometry and cross field representations within a joint latent space. Our method enables extremely fast computation of high-quality cross fields of general input shapes, typically within one second without per-shape optimization. Our method assumes a point-sampled surface, also called a point-cloud surface , as input, so we can accommodate various surface representations by a straightforward point sampling process. Using an auto-encoder network architecture, we encode input point-cloud surfaces into a sparse voxel grid with fine-grained latent spaces, which are decoded into both SDF-based surface geometry and cross fields (see the teaser figure). We also contribute a dataset of models with both high-quality signed distance fields (SDFs) representations and their corresponding cross fields, and use it to train our network. Once trained, the network is capable of computing a cross field of an input surface in a feed-forward manner, ensuring high geometric fidelity, noise resilience, and rapid inference. Furthermore, leveraging the same unified latent representation, we incorporate a diffusion model for computing cross fields of new shapes generated from partial input, such as sketches. To demonstrate its practical applications, we validate CrossGen on the quad mesh generation task for a large variety of surface shapes. Experimental results demonstrate that CrossGen generalizes well across diverse shapes and consistently yields high-fidelity cross fields, thus facilitating the generation of high-quality quad meshes.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_2499">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/WzmTyYQ5vy4P5bkv.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3763368" target="_blank" rel="noopener">Sums of Wedges: Conforming Weighted Delaunay Triangulations are Polynomial in Fixed Dimension</a></h3>
                    <p class="authors">Dimitrios Bogiokas, Ugo Finnendahl, Thorsten Seidelmann, Marc Alexa</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3763368" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_2499', 'url', 'https://dl.acm.org/doi/10.1145/3763368')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_2499', 'abstract', 'We show how the problem of creating a triangulation in d -dimensional space that conforms to constraints given as sub-simplices can be turned into the problem of computing the lower hull of a sum of wedge functions. This sum can be interpreted as a Weighted Delaunay Triangulations, necessarily containing the constraints as unions of its elements. Intersections of wedges lead to Steiner points. As the number of such intersections is polynomial in the number of wedges, and the number of wedges per element is typically 1 (at most d ), this proves that the complexity of the output is polynomial. Moreover, we show that the majority of wedge intersections is unnecessary for a conforming triangulation and further heuristically reduce the number of Steiner points. Using appropriate data structures, the function can be evaluated in quasi-linear time, leading to an output-sensitive algorithm.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">We show how the problem of creating a triangulation in d -dimensional space that conforms to constraints given as sub-simplices can be turned into the problem of computing the lower hull of a sum of wedge functions. This sum can be interpreted as a Weighted Delaunay Triangulations, necessarily containing the constraints as unions of its elements. Intersections of wedges lead to Steiner points. As the number of such intersections is polynomial in the number of wedges, and the number of wedges per element is typically 1 (at most d ), this proves that the complexity of the output is polynomial. Moreover, we show that the majority of wedge intersections is unnecessary for a conforming triangulation and further heuristically reduce the number of Steiner points. Using appropriate data structures, the function can be evaluated in quasi-linear time, leading to an output-sensitive algorithm.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_2224">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/xDAenwaRNYneCSYK.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3763360" target="_blank" rel="noopener">Lifted Surfacing of Generalized Sweep Volumes</a></h3>
                    <p class="authors">Yiwen Ju, Qingnan Zhou, Xingyi Du, Nathan Carr, Tao Ju</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3763360" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_2224', 'url', 'https://dl.acm.org/doi/10.1145/3763360')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_2224', 'abstract', 'Computing the boundary surface of the 3D volume swept by a rigid or deforming solid remains a challenging problem in geometric modeling. Existing approaches are often limited to sweeping rigid shapes, cannot guarantee a watertight surface, or struggle with modeling the intricate geometric features (e.g., sharp creases and narrow gaps) and topological features (e.g., interior voids). We make the observation that the sweep boundary is a subset of the projection of the intersection of two implicit surfaces in a higher dimension, and we derive a characterization of the subset using winding numbers. These insights lead to a general algorithm for any sweep represented as a smooth time-varying implicit function satisfying a genericity assumption, and it produces a watertight and intersection-free surface that better approximates the geometric and topological features than existing methods.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">Computing the boundary surface of the 3D volume swept by a rigid or deforming solid remains a challenging problem in geometric modeling. Existing approaches are often limited to sweeping rigid shapes, cannot guarantee a watertight surface, or struggle with modeling the intricate geometric features (e.g., sharp creases and narrow gaps) and topological features (e.g., interior voids). We make the observation that the sweep boundary is a subset of the projection of the intersection of two implicit surfaces in a higher dimension, and we derive a characterization of the subset using winding numbers. These insights lead to a general algorithm for any sweep represented as a smooth time-varying implicit function satisfying a genericity assumption, and it produces a watertight and intersection-free surface that better approximates the geometric and topological features than existing methods.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            </div>
        </section>
        
        <section class="session">
            <div class="session-header">
                <h2>4D & Dynamic Scene Generation and Reconstruction</h2>
                <span class="session-count">6 papers</span>
            </div>
            <div class="papers-grid">
                
            <article class="paper-card" data-paper-id="papers_1168">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/xwdTNpkHdraBZson.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763836" target="_blank" rel="noopener">PAD3R: Pose-Aware Dynamic 3D Reconstruction from Casual Videos</a></h3>
                    <p class="authors">Ting-Hsuan Liao, Haowen Liu, Yiran Xu, Songwei Ge, Gengshan Yang, Jia-Bin Huang</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763836" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1168', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763836')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1168', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1841">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/Pxn16cdZug2fYGg2.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3763336" target="_blank" rel="noopener">Detail Enhanced Gaussian Splatting for Large-Scale Volumetric Capture</a></h3>
                    <p class="authors">Julien Philip, Li Ma, Pascal Clausen, Wenqi Xian, Ahmet Levent Ta≈üel, Mingming He</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3763336" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1841', 'url', 'https://dl.acm.org/doi/10.1145/3763336')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1841', 'abstract', 'We present a unique system for large-scale, multi-performer, high resolution 4D volumetric capture providing realistic free-viewpoint video up to and including 4K resolution facial closeups. To achieve this, we employ a novel volumetric capture, reconstruction and rendering pipeline based on Dynamic Gaussian Splatting and Diffusion-based Detail Enhancement. We design our pipeline specifically to meet the demands of high-end media production. We employ two capture rigs: the Scene Rig , which captures multi-actor performances at a resolution which falls short of 4K production quality, and the Face Rig , which records high-fidelity single-actor facial detail to serve as a reference for detail enhancement. We first reconstruct dynamic performances from the Scene Rig using 4D Gaussian Splatting, incorporating new model designs and training strategies to improve reconstruction, dynamic range, and rendering quality. Then to render high-quality images for facial closeups, we introduce a diffusion-based detail enhancement model. This model is fine-tuned with high-fidelity data from the same actors recorded in the Face Rig. We train on paired data generated from low- and high-quality Gaussian Splatting (GS) models, using the low-quality input to match the quality of the Scene Rig , with the high-quality GS as ground truth. Our results demonstrate the effectiveness of this pipeline in bridging the gap between the scalable performance capture of a large-scale rig and the high-resolution standards required for film and media production.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">We present a unique system for large-scale, multi-performer, high resolution 4D volumetric capture providing realistic free-viewpoint video up to and including 4K resolution facial closeups. To achieve this, we employ a novel volumetric capture, reconstruction and rendering pipeline based on Dynamic Gaussian Splatting and Diffusion-based Detail Enhancement. We design our pipeline specifically to meet the demands of high-end media production. We employ two capture rigs: the Scene Rig , which captures multi-actor performances at a resolution which falls short of 4K production quality, and the Face Rig , which records high-fidelity single-actor facial detail to serve as a reference for detail enhancement. We first reconstruct dynamic performances from the Scene Rig using 4D Gaussian Splatting, incorporating new model designs and training strategies to improve reconstruction, dynamic range, and rendering quality. Then to render high-quality images for facial closeups, we introduce a diffusion-based detail enhancement model. This model is fine-tuned with high-fidelity data from the same actors recorded in the Face Rig. We train on paired data generated from low- and high-quality Gaussian Splatting (GS) models, using the low-quality input to match the quality of the Scene Rig , with the high-quality GS as ground truth. Our results demonstrate the effectiveness of this pipeline in bridging the gap between the scalable performance capture of a large-scale rig and the high-resolution standards required for film and media production.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1545">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/KeNNaRm5Eo5BAzjm.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763892" target="_blank" rel="noopener">GS-RoadPatching: Inpainting Gaussians via 3D Searching and Placing for Driving Scenes</a></h3>
                    <p class="authors">Guo Chen, Jiarun Liu, Sicong Du, Chenming Wu, Deqi Li, Shi-Sheng Huang</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763892" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1545', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763892')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1545', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_2405">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/7dTwGkimqGzMaEcG.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3764000" target="_blank" rel="noopener">Neural Hamiltonian Deformation Fields for Dynamic Scene Rendering</a></h3>
                    <p class="authors">Hai-Long Qin, Sixian Wang, Guo Lu, Jincheng Dai</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3764000" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_2405', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3764000')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_2405', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1532">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/zWMCCa4DfgSnaWRz.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763889" target="_blank" rel="noopener">MVP4D: Multi-View Portrait Video Diffusion for Animatable 4D Avatars</a></h3>
                    <p class="authors">Felix Taubner, Ruihang Zhang, Mathieu Tuli, Sherwin Bahmani, David B. Lindell</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763889" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1532', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763889')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1532', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1837">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/kNr2Gt1NEbb5w4Yk.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763935" target="_blank" rel="noopener">MV-Performer: Taming Video Diffusion Model for Faithful and Synchronized Multi-view Performer Synthesis</a></h3>
                    <p class="authors">Yihao Zhi, Chenghong Li, Hongjie Liao, Xihe Yang, Zhengwentai Sun, Jiahao Chang</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763935" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1837', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763935')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1837', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            </div>
        </section>
        
        <section class="session">
            <div class="session-header">
                <h2>Advanced Light Transport & PDE Solvers</h2>
                <span class="session-count">6 papers</span>
            </div>
            <div class="papers-grid">
                
            <article class="paper-card" data-paper-id="papers_1176">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/qUS1P5N45MtWRGAX.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3763286" target="_blank" rel="noopener">Jump Restore Light Transport</a></h3>
                    <p class="authors">Sascha Holl, Gurprit Singh, Hans-Peter Seidel</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3763286" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1176', 'url', 'https://dl.acm.org/doi/10.1145/3763286')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1176', 'abstract', 'Markov chain Monte Carlo (MCMC) algorithms are indispensable when sampling from a complex, high-dimensional distribution by a conventional method is intractable. Even though MCMC is a powerful tool, it is also hard to control and tune in practice. Simultaneously achieving both rapid local exploration of the state space and efficient global discovery of the target distribution is a challenging task. In this work, we introduce a novel continuous-time MCMC formulation to the computer science community. Generalizing existing work from the statistics community, we propose a novel framework for adjusting an arbitrary family of Markov processes - used for local exploration of the state space only - to an overall process which is invariant with respect to a target distribution. To demonstrate the potential of our framework, we focus on a simple, but yet insightful, application in light transport simulation. As a by-product, we introduce continuous-time MCMC sampling to the computer graphics community. We show how any existing MCMC-based light transport algorithm can be seamlessly integrated into our framework. We prove empirically and theoretically that the integrated version is superior to the ordinary algorithm. In fact, our approach will convert any existing algorithm into a highly parallelizable variant with shorter running time, smaller error and less variance.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">Markov chain Monte Carlo (MCMC) algorithms are indispensable when sampling from a complex, high-dimensional distribution by a conventional method is intractable. Even though MCMC is a powerful tool, it is also hard to control and tune in practice. Simultaneously achieving both rapid local exploration of the state space and efficient global discovery of the target distribution is a challenging task. In this work, we introduce a novel continuous-time MCMC formulation to the computer science community. Generalizing existing work from the statistics community, we propose a novel framework for adjusting an arbitrary family of Markov processes - used for local exploration of the state space only - to an overall process which is invariant with respect to a target distribution. To demonstrate the potential of our framework, we focus on a simple, but yet insightful, application in light transport simulation. As a by-product, we introduce continuous-time MCMC sampling to the computer graphics community. We show how any existing MCMC-based light transport algorithm can be seamlessly integrated into our framework. We prove empirically and theoretically that the integrated version is superior to the ordinary algorithm. In fact, our approach will convert any existing algorithm into a highly parallelizable variant with shorter running time, smaller error and less variance.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1648">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/og6pSGoWy4ARqRAu.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3763321" target="_blank" rel="noopener">Gaussian Integral Linear Operators for Precomputed Graphics</a></h3>
                    <p class="authors">Haolin Lu, Yash Belhe, Gurprit Singh, Tzu-Mao Li, Toshiya Hachisuka</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3763321" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1648', 'url', 'https://dl.acm.org/doi/10.1145/3763321')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1648', 'abstract', 'Integral linear operators play a key role in many graphics problems, but solutions obtained via Monte Carlo methods often suffer from high variance. A common strategy to improve the efficiency of integration across various inputs is to precompute the kernel function. Traditional methods typically rely on basis expansions for both the input and output functions. However, using fixed output bases can restrict the precision of output reconstruction and limit the compactness of the kernel representation. In this work, we introduce a new method that approximates both the kernel and the input function using Gaussian mixtures. This formulation allows the integral operator to be evaluated analytically, leading to improved flexibility in kernel storage and output representation. Moreover, our method naturally supports the sequential application of multiple operators and enables closed-form operator composition, which is particularly beneficial in tasks involving chains of operators. We demonstrate the versatility and effectiveness of our approach across a variety of graphics problems, including environment map relighting, boundary value problems, and fluorescence rendering.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">Integral linear operators play a key role in many graphics problems, but solutions obtained via Monte Carlo methods often suffer from high variance. A common strategy to improve the efficiency of integration across various inputs is to precompute the kernel function. Traditional methods typically rely on basis expansions for both the input and output functions. However, using fixed output bases can restrict the precision of output reconstruction and limit the compactness of the kernel representation. In this work, we introduce a new method that approximates both the kernel and the input function using Gaussian mixtures. This formulation allows the integral operator to be evaluated analytically, leading to improved flexibility in kernel storage and output representation. Moreover, our method naturally supports the sequential application of multiple operators and enables closed-form operator composition, which is particularly beneficial in tasks involving chains of operators. We demonstrate the versatility and effectiveness of our approach across a variety of graphics problems, including environment map relighting, boundary value problems, and fluorescence rendering.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1659">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/zMNm9qmvKc2HPJP1.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3763322" target="_blank" rel="noopener">Harmonic Caching for Walk on Spheres</a></h3>
                    <p class="authors">Zihong Zhou, Eugene d&#x27;Eon, Rohan Sawhney, Wojciech Jarosz</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3763322" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1659', 'url', 'https://dl.acm.org/doi/10.1145/3763322')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1659', 'abstract', 'We present a variance reduction technique for Walk on Spheres (WoS) that solves elliptic partial differential equations (PDEs) by combining overlapping harmonic expansions of the solution, each estimated using unbiased Monte Carlo random walks. Our method supports Laplace and screened Poisson equations with Dirichlet, Neumann, and Robin boundary conditions in both 2D and 3D. By adaptively covering the domain with local expansion regions and reconstructing the solution inside each region using an infinite Fourier series of the harmonic function, our method achieves over an order of magnitude lower error than traditional pointwise WoS in equal time. While low-order truncations of the series typically introduce limited bias, we also introduce a stochastic truncation scheme that eliminates this bias in the reconstructed solution. Compared to recently developed caching algorithms for WoS, such as Boundary and Mean Value Caching, our approach yields solutions with lower error and fewer correlation artifacts.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">We present a variance reduction technique for Walk on Spheres (WoS) that solves elliptic partial differential equations (PDEs) by combining overlapping harmonic expansions of the solution, each estimated using unbiased Monte Carlo random walks. Our method supports Laplace and screened Poisson equations with Dirichlet, Neumann, and Robin boundary conditions in both 2D and 3D. By adaptively covering the domain with local expansion regions and reconstructing the solution inside each region using an infinite Fourier series of the harmonic function, our method achieves over an order of magnitude lower error than traditional pointwise WoS in equal time. While low-order truncations of the series typically introduce limited bias, we also introduce a stochastic truncation scheme that eliminates this bias in the reconstructed solution. Compared to recently developed caching algorithms for WoS, such as Boundary and Mean Value Caching, our approach yields solutions with lower error and fewer correlation artifacts.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1782">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/E5SB7PginpzDzuN9.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3763333" target="_blank" rel="noopener">Robust Derivative Estimation with Walk on Stars</a></h3>
                    <p class="authors">Zihan Yu, Rohan Sawhney, Bailey Miller, Lifan Wu, Shuang Zhao</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3763333" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1782', 'url', 'https://dl.acm.org/doi/10.1145/3763333')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1782', 'abstract', 'Monte Carlo methods based on the walk on spheres (WoS) algorithm offer a parallel, progressive, and output-sensitive approach for solving partial differential equations (PDEs) in complex geometric domains. Building on this foundation, the walk on stars (WoSt) method generalizes WoS to support mixed Dirichlet, Neumann, and Robin boundary conditions. However, accurately computing spatial derivatives of PDE solutions remains a major challenge: existing methods exhibit high variance and bias near the domain boundary, especially in Neumann-dominated problems. We address this limitation with a new extension of WoSt specifically designed for derivative estimation. Our method reformulates the boundary integral equation (BIE) for Poisson PDEs by directly leveraging the harmonicity of spatial derivatives. Combined with a tailored random-walk sampling scheme and an unbiased early termination strategy, we achieve significantly improved accuracy in derivative estimates near the Neumann boundary. We further demonstrate the effectiveness of our approach across various tasks, including recovering the non-unique solution to a pure Neumann problem with reduced bias and variance, constructing divergence-free vector fields, and optimizing parametrically defined boundaries under PDE constraints.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">Monte Carlo methods based on the walk on spheres (WoS) algorithm offer a parallel, progressive, and output-sensitive approach for solving partial differential equations (PDEs) in complex geometric domains. Building on this foundation, the walk on stars (WoSt) method generalizes WoS to support mixed Dirichlet, Neumann, and Robin boundary conditions. However, accurately computing spatial derivatives of PDE solutions remains a major challenge: existing methods exhibit high variance and bias near the domain boundary, especially in Neumann-dominated problems. We address this limitation with a new extension of WoSt specifically designed for derivative estimation. Our method reformulates the boundary integral equation (BIE) for Poisson PDEs by directly leveraging the harmonicity of spatial derivatives. Combined with a tailored random-walk sampling scheme and an unbiased early termination strategy, we achieve significantly improved accuracy in derivative estimates near the Neumann boundary. We further demonstrate the effectiveness of our approach across various tasks, including recovering the non-unique solution to a pure Neumann problem with reduced bias and variance, constructing divergence-free vector fields, and optimizing parametrically defined boundaries under PDE constraints.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1258">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/FUR2VXRSu3w8Zs5m.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763852" target="_blank" rel="noopener">Off-Centered WoS-Type Solvers with Statistical Weighting</a></h3>
                    <p class="authors">Anchang Bao, Jie Xu, Enya Shen, Jianmin Wang</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763852" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1258', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763852')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1258', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1656">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/9ctqM5azwxCJFf4q.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763903" target="_blank" rel="noopener">An Adjoint Method for Differentiable Fluid Simulation on Flow Maps</a></h3>
                    <p class="authors">Zhiqi Li, Jinjin He, Barnab√°s B√∂rcs√∂k, Taiyuan Zhang, Duowen Chen, Tao Du</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763903" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1656', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763903')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1656', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            </div>
        </section>
        
        <section class="session">
            <div class="session-header">
                <h2>Efficient and Robust Algorithms for Geometric Computing</h2>
                <span class="session-count">3 papers</span>
            </div>
            <div class="papers-grid">
                
            <article class="paper-card" data-paper-id="papers_2317">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/bHf4WuC9uThKXFhm.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763993" target="_blank" rel="noopener">AD-GS: Alternating Densification for Sparse-Input 3D Gaussian Splatting</a></h3>
                    <p class="authors">Gurutva Patle, Nilay Girgaonkar, Nagabhushan Somraj, Rajiv Soundararajan</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763993" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_2317', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763993')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_2317', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_2108">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/64mUKCeNYSWbtCYd.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763977" target="_blank" rel="noopener">Training-Free Instance-Aware 3D Scene Reconstruction and Diffusion-Based View Synthesis from Sparse Images</a></h3>
                    <p class="authors">Jiatong Xia, Lingqiao Liu</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763977" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_2108', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763977')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_2108', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_2307">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/FsbY8qiGVqkmQ7cZ.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763991" target="_blank" rel="noopener">MODepth: Benchmarking Mobile Multi-frame Monocular Depth Estimation with Optical Image Stabilization</a></h3>
                    <p class="authors">Yu Lu, Hao Pan, Dian Ding, Jiatong Ding, Yongjian Fu, Yi-Chao Chen</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763991" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_2307', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763991')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_2307', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            </div>
        </section>
        
        <section class="session">
            <div class="session-header">
                <h2>3D Reconstruction & View Synthesis</h2>
                <span class="session-count">6 papers</span>
            </div>
            <div class="papers-grid">
                
            <article class="paper-card" data-paper-id="papers_1116">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/h92s71rMJn3P2o7s.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3763280" target="_blank" rel="noopener">From Rigging to Waving: 3D-Guided Diffusion for Natural Animation of Hand-Drawn Characters</a></h3>
                    <p class="authors">Jie ZHOU, Linzi QU, Miu-Ling LAM, Hongbo FU</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3763280" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1116', 'url', 'https://dl.acm.org/doi/10.1145/3763280')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1116', 'abstract', 'Hand-drawn character animation is a vibrant research area in computer graphics and presents unique challenges in achieving geometric consistency while conveying expressive motion details. Traditional skeletal animation methods maintain geometric consistency but often struggle with complex non-rigid elements like flowing hair and skirts, resulting in unnatural deformation and missing secondary dynamics. In contrast, video diffusion models effectively synthesize physically plausible dynamics, but exhibit real-human-like characteristics and geometric distortions when applied to stylized drawings due to the domain gap. In this work, we propose a novel hybrid animation system that integrates the strengths of skeletal animation and video diffusion priors. The core idea is to first generate coarse images from characters retargeted with skeletal animations for geometric consistency guidance, and then enhance these images in terms of texture details and secondary dynamics using video diffusion priors. We formulate the enhancement of coarse images as an inpainting task and propose a domain-adapted diffusion model to refine user-masked regions requiring improvement, particularly those involving secondary dynamics. To further enhance motion realism, we propose a Secondary Dynamics Injection (SDI) strategy during the denoising process to incorporate latent features from a pre-trained diffusion model enriched with human motion priors. Additionally, to address unnatural deformation artifacts caused by the integrated hair-body geometry in low-poly single-mesh character modeling, we introduce a Hair Layering Modeling (HLM) technique that employs segmentation maps to separate hair from the body in implicit fields, enabling more natural animation of challenging long-hair characters. Through extensive experiments, we demonstrate that our system outperforms state-of-the-art works in both quantitative and qualitative evaluations. Please refer to our project page (https://lordliang.github.io/From-Rigging-to-Waving) for the code and data for our method.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">Hand-drawn character animation is a vibrant research area in computer graphics and presents unique challenges in achieving geometric consistency while conveying expressive motion details. Traditional skeletal animation methods maintain geometric consistency but often struggle with complex non-rigid elements like flowing hair and skirts, resulting in unnatural deformation and missing secondary dynamics. In contrast, video diffusion models effectively synthesize physically plausible dynamics, but exhibit real-human-like characteristics and geometric distortions when applied to stylized drawings due to the domain gap. In this work, we propose a novel hybrid animation system that integrates the strengths of skeletal animation and video diffusion priors. The core idea is to first generate coarse images from characters retargeted with skeletal animations for geometric consistency guidance, and then enhance these images in terms of texture details and secondary dynamics using video diffusion priors. We formulate the enhancement of coarse images as an inpainting task and propose a domain-adapted diffusion model to refine user-masked regions requiring improvement, particularly those involving secondary dynamics. To further enhance motion realism, we propose a Secondary Dynamics Injection (SDI) strategy during the denoising process to incorporate latent features from a pre-trained diffusion model enriched with human motion priors. Additionally, to address unnatural deformation artifacts caused by the integrated hair-body geometry in low-poly single-mesh character modeling, we introduce a Hair Layering Modeling (HLM) technique that employs segmentation maps to separate hair from the body in implicit fields, enabling more natural animation of challenging long-hair characters. Through extensive experiments, we demonstrate that our system outperforms state-of-the-art works in both quantitative and qualitative evaluations. Please refer to our project page (https://lordliang.github.io/From-Rigging-to-Waving) for the code and data for our method.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1307">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/fnuoviKUJKq9frT1.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763855" target="_blank" rel="noopener">Sketch2PoseNet: Efficient and Generalized Sketch to 3D Human Pose Prediction</a></h3>
                    <p class="authors">Li Wang, Yiyu Zhuang, Yanwen Wang, Xun Cao, Chuan Guo, Xinxin Zuo</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763855" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1307', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763855')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1307', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1495">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/iU1Zr1TUajtMArcz.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763885" target="_blank" rel="noopener">AnimaX: Animating the Inanimate in 3D with Joint Video-Pose Diffusion Models</a></h3>
                    <p class="authors">Zehuan Huang, Haoran Feng, Yang-Tian Sun, Yuan-Chen Guo, Yan-Pei Cao, Lu Sheng</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763885" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1495', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763885')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1495', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1745">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/9rCkaGMPPYMFrLHP.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763916" target="_blank" rel="noopener">Animus3D: Text-driven 3D Animation via Motion Score Distillation</a></h3>
                    <p class="authors">Qi Sun, Can Wang, Jiaxiang Shang, Wensen Feng, Jing Liao</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763916" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1745', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763916')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1745', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1951">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/yparveyNAvr4PMRU.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763952" target="_blank" rel="noopener">X-UniMotion: Animating Human Images with Expressive, Unified and Identity-Agnostic Motion Latents</a></h3>
                    <p class="authors">GUOXIAN SONG, Hongyi Xu, Xiaochen Zhao, You Xie, Tianpei Gu, Zenan Li</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763952" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1951', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763952')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1951', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_2503">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/exJuKTaCNTJA2weJ.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3764007" target="_blank" rel="noopener">FairyGen: Storied Cartoon Video from a Single Child-Drawn Character</a></h3>
                    <p class="authors">Jiayi Zheng, Xiaodong Cun</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3764007" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_2503', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3764007')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_2503', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            </div>
        </section>
        
        <section class="session">
            <div class="session-header">
                <h2>Animating Images, Sketches and Text</h2>
                <span class="session-count">6 papers</span>
            </div>
            <div class="papers-grid">
                
            <article class="paper-card" data-paper-id="papers_1120">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/7a3gQo5RNx17LH8c.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3763282" target="_blank" rel="noopener">Evaluating and Sampling Glinty NDFs in Constant Time</a></h3>
                    <p class="authors">Pauli Kemppinen, Lo√Øs Paulin, Th√©o Thonat, Jean-Marc Thiery, Jaakko Lehtinen, Tamy Boubekeur</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3763282" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1120', 'url', 'https://dl.acm.org/doi/10.1145/3763282')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1120', 'abstract', 'Geometric features between the micro and macro scales produce an expressive family of visual effects grouped under the term "glints". Efficiently rendering these effects amounts to finding the highlights caused by the geometry under each pixel. To allow for fast rendering, we represent our faceted geometry as a 4D point process on an implicit multiscale grid, designed to efficiently find the facets most likely to cause a highlight. The facets\' normals are generated to match a given micro-facet normal distribution such as Trowbridge-Reitz (GGX) or Beckmann, to which our model converges under increasing surface area. Our method is simple to implement, memory-and-precomputation-free, allows for importance sampling and covers a wide range of different appearances such as anisotropic as well as individually colored particles. We provide a base implementation as a standalone fragment shader.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">Geometric features between the micro and macro scales produce an expressive family of visual effects grouped under the term &quot;glints&quot;. Efficiently rendering these effects amounts to finding the highlights caused by the geometry under each pixel. To allow for fast rendering, we represent our faceted geometry as a 4D point process on an implicit multiscale grid, designed to efficiently find the facets most likely to cause a highlight. The facets&#x27; normals are generated to match a given micro-facet normal distribution such as Trowbridge-Reitz (GGX) or Beckmann, to which our model converges under increasing surface area. Our method is simple to implement, memory-and-precomputation-free, allows for importance sampling and covers a wide range of different appearances such as anisotropic as well as individually colored particles. We provide a base implementation as a standalone fragment shader.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1626">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/qC4aJfoC55RsJ2Tx.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763899" target="_blank" rel="noopener">3D SMoE Splatting for Edge-aware Realtime Radiance Field Rendering</a></h3>
                    <p class="authors">Yi-Hsin Li, Thomas Sikora, Sebastian Knorr, M√•rten Sj√∂str√∂m</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763899" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1626', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763899')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1626', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1712">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/xRLS7fBzDbuSngA8.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3763326" target="_blank" rel="noopener">AnySplat: Feed-forward 3D Gaussian Splatting from Unconstrained Views</a></h3>
                    <p class="authors">Lihan Jiang, Yucheng Mao, Linning Xu, Tao Lu, Kerui Ren, Yichen Jin</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3763326" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1712', 'url', 'https://dl.acm.org/doi/10.1145/3763326')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1712', 'abstract', 'We introduce AnySplat, a feed-forward network for novel-view synthesis from uncalibrated image collections. In contrast to traditional neural-rendering pipelines that demand known camera poses and per-scene optimization, or recent feed-forward methods that buckle under the computational weight of dense views‚Äîour model predicts everything in one shot. A single forward pass yields a set of 3D Gaussian primitives encoding both scene geometry and appearance, and the corresponding camera intrinsics and extrinsics for each input image. This unified design scales effortlessly to casually captured, multi-view datasets without any pose annotations. In extensive zero-shot evaluations, AnySplat matches the quality of pose-aware baselines in both sparse- and dense-view scenarios while surpassing existing pose-free approaches. Moreover, it greatly reduces rendering latency compared to optimization-based neural fields, bringing real-time novel-view synthesis within reach for unconstrained capture settings. Project page: https://city-super.github.io/anysplat/.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">We introduce AnySplat, a feed-forward network for novel-view synthesis from uncalibrated image collections. In contrast to traditional neural-rendering pipelines that demand known camera poses and per-scene optimization, or recent feed-forward methods that buckle under the computational weight of dense views‚Äîour model predicts everything in one shot. A single forward pass yields a set of 3D Gaussian primitives encoding both scene geometry and appearance, and the corresponding camera intrinsics and extrinsics for each input image. This unified design scales effortlessly to casually captured, multi-view datasets without any pose annotations. In extensive zero-shot evaluations, AnySplat matches the quality of pose-aware baselines in both sparse- and dense-view scenarios while surpassing existing pose-free approaches. Moreover, it greatly reduces rendering latency compared to optimization-based neural fields, bringing real-time novel-view synthesis within reach for unconstrained capture settings. Project page: https://city-super.github.io/anysplat/.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1253">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/ZFyx1XhmHn5V9P9F.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763851" target="_blank" rel="noopener">PowerGS: Display-Rendering Power Co-Optimization for Neural Rendering in Power-Constrained XR Systems</a></h3>
                    <p class="authors">Weikai Lin, Sushant Kondguli, Carl Marshall, Yuhao Zhu</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763851" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1253', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763851')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1253', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_2051">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/UpxRnhWemoF6iULQ.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763968" target="_blank" rel="noopener">Sparse Cache Updates for Scalable Distributed Effect-Based Rendering</a></h3>
                    <p class="authors">Wolfgang Tatzgern, Pascal Stadlbauer, Joerg H. Mueller, Martin Winter, Martin Sattlecker, Markus Steinberger</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763968" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_2051', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763968')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_2051', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_2009">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/z2DbLwKFs4XqdavV.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763965" target="_blank" rel="noopener">StereoFG: Generating Stereo Frames from Centered Feature Stream</a></h3>
                    <p class="authors">Chenyu Zuo, Yazhen Yuan, Zhizhen Wu, Zhijian Liu, Jingzhen Lan, Ming Fu</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763965" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_2009', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763965')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_2009', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            </div>
        </section>
        
        <section class="session">
            <div class="session-header">
                <h2>Real-Time Rendering & System Optimization</h2>
                <span class="session-count">6 papers</span>
            </div>
            <div class="papers-grid">
                
            <article class="paper-card" data-paper-id="papers_2102">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/rwVfKGQ753Ug7LZm.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3763353" target="_blank" rel="noopener">Imaginarium: Vision-guided High-Quality 3D Scene Layout Generation</a></h3>
                    <p class="authors">Xiaoming Zhu, Xu Huang, Qinghongbing Xie, Zhi Deng, Junsheng Yu, Yirui Guan</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3763353" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_2102', 'url', 'https://dl.acm.org/doi/10.1145/3763353')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_2102', 'abstract', 'Generating artistic and coherent 3D scene layouts is crucial in digital content creation. Traditional optimization-based methods are often constrained by cumbersome manual rules, while deep generative models face challenges in producing content with richness and diversity. Furthermore, approaches that utilize large language models frequently lack robustness and fail to accurately capture complex spatial relationships. To address these challenges, this paper presents a novel vision-guided 3D layout generation system. We first construct a high-quality asset library containing 2,037 scene assets and 147 3D scene layouts. Subsequently, we employ an image generation model to expand prompt representations into images, fine-tuning it to align with our asset library. We then develop a robust image parsing module to recover the 3D layout of scenes based on visual semantics and geometric information. Finally, we optimize the scene layout using scene graphs and overall visual semantics to ensure logical coherence and alignment with the images. Extensive user testing demonstrates that our algorithm significantly outperforms existing methods in terms of layout richness and quality. The code and dataset will be available at https://github.com/HiHiAllen/Imaginarium.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">Generating artistic and coherent 3D scene layouts is crucial in digital content creation. Traditional optimization-based methods are often constrained by cumbersome manual rules, while deep generative models face challenges in producing content with richness and diversity. Furthermore, approaches that utilize large language models frequently lack robustness and fail to accurately capture complex spatial relationships. To address these challenges, this paper presents a novel vision-guided 3D layout generation system. We first construct a high-quality asset library containing 2,037 scene assets and 147 3D scene layouts. Subsequently, we employ an image generation model to expand prompt representations into images, fine-tuning it to align with our asset library. We then develop a robust image parsing module to recover the 3D layout of scenes based on visual semantics and geometric information. Finally, we optimize the scene layout using scene graphs and overall visual semantics to ensure logical coherence and alignment with the images. Extensive user testing demonstrates that our algorithm significantly outperforms existing methods in terms of layout richness and quality. The code and dataset will be available at https://github.com/HiHiAllen/Imaginarium.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1820">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/ejHEAAjCsQio1AQm.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763930" target="_blank" rel="noopener">Procedural Scene Programs for Open-Universe Scene Generation: LLM-Free Error Correction via Program Search</a></h3>
                    <p class="authors">Maxim Gumin, Do Heon Han, Seung Jean Yoo, Aditya Ganeshan, R. Kenny Jones, Kailiang Fu</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763930" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1820', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763930')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1820', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1789">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/qMXMy6Ce97jSpD9T.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763921" target="_blank" rel="noopener">Hierarchical Neural Semantic Representation for 3D Semantic Correspondence</a></h3>
                    <p class="authors">Keyu Du, Jingyu Hu, Haipeng Li, Hao Xu, Haibin Huang, Chi-Wing Fu</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763921" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1789', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763921')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1789', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1817">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/SeNLcAtrsj8y1Waj.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763929" target="_blank" rel="noopener">StyleSculptor: Zero-Shot Style-Controllable 3D Asset Generation with Texture-Geometry Dual Guidance</a></h3>
                    <p class="authors">Zefan Qu, Zhenwei Wang, Haoyuan Wang, Ke Xu, Gerhard Petrus HANCKE, Rynson. W. H. Lau</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763929" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1817', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763929')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1817', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1881">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/pCrZvFDhiaeTKpgQ.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763943" target="_blank" rel="noopener">Fuse3D: Generating 3D Assets Controlled by Multi-Image Fusion</a></h3>
                    <p class="authors">Xuancheng Jin, Rengan Xie, Wenting Zheng, Rui Wang, Hujun Bao, Yuchi Huo</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763943" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1881', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763943')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1881', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1660">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/Q5domPPKzKzzCgpg.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763904" target="_blank" rel="noopener">In-2-4D: Inbetweening from Two Single-View Images to 4D Generation</a></h3>
                    <p class="authors">Sauradip Nag, Daniel Cohen-Or, Hao Zhang, Ali-Mahdavi Amiri</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763904" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1660', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763904')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1660', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            </div>
        </section>
        
        <section class="session">
            <div class="session-header">
                <h2>Cameras, Sensors, and Acquisition</h2>
                <span class="session-count">6 papers</span>
            </div>
            <div class="papers-grid">
                
            <article class="paper-card" data-paper-id="papers_1096">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/zSknozx2ETkNjSYa.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3763278" target="_blank" rel="noopener">Fovea Stacking: Imaging with Dynamic Localized Aberration Correction</a></h3>
                    <p class="authors">Shi Mao, Yogeshwar Nath Mishra, Wolfgang Heidrich</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3763278" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1096', 'url', 'https://dl.acm.org/doi/10.1145/3763278')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1096', 'abstract', 'The desire for cameras with smaller form factors has recently led to a push for exploring computational imaging systems with reduced optical complexity such as a smaller number of lens elements. Unfortunately such simplified optical systems usually suffer from severe aberrations, especially in off-axis regions, which can be difficult to correct purely in software. In this paper we introduce Fovea Stacking, a new type of imaging system that utilizes an emerging dynamic optical component called the deformable phase plate (DPP) for localized aberration correction anywhere on the image sensor. By optimizing DPP deformations through a differentiable optical model, off-axis aberrations are corrected locally, producing a foveated image with enhanced sharpness at the fixation point - analogous to the eye\'s fovea. Stacking multiple such foveated images, each with a different fixation point, yields a composite image free from aberrations. To efficiently cover the entire field of view, we propose joint optimization of DPP deformations under imaging budget constraints. Due to the DPP device\'s non-linear behavior, we introduce a neural network-based control model for improved agreement between simulation and hardware performance. We further demonstrated that for extended depth-of-field imaging, Fovea Stacking outperforms traditional focus stacking in image quality. By integrating object detection or eye-tracking, the system can dynamically adjust the lens to track the object of interest-enabling real-time foveated video suitable for downstream applications such as surveillance or foveated virtual reality displays.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">The desire for cameras with smaller form factors has recently led to a push for exploring computational imaging systems with reduced optical complexity such as a smaller number of lens elements. Unfortunately such simplified optical systems usually suffer from severe aberrations, especially in off-axis regions, which can be difficult to correct purely in software. In this paper we introduce Fovea Stacking, a new type of imaging system that utilizes an emerging dynamic optical component called the deformable phase plate (DPP) for localized aberration correction anywhere on the image sensor. By optimizing DPP deformations through a differentiable optical model, off-axis aberrations are corrected locally, producing a foveated image with enhanced sharpness at the fixation point - analogous to the eye&#x27;s fovea. Stacking multiple such foveated images, each with a different fixation point, yields a composite image free from aberrations. To efficiently cover the entire field of view, we propose joint optimization of DPP deformations under imaging budget constraints. Due to the DPP device&#x27;s non-linear behavior, we introduce a neural network-based control model for improved agreement between simulation and hardware performance. We further demonstrated that for extended depth-of-field imaging, Fovea Stacking outperforms traditional focus stacking in image quality. By integrating object detection or eye-tracking, the system can dynamically adjust the lens to track the object of interest-enabling real-time foveated video suitable for downstream applications such as surveillance or foveated virtual reality displays.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1197">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/16X2GFNiWg5qF5dp.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3763289" target="_blank" rel="noopener">Underwater Optical Backscatter Communications using Acousto-Optic Beam Steering</a></h3>
                    <p class="authors">Atul Rohit Agarwal, Dhawal Sirikonda, Atharv Agashe, Ziang Ren, Dinithi Silva-Sassaman, Charles Carver</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3763289" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1197', 'url', 'https://dl.acm.org/doi/10.1145/3763289')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1197', 'abstract', 'We present a high-speed underwater optical backscatter communication technique based on acousto-optic light steering. Our approach enables underwater assets to transmit data at rates potentially reaching hundreds of Mbps, vastly outperforming current state-of-the-art optical and underwater backscatter systems, which typically operate at only a few kbps. In our system, a base station illuminates the backscatter device with a pulsed laser and captures the retroreflected signal using an ultrafast photodetector. The backscatter device comprises a retroreflector and a 2 MHz ultrasound transducer. The transducer generates pressure waves that dynamically modulate the refractive index of the surrounding medium, steering the light either toward the photodetector (encoding bit 1) or away from it (encoding bit 0). Using a 3-bit redundancy scheme, our prototype achieves a communication rate of approximately 0.66 Mbps with an energy consumption of ‚â§ 1 ŒºJ/bit, representing a 60√ó improvement over prior techniques. We validate its performance through extensive laboratory experiments in which remote underwater assets wirelessly transmit multimedia data to the base station under various environmental conditions.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">We present a high-speed underwater optical backscatter communication technique based on acousto-optic light steering. Our approach enables underwater assets to transmit data at rates potentially reaching hundreds of Mbps, vastly outperforming current state-of-the-art optical and underwater backscatter systems, which typically operate at only a few kbps. In our system, a base station illuminates the backscatter device with a pulsed laser and captures the retroreflected signal using an ultrafast photodetector. The backscatter device comprises a retroreflector and a 2 MHz ultrasound transducer. The transducer generates pressure waves that dynamically modulate the refractive index of the surrounding medium, steering the light either toward the photodetector (encoding bit 1) or away from it (encoding bit 0). Using a 3-bit redundancy scheme, our prototype achieves a communication rate of approximately 0.66 Mbps with an energy consumption of ‚â§ 1 ŒºJ/bit, representing a 60√ó improvement over prior techniques. We validate its performance through extensive laboratory experiments in which remote underwater assets wirelessly transmit multimedia data to the base station under various environmental conditions.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1890">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/BwinnvJHxE4hRNvr.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763945" target="_blank" rel="noopener">Shoot-Bounce-3D: Single-Shot Occlusion-Aware 3D from Lidar by Decomposing Two-Bounce Light</a></h3>
                    <p class="authors">Tzofi Klinghoffer, Siddharth Somasundaram, Xiaoyu Xiang, Yuchen Fan, Christian Richardt, Akshat Dave</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763945" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1890', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763945')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1890', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1794">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/BPrjqT3P3ny9d5GV.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763924" target="_blank" rel="noopener">Robust Single-shot Structured Light 3D Imaging via Neural Feature Decoding</a></h3>
                    <p class="authors">Jiaheng Li, Qiyu Dai, Lihan Li, Praneeth Chakravarthula, He Sun, Baoquan Chen</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763924" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1794', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763924')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1794', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1688">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/tAupxxBpZpMfFFG3.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763911" target="_blank" rel="noopener">Transient LASSO: Transient Large-Scale Scene Reconstruction</a></h3>
                    <p class="authors">Dominik Scheuble, Andrea Ramazzina, Hanno Holzh√ºter, Stefano Gasperini, Steven Peters, Federico Tombari</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763911" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1688', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763911')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1688', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_2209">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/owa7GfB1zadyUCCB.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763986" target="_blank" rel="noopener">LookUp3D: Data-Driven 3D Scanning</a></h3>
                    <p class="authors">Giancarlo Pereira, Yidan Gao, Yurii Piadyk, David Fouhey, Claudio Silva, Daniele Panozzo</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763986" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_2209', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763986')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_2209', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            </div>
        </section>
        
        <section class="session">
            <div class="session-header">
                <h2>Generative 3D Modeling</h2>
                <span class="session-count">6 papers</span>
            </div>
            <div class="papers-grid">
                
            <article class="paper-card" data-paper-id="papers_1004">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/Xa1gu6THUNxTmDsD.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763811" target="_blank" rel="noopener">Motion2Motion: Cross-topology Motion Retargeting with Sparse Correspondence</a></h3>
                    <p class="authors">Ling-Hao Chen, Yuhong Zhang, Zixin Yin, Zhiyang Dou, Xin Chen, Jingbo Wang</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763811" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1004', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763811')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1004', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_2085">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/dbTVA1HGkSsHaGdx.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3763351" target="_blank" rel="noopener">Ultrafast and Controllable Online Motion Retargeting for Game Scenarios</a></h3>
                    <p class="authors">Tianze Guo, Zhedong Chen, Yi Jiang, Linjun Wu, Xilei Wei, Lang Xu</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3763351" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_2085', 'url', 'https://dl.acm.org/doi/10.1145/3763351')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_2085', 'abstract', 'Geometry-aware online motion retargeting is crucial for real-time character animation in gaming and virtual reality. However, existing methods often rely on complex optimization procedures or deep neural networks, which constrain their applicability in real-time scenarios. Moreover, they offer limited control over fine-grained motion details involved in character interactions, resulting in less realistic outcomes. To overcome these limitations, we propose a novel optimization framework for ultrafast, lightweight motion retargeting with joint-level control (i.e., controls over joint position, bone orientation, etc,). Our approach introduces a semantic-aware objective grounded in a spherical geometry representation, coupled with a bone-length-preserving algorithm that iteratively solves this objective. This formulation preserves spatial relationships among spheres, thereby maintaining motion semantics, mitigating interpenetration, and ensuring contact. It is lightweight and computationally efficient, making it particularly suitable for time-critical real-time deployment scenarios. Additionally, we incorporate a heuristic optimization strategy that enables rapid convergence and precise joint-level control. We evaluate our method against state-of-the-art approaches on the Mixamo dataset, and experimental results demonstrate that it achieves comparable performance while delivering an order-of-magnitude speedup.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">Geometry-aware online motion retargeting is crucial for real-time character animation in gaming and virtual reality. However, existing methods often rely on complex optimization procedures or deep neural networks, which constrain their applicability in real-time scenarios. Moreover, they offer limited control over fine-grained motion details involved in character interactions, resulting in less realistic outcomes. To overcome these limitations, we propose a novel optimization framework for ultrafast, lightweight motion retargeting with joint-level control (i.e., controls over joint position, bone orientation, etc,). Our approach introduces a semantic-aware objective grounded in a spherical geometry representation, coupled with a bone-length-preserving algorithm that iteratively solves this objective. This formulation preserves spatial relationships among spheres, thereby maintaining motion semantics, mitigating interpenetration, and ensuring contact. It is lightweight and computationally efficient, making it particularly suitable for time-critical real-time deployment scenarios. Additionally, we incorporate a heuristic optimization strategy that enables rapid convergence and precise joint-level control. We evaluate our method against state-of-the-art approaches on the Mixamo dataset, and experimental results demonstrate that it achieves comparable performance while delivering an order-of-magnitude speedup.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1529">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/BYrnExspZB2UtwBH.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3763309" target="_blank" rel="noopener">SMF: Template-free and Rig-free Animation Transfer using Kinetic Codes</a></h3>
                    <p class="authors">Sanjeev Muralikrishnan, Niladri Shekhar Dutt, Niloy J. Mitra</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3763309" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1529', 'url', 'https://dl.acm.org/doi/10.1145/3763309')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1529', 'abstract', 'Animation retargetting applies sparse motion description (e.g., keypoint sequences) to a character mesh to produce a semantically plausible and temporally coherent full-body mesh sequence. Existing approaches come with restrictions - they require access to template-based shape priors or artist-designed deformation rigs, suffer from limited generalization to unseen motion and/or shapes, or exhibit motion jitter. We propose Self-supervised Motion Fields (SMF), a self-supervised framework that is trained with only sparse motion representations, without requiring dataset-specific annotations, templates, or rigs. At the heart of our method are Kinetic Codes, a novel autoencoder-based sparse motion encoding, that exposes a semantically rich latent space, simplifying large-scale training. Our architecture comprises dedicated spatial and temporal gradient predictors, which are jointly trained in an end-to-end fashion. The combined network, regularized by the Kinetic Codes\' latent space, has good generalization across both unseen shapes and new motions. We evaluated our method on unseen motion sampled from AMASS, D4D, Mixamo, and raw monocular video for animation transfer on various characters with varying shapes and topology. We report a new SoTA on the AMASS dataset in the context of generalization to unseen motion.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">Animation retargetting applies sparse motion description (e.g., keypoint sequences) to a character mesh to produce a semantically plausible and temporally coherent full-body mesh sequence. Existing approaches come with restrictions - they require access to template-based shape priors or artist-designed deformation rigs, suffer from limited generalization to unseen motion and/or shapes, or exhibit motion jitter. We propose Self-supervised Motion Fields (SMF), a self-supervised framework that is trained with only sparse motion representations, without requiring dataset-specific annotations, templates, or rigs. At the heart of our method are Kinetic Codes, a novel autoencoder-based sparse motion encoding, that exposes a semantically rich latent space, simplifying large-scale training. Our architecture comprises dedicated spatial and temporal gradient predictors, which are jointly trained in an end-to-end fashion. The combined network, regularized by the Kinetic Codes&#x27; latent space, has good generalization across both unseen shapes and new motions. We evaluated our method on unseen motion sampled from AMASS, D4D, Mixamo, and raw monocular video for animation transfer on various characters with varying shapes and topology. We report a new SoTA on the AMASS dataset in the context of generalization to unseen motion.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1949">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/CdLpUE2B1CVVAwK1.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763951" target="_blank" rel="noopener">PhysHMR: Learning Humanoid Control Policies from Vision for Physically Plausible Human Motion Reconstruction</a></h3>
                    <p class="authors">Qiao Feng, Yiming Huang, Yufu Wang, Jiatao Gu, Lingjie Liu</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763951" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1949', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763951')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1949', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1425">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/PyQattvQEvgPV5qB.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763874" target="_blank" rel="noopener">Generating Detailed Character Motion from Blocking Poses</a></h3>
                    <p class="authors">Purvi Goel, Guy Tevet, C. Karen Liu, Kayvon Fatahalian</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763874" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1425', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763874')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1425', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1834">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/WeYgeXByNajscb7Z.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3>MaskedManipulator: Versatile Whole-Body Manipulation</h3>
                    <p class="authors">Chen Tessler, Yifeng Jiang, Erwin Coumans, Zhengyi Luo, Gal Chechik, Xue Bin Peng</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <button class="edit-btn" onclick="openEditModal('papers_1834', 'url', '')" title="Add URL">‚úèÔ∏è Add link</button>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1834', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            </div>
        </section>
        
        <section class="session">
            <div class="session-header">
                <h2>Motion Transfer & Control</h2>
                <span class="session-count">6 papers</span>
            </div>
            <div class="papers-grid">
                
            <article class="paper-card" data-paper-id="papers_1209">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/WQP2fYFadJgKgc8i.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763845" target="_blank" rel="noopener">FreeArt3D: Training-Free Articulated Object Generation using 3D Diffusion</a></h3>
                    <p class="authors">Chuhao Chen, Isabella Liu, Xinyue Wei, Hao Su, Minghua Liu</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763845" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1209', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763845')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1209', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1551">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/z6J2i7JurdSm4fD6.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763893" target="_blank" rel="noopener">Generating Objects with Part-Articulation from a Single Image</a></h3>
                    <p class="authors">Ruijie Lu, Yu Liu, Jiaxiang Tang, Junfeng Ni, Yuxiang Wang, Diwen Wan</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763893" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1551', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763893')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1551', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1208">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/Vsh4SMkV5LJaWwgm.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763844" target="_blank" rel="noopener">LARM: A Large Articulated Object Reconstruction Model</a></h3>
                    <p class="authors">Sylvia Yuan, Ruoxi Shi, Xinyue Wei, Xiaoshuai Zhang, Hao Su, Minghua Liu</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763844" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1208', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763844')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1208', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1790">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/m8q1MzqZ82Fziwjy.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763922" target="_blank" rel="noopener">ArtiLatent: Realistic Articulated 3D Object Generation via Structured Latents</a></h3>
                    <p class="authors">Honghua Chen, Yushi Lan, Yongwei Chen, Xingang Pan</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763922" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1790', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763922')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1790', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_2073">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/48YwY6HNJCw2UZNB.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763972" target="_blank" rel="noopener">Assembler: Scalable 3D Part Assembly via Anchor Point Diffusion</a></h3>
                    <p class="authors">Wang Zhao, Yanpei Cao, Jiale Xu, Yuejiang Dong, Ying Shan</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763972" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_2073', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763972')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_2073', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1332">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/NuoL8Cj4iHP5rodr.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763857" target="_blank" rel="noopener">LLM-Primitives: Large Language Model for 3D Reconstruction with Primitives</a></h3>
                    <p class="authors">kuan tian, zhihao hu, yonghang guan, jun zhang</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763857" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1332', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763857')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1332', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            </div>
        </section>
        
        <section class="session">
            <div class="session-header">
                <h2>Advanced Fluid and Multiphase Simulation</h2>
                <span class="session-count">4 papers</span>
            </div>
            <div class="papers-grid">
                
            <article class="paper-card" data-paper-id="papers_1424">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/7CLJg18ozD1LToCK.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3763303" target="_blank" rel="noopener">ConsiStyle: Style Diversity in Training-Free Consistent T2I Generation</a></h3>
                    <p class="authors">Yohai Mazuz, Janna Bruner, Lior Wolf</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3763303" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1424', 'url', 'https://dl.acm.org/doi/10.1145/3763303')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1424', 'abstract', 'In text-to-image models, consistent character generation is the task of achieving text alignment while maintaining the subject\'s appearance across different prompts. However, since style and appearance are often entangled, the existing methods struggle to preserve consistent subject characteristics while adhering to varying style prompts. Current approaches for consistent text-to-image generation typically rely on large-scale fine-tuning on curated image sets or per-subject optimization, which either fail to generalize across prompts or do not align well with textual descriptions. Meanwhile, training-free methods often fail to maintain subject consistency across different styles. In this work, we introduce a training-free method that, for the first time, jointly achieves style preservation and subject consistency across varied styles. The attention matrices are manipulated such that Queries and Keys are obtained from the anchor image(s) that are used to define the subject, while the Values are imported from a parallel copy that is not subject-anchored. Additionally, cross-image components are added to the self-attention mechanism by expanding the Key and Value matrices. To do without shifting from the target style, we align the statistics of the Value matrices. As is demonstrated in a comprehensive battery of qualitative and quantitative experiments, our method effectively decouples style from subject appearance and enables faithful generation of text-aligned images with consistent characters across diverse styles. Code will be available at our project page: jbruner23.github.io/consistyle.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">In text-to-image models, consistent character generation is the task of achieving text alignment while maintaining the subject&#x27;s appearance across different prompts. However, since style and appearance are often entangled, the existing methods struggle to preserve consistent subject characteristics while adhering to varying style prompts. Current approaches for consistent text-to-image generation typically rely on large-scale fine-tuning on curated image sets or per-subject optimization, which either fail to generalize across prompts or do not align well with textual descriptions. Meanwhile, training-free methods often fail to maintain subject consistency across different styles. In this work, we introduce a training-free method that, for the first time, jointly achieves style preservation and subject consistency across varied styles. The attention matrices are manipulated such that Queries and Keys are obtained from the anchor image(s) that are used to define the subject, while the Values are imported from a parallel copy that is not subject-anchored. Additionally, cross-image components are added to the self-attention mechanism by expanding the Key and Value matrices. To do without shifting from the target style, we align the statistics of the Value matrices. As is demonstrated in a comprehensive battery of qualitative and quantitative experiments, our method effectively decouples style from subject appearance and enables faithful generation of text-aligned images with consistent characters across diverse styles. Code will be available at our project page: jbruner23.github.io/consistyle.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1676">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/UL3TmkXDQ6RDmtTZ.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763906" target="_blank" rel="noopener">Bokeh Diffusion: Defocus Blur Control in Text-to-Image Diffusion Models</a></h3>
                    <p class="authors">Armando Fortes, Tianyi Wei, Shangchen Zhou, Xingang Pan</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763906" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1676', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763906')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1676', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1961">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/Mq9oDsqQBxFNdPa6.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3763342" target="_blank" rel="noopener">PractiLight: Practical Light Control Using Foundational Diffusion Models</a></h3>
                    <p class="authors">Yotam Erel, Rishabh Dabral, Vladislav Golyanik, Amit H. Bermano, Christian Theobalt</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3763342" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1961', 'url', 'https://dl.acm.org/doi/10.1145/3763342')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1961', 'abstract', 'Light control in generated images is a difficult task, posing specific challenges, spanning over the entire image and frequency spectrum. Most approaches tackle this problem by training on extensive yet domain-specific datasets, limiting the inherent generalization and applicability of the foundational backbones used. Instead, PractiLight is a practical approach, effectively leveraging foundational understanding of recent generative models for the task. Our key insight is that lighting relationships in an image are similar in nature to token interaction in self-attention layers, and hence are best represented there. Based on this and other analyses regarding the importance of early diffusion iterations, PractiLight trains a lightweight LoRA regressor to produce the direct-irradiance map for a given image, using a small set of training images. We then employ this regressor to incorporate the desired lighting into the generation process of another image using Classifier Guidance. This careful design generalizes well to diverse conditions and image domains. We demonstrate state-of-the-art performance in terms of quality and control with proven parameter and data efficiency compared to leading works over a wide variety of scene types. We hope this work affirms that image lighting can feasibly be controlled by tapping into foundational knowledge, enabling practical and general relighting.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">Light control in generated images is a difficult task, posing specific challenges, spanning over the entire image and frequency spectrum. Most approaches tackle this problem by training on extensive yet domain-specific datasets, limiting the inherent generalization and applicability of the foundational backbones used. Instead, PractiLight is a practical approach, effectively leveraging foundational understanding of recent generative models for the task. Our key insight is that lighting relationships in an image are similar in nature to token interaction in self-attention layers, and hence are best represented there. Based on this and other analyses regarding the importance of early diffusion iterations, PractiLight trains a lightweight LoRA regressor to produce the direct-irradiance map for a given image, using a small set of training images. We then employ this regressor to incorporate the desired lighting into the generation process of another image using Classifier Guidance. This careful design generalizes well to diverse conditions and image domains. We demonstrate state-of-the-art performance in terms of quality and control with proven parameter and data efficiency compared to leading works over a wide variety of scene types. We hope this work affirms that image lighting can feasibly be controlled by tapping into foundational knowledge, enabling practical and general relighting.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_2210">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/npgi998LdshjxwUf.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763987" target="_blank" rel="noopener">Zero-Shot Dynamic Concept Personalization with Grid-Based LoRA</a></h3>
                    <p class="authors">Rameen Abdal, Or Patashnik, Ekaterina Deyneka, Hao Chen, Aliaksandr Siarohin, Sergey Tulyakov</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763987" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_2210', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763987')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_2210', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            </div>
        </section>
        
        <section class="session">
            <div class="session-header">
                <h2>Material & Reflectance Modeling</h2>
                <span class="session-count">6 papers</span>
            </div>
            <div class="papers-grid">
                
            <article class="paper-card" data-paper-id="papers_1001">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/wBqgRU358wfYzmUK.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763809" target="_blank" rel="noopener">Diffusion-Guided Relighting for Single-Image SVBRDF Estimation</a></h3>
                    <p class="authors">Youxin Xing, Zheng Zeng, Youyang Du, Lu Wang, Beibei Wang</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763809" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1001', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763809')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1001', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1674">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/1wtDGGvCmmmXFWNp.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763905" target="_blank" rel="noopener">EBREnv: SVBRDF Estimation in Uncontrolled Environment Lighting via Exemplar-Based Representation</a></h3>
                    <p class="authors">Li Wang, Jiajun Zhao, Lianghao Zhang, Fangzhou Gao, Jiawan Zhang</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763905" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1674', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763905')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1674', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1689">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/U3VUcZ67AtyJBNmj.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3763324" target="_blank" rel="noopener">Sparse SVBRDF Acquisition via Importance-Aware Illumination Multiplexing</a></h3>
                    <p class="authors">Lianghao Zhang, Zixuan Wang, Li Wang, Fangzhou Gao, Ruya Sun, Jiawan Zhang</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3763324" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1689', 'url', 'https://dl.acm.org/doi/10.1145/3763324')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1689', 'abstract', 'Reflectance acquisition from sparse images has been a long-standing problem in computer graphics. Previous works have addressed this by introducing either material-related priors or illumination multiplexing with a general sampling strategy. However, fixed lighting patterns in multiplexing can lead to redundant sampling and entangled observations, making it necessary to adaptively capture salient reflectance responses in each shot based on material behavior. In this paper, we propose combining adaptive sampling with illumination multiplexing for SVBRDF reconstruction from sparse images lit by a planar light source. Central to our method is the modeling of a sampling importance distribution on lighting surface, guided by the statistical nature of microfacet theory. Based on this sampling structure, our framework jointly trains networks to learn an adaptive sampling strategy in the lighting domain, and furthermore, approximately separates pure specular-related information from observations to reduce ambiguities in reconstruction. We validate our approach through experiments and comparisons with previous works on both synthetic and real materials.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">Reflectance acquisition from sparse images has been a long-standing problem in computer graphics. Previous works have addressed this by introducing either material-related priors or illumination multiplexing with a general sampling strategy. However, fixed lighting patterns in multiplexing can lead to redundant sampling and entangled observations, making it necessary to adaptively capture salient reflectance responses in each shot based on material behavior. In this paper, we propose combining adaptive sampling with illumination multiplexing for SVBRDF reconstruction from sparse images lit by a planar light source. Central to our method is the modeling of a sampling importance distribution on lighting surface, guided by the statistical nature of microfacet theory. Based on this sampling structure, our framework jointly trains networks to learn an adaptive sampling strategy in the lighting domain, and furthermore, approximately separates pure specular-related information from observations to reduce ambiguities in reconstruction. We validate our approach through experiments and comparisons with previous works on both synthetic and real materials.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1240">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/BdCHxFh4gtJvefKU.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763848" target="_blank" rel="noopener">Chord: Chain of Rendering Decomposition for PBR Material Estimation from Generated Texture Images</a></h3>
                    <p class="authors">Zhi Ying, Boxiang Rong, Jingyu Wang, Maoyuan Xu</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763848" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1240', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763848')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1240', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1843">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/Fxb7vQNZ1mQGNXJp.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763936" target="_blank" rel="noopener">AniTex: Light-Geometry Consistent PBR Material Generation for Animatable Objects</a></h3>
                    <p class="authors">Jieting Xu, Ziyi Xu, Guoyuan An, Yiwei Hu, Rengan Xie, Zhijian Liu</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763936" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1843', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763936')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1843', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1261">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/Nq6CpoXHfXnQFEP1.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763853" target="_blank" rel="noopener">Hyperspectral Polarimetric BRDFs of Real-world Materials</a></h3>
                    <p class="authors">Yunseong Moon, Ryota Maeda, Suhyun Shin, Inseung Hwang, Youngchan Kim, Min H. Kim</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763853" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1261', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763853')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1261', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            </div>
        </section>
        
        <section class="session">
            <div class="session-header">
                <h2>Objects in Parts & Articulation</h2>
                <span class="session-count">6 papers</span>
            </div>
            <div class="papers-grid">
                
            <article class="paper-card" data-paper-id="papers_1103">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/EL952io6rjZhDqdd.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3763279" target="_blank" rel="noopener">The Granule-In-Cell Method for Simulating Sand--Water Mixtures</a></h3>
                    <p class="authors">Yizao Tang, Yuechen Zhu, Xingyu Ni, Baoquan Chen</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3763279" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1103', 'url', 'https://dl.acm.org/doi/10.1145/3763279')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1103', 'abstract', 'The simulation of sand-water mixtures requires capturing the stochastic behavior of individual sand particles within a uniform, continuous fluid medium. However, most existing approaches, which only treat sand particles as markers within fluid solvers, fail to account for both the forces acting on individual sand particles and the collective feedback of the particle assemblies on the fluid. This prevents faithful reproduction of characteristic phenomena including transport, deposition, and clogging. Building upon kinetic ensemble averaging technique, we propose a physically consistent coupling strategy and introduce a novel Granule-In-Cell (GIC) method for modeling such sand-water interactions. We employ the Discrete Element Method (DEM) to capture fine-scale granule dynamics and the Particle-In-Cell (PIC) method for continuous spatial representation and density projection. To bridge these two frameworks, we treat granules as macroscopic transport flow rather than solid boundaries within the fluid domain. This bidirectional coupling allows our model to incorporate a range of interphase forces using different discretization schemes, resulting in more realistic simulations that strictly adhere to the mass conservation law. Experimental results demonstrate the effectiveness of our method in simulating complex sand-water interactions, uniquely capturing intricate physical phenomena and ensuring exact volume preservation compared to existing approaches.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">The simulation of sand-water mixtures requires capturing the stochastic behavior of individual sand particles within a uniform, continuous fluid medium. However, most existing approaches, which only treat sand particles as markers within fluid solvers, fail to account for both the forces acting on individual sand particles and the collective feedback of the particle assemblies on the fluid. This prevents faithful reproduction of characteristic phenomena including transport, deposition, and clogging. Building upon kinetic ensemble averaging technique, we propose a physically consistent coupling strategy and introduce a novel Granule-In-Cell (GIC) method for modeling such sand-water interactions. We employ the Discrete Element Method (DEM) to capture fine-scale granule dynamics and the Particle-In-Cell (PIC) method for continuous spatial representation and density projection. To bridge these two frameworks, we treat granules as macroscopic transport flow rather than solid boundaries within the fluid domain. This bidirectional coupling allows our model to incorporate a range of interphase forces using different discretization schemes, resulting in more realistic simulations that strictly adhere to the mass conservation law. Experimental results demonstrate the effectiveness of our method in simulating complex sand-water interactions, uniquely capturing intricate physical phenomena and ensuring exact volume preservation compared to existing approaches.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1192">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/2y6uePeHG1juM4nT.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3763288" target="_blank" rel="noopener">Kinetic Free-Surface Flows and Foams with Sharp Interfaces</a></h3>
                    <p class="authors">Haoxiang Wang, Kui Wu, Hui Qiao, Mathieu Desbrun, Wei Li</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3763288" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1192', 'url', 'https://dl.acm.org/doi/10.1145/3763288')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1192', 'abstract', 'Kinetic multiphase flow solvers have recently demonstrated exquisitely complex and turbulent fluid phenomena involving splashing and bubbling. However, they require full simulation of both the liquid phase and the air to capture a large spectrum of fluid behaviors. Moreover, they rely on diffuse interface tracking to properly account for the interfacial forces involved in fluid-air interactions. Consequently, simulating visually appealing fluids is extremely compute intensive given the required resolution to capture small bubbles, and foam simulation is unattainable with this family of methods. While water simulation involves density and viscosity differences between the two phases so large that one can safely ignore the dynamics of air, so-called kinetic free-surface solvers that only consider the liquid motion have been unable to reproduce the full gamut of turbulent fluid behaviors, being often unstable for even moderately complex scenarios. By revisiting kinetic solvers using sharp interfaces and incorporating recent advances in single-phase and multiphase LBM solvers, we propose a free-surface kinetic solver, which we call HOME-FREE LBM, that not only handles turbulence, glugging, and bubbling, but even foam where bubbles stick to each other through surface tension. We demonstrate that our fluid simulator allows for fast and robust bubble growth, breakup, and coalescence, at a fraction of the computational time that existing CG fluid solvers require.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">Kinetic multiphase flow solvers have recently demonstrated exquisitely complex and turbulent fluid phenomena involving splashing and bubbling. However, they require full simulation of both the liquid phase and the air to capture a large spectrum of fluid behaviors. Moreover, they rely on diffuse interface tracking to properly account for the interfacial forces involved in fluid-air interactions. Consequently, simulating visually appealing fluids is extremely compute intensive given the required resolution to capture small bubbles, and foam simulation is unattainable with this family of methods. While water simulation involves density and viscosity differences between the two phases so large that one can safely ignore the dynamics of air, so-called kinetic free-surface solvers that only consider the liquid motion have been unable to reproduce the full gamut of turbulent fluid behaviors, being often unstable for even moderately complex scenarios. By revisiting kinetic solvers using sharp interfaces and incorporating recent advances in single-phase and multiphase LBM solvers, we propose a free-surface kinetic solver, which we call HOME-FREE LBM, that not only handles turbulence, glugging, and bubbling, but even foam where bubbles stick to each other through surface tension. We demonstrate that our fluid simulator allows for fast and robust bubble growth, breakup, and coalescence, at a fraction of the computational time that existing CG fluid solvers require.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1701">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/mmoHWZdQJoUpHsrk.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3763325" target="_blank" rel="noopener">Implicit Incompressible Porous Flow using SPH</a></h3>
                    <p class="authors">Timna B√∂ttcher, Lukas Westhofen, Stefan Rhys Jeske, Jan Bender</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3763325" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1701', 'url', 'https://dl.acm.org/doi/10.1145/3763325')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1701', 'abstract', 'We present a novel implicit porous flow solver using SPH, which maintains fluid incompressibility and is able to model a wide range of scenarios, driven by strongly coupled solid-fluid interaction forces. Many previous SPH porous flow methods reduce particle volumes as they transition across the solid-fluid interface, resulting in significant stability issues. We instead allow fluid and solid to overlap by deriving a new density estimation. This further allows us to extend SPH pressure solvers to take local porosity into account and results in strict enforcement of incompressibility. As a result, we can simulate porous flow using physically consistent pressure forces between fluid and solid. In contrast to previous SPH porous flow methods, which use explicit forces for internal fluid flow, we employ implicit non-pressure forces. These we solve as a linear system and strongly couple with fluid viscosity and solid elasticity. We capture the most common effects observed in porous flow, namely drag, buoyancy and capillary action due to adhesion. To achieve elastic behavior change based on local fluid saturation, such as bloating or softening, we propose an extension to the elasticity model. We demonstrate the efficacy of our model with various simulations that showcase the different aspects of porous flow behavior. To summarize, our system of strongly coupled non-pressure forces and enforced incompressibility across overlapping phases allows us to naturally model and stably simulate complex porous interactions.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">We present a novel implicit porous flow solver using SPH, which maintains fluid incompressibility and is able to model a wide range of scenarios, driven by strongly coupled solid-fluid interaction forces. Many previous SPH porous flow methods reduce particle volumes as they transition across the solid-fluid interface, resulting in significant stability issues. We instead allow fluid and solid to overlap by deriving a new density estimation. This further allows us to extend SPH pressure solvers to take local porosity into account and results in strict enforcement of incompressibility. As a result, we can simulate porous flow using physically consistent pressure forces between fluid and solid. In contrast to previous SPH porous flow methods, which use explicit forces for internal fluid flow, we employ implicit non-pressure forces. These we solve as a linear system and strongly couple with fluid viscosity and solid elasticity. We capture the most common effects observed in porous flow, namely drag, buoyancy and capillary action due to adhesion. To achieve elastic behavior change based on local fluid saturation, such as bloating or softening, we propose an extension to the elasticity model. We demonstrate the efficacy of our model with various simulations that showcase the different aspects of porous flow behavior. To summarize, our system of strongly coupled non-pressure forces and enforced incompressibility across overlapping phases allows us to naturally model and stably simulate complex porous interactions.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1850">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/5puW22yVjARXMWXi.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3763338" target="_blank" rel="noopener">Fire-X: Extinguishing Fire with Stoichiometric Heat Release</a></h3>
                    <p class="authors">Helge Wrede, Anton Wagner, Sarker Miraz Mahfuz, Wojtek Palubicki, Dominik Michels, S√∂ren Pirk</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3763338" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1850', 'url', 'https://dl.acm.org/doi/10.1145/3763338')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1850', 'abstract', 'We present a novel combustion simulation framework to model fire phenomena across solids, liquids, and gases. Our approach extends traditional fluid solvers by incorporating multi-species thermodynamics and reactive transport for fuel, oxygen, nitrogen, carbon dioxide, water vapor, and residuals. Combustion reactions are governed by stoichiometry-dependent heat release, allowing an accurate simulation of premixed and diffusive flames with varying intensity and composition. We support a wide range of scenarios including jet fires, water suppression (sprays and sprinklers), fuel evaporation, and starvation conditions. Our framework enables interactive heat sources, fire detectors, and realistic rendering of flames (e.g., laminar-to-turbulent transitions and blue-to-orange color shifts). Our key contributions include the tight coupling of species dynamics with thermodynamic feedback, evaporation modeling, and a hybrid SPH-grid representation for the efficient simulation of extinguishing fires. We validate our method through numerous experiments that demonstrate its versatility in both indoor and outdoor fire scenarios.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">We present a novel combustion simulation framework to model fire phenomena across solids, liquids, and gases. Our approach extends traditional fluid solvers by incorporating multi-species thermodynamics and reactive transport for fuel, oxygen, nitrogen, carbon dioxide, water vapor, and residuals. Combustion reactions are governed by stoichiometry-dependent heat release, allowing an accurate simulation of premixed and diffusive flames with varying intensity and composition. We support a wide range of scenarios including jet fires, water suppression (sprays and sprinklers), fuel evaporation, and starvation conditions. Our framework enables interactive heat sources, fire detectors, and realistic rendering of flames (e.g., laminar-to-turbulent transitions and blue-to-orange color shifts). Our key contributions include the tight coupling of species dynamics with thermodynamic feedback, evaporation modeling, and a hybrid SPH-grid representation for the efficient simulation of extinguishing fires. We validate our method through numerous experiments that demonstrate its versatility in both indoor and outdoor fire scenarios.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1993">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/K2A93StMkt5JSoxs.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763960" target="_blank" rel="noopener">Multiphase Particle-Based Simulation of Poro-Elasto-Capillary Effects</a></h3>
                    <p class="authors">Ruolan Li, Yanrui Xu, Yalan Zhang, Jiri Kosinka, Alexandru C. Telea, Jian Chang</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763960" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1993', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763960')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1993', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_2284">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/QMv5cxaGxs1Ett19.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3763364" target="_blank" rel="noopener">Wavelet Fluids</a></h3>
                    <p class="authors">Luan Lyu, Xiaohua Ren, Wei Cao, Jian Zhu, Ziyang Ma, Enhua Wu</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3763364" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_2284', 'url', 'https://dl.acm.org/doi/10.1145/3763364')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_2284', 'abstract', 'This paper introduces a novel wavelet-based framework for simulating both single-phase (e.g., smoke) and two-phase (e.g., bubbly water) flows, featuring unified boundary condition handling for free surfaces and solid obstacles. In liquid simulations, conventional pressure projection methods enforce zero-pressure Dirichlet conditions at free surfaces by solving a simplified pressure Poisson equation. However, these approaches neglect air-phase incompressibility, leading to artificial bubble collapse. Stream function methods overcome this limitation by solving a density-variable vector potential Poisson equation, ensuring incompressibility in both simulated and unsimulated regions while maintaining divergence-free liquid phases independent of solver accuracy. Yet, they triple the linear system\'s dimensionality and exhibit poor convergence near solid boundaries. The fundamental limitation of both methods stems from their governing equations: singularities emerge as density approaches extreme values. The pressure Poisson equation becomes ill-conditioned when density nears zero (air phase), compromising air-phase incompressibility, while the vector potential equation degrades as density approaches infinity (solid phase), impeding solid-boundary convergence. To address these singularities, we first propose a novel decomposition where zero and infinite densities are well-defined. We then reformulate this decomposition as a fixed-point iteration using density-agnostic curl-free and divergence-free projections, eliminating the need for linear system solves. The error equation is derived, and a necessary and sufficient convergence condition is established. Building on this, we develop an iterative algorithm that efficiently solves the fixed-point problem through alternating wavelet-based non-orthogonal curl-free and divergence-free projections. Additionally, we investigate orthogonal curl-free projections (e.g., Fourier methods) and their complementary divergence-free counterparts, providing a comprehensive comparison between wavelet and Fourier approaches. Our method simultaneously computes pressure and stream functions, retaining the incompressibility benefits of stream function approaches while resolving their computational inefficiencies and solid-boundary convergence issues. Experiments demonstrate our framework\'s ability to efficiently simulate complex two-phase phenomena, such as the glugging effect during water pouring and multi-liquid-region interactions across zero-density air.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">This paper introduces a novel wavelet-based framework for simulating both single-phase (e.g., smoke) and two-phase (e.g., bubbly water) flows, featuring unified boundary condition handling for free surfaces and solid obstacles. In liquid simulations, conventional pressure projection methods enforce zero-pressure Dirichlet conditions at free surfaces by solving a simplified pressure Poisson equation. However, these approaches neglect air-phase incompressibility, leading to artificial bubble collapse. Stream function methods overcome this limitation by solving a density-variable vector potential Poisson equation, ensuring incompressibility in both simulated and unsimulated regions while maintaining divergence-free liquid phases independent of solver accuracy. Yet, they triple the linear system&#x27;s dimensionality and exhibit poor convergence near solid boundaries. The fundamental limitation of both methods stems from their governing equations: singularities emerge as density approaches extreme values. The pressure Poisson equation becomes ill-conditioned when density nears zero (air phase), compromising air-phase incompressibility, while the vector potential equation degrades as density approaches infinity (solid phase), impeding solid-boundary convergence. To address these singularities, we first propose a novel decomposition where zero and infinite densities are well-defined. We then reformulate this decomposition as a fixed-point iteration using density-agnostic curl-free and divergence-free projections, eliminating the need for linear system solves. The error equation is derived, and a necessary and sufficient convergence condition is established. Building on this, we develop an iterative algorithm that efficiently solves the fixed-point problem through alternating wavelet-based non-orthogonal curl-free and divergence-free projections. Additionally, we investigate orthogonal curl-free projections (e.g., Fourier methods) and their complementary divergence-free counterparts, providing a comprehensive comparison between wavelet and Fourier approaches. Our method simultaneously computes pressure and stream functions, retaining the incompressibility benefits of stream function approaches while resolving their computational inefficiencies and solid-boundary convergence issues. Experiments demonstrate our framework&#x27;s ability to efficiently simulate complex two-phase phenomena, such as the glugging effect during water pouring and multi-liquid-region interactions across zero-density air.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            </div>
        </section>
        
        <section class="session">
            <div class="session-header">
                <h2>Text-to-Image & Customization</h2>
                <span class="session-count">6 papers</span>
            </div>
            <div class="papers-grid">
                
            <article class="paper-card" data-paper-id="papers_1026">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/2RXGZNmEq7BCWzLB.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3763270" target="_blank" rel="noopener">RL-ACD: Reinforcement Learning-based Approximate Convex Decomposition</a></h3>
                    <p class="authors">Yuzhe Luo, Zherong Pan, Kui Wu, Xingyi Du, Yun Zeng, Xiangjun Tang</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3763270" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1026', 'url', 'https://dl.acm.org/doi/10.1145/3763270')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1026', 'abstract', 'Approximate Convex Decomposition (ACD) aims to approximate complex 3D shapes with convex components, which is widely applied to create compact collision representations for real-time applications, including VR/AR, interactive games, and robotic simulations. Efficiency and optimality are critical for ACD algorithms in approximating large-scale, complex 3D shapes, enabling high-quality decompositions with minimal components. Unfortunately, existing methods either employ sub-optimal greedy strategies or rely on computationally intensive multi-step searches. In this work, we propose RL-ACD, a data-driven, reinforcement learning-based approach for efficient and near-optimal convex shape decomposition. We formulate ACD as a Markov Decision Process (MDP), where cutting planes are iteratively applied based on the current stage\'s mesh fragments rather than the entire fine-grained mesh, leading to a novel, efficient geometric encoding. To train near-optimal policies for ACD, we propose a novel dual-state Bellman loss and analyze its convergence using a Q-learning algorithm. Comprehensive evaluations across diverse datasets validate the efficiency and accuracy of RL-ACD for convex decomposition tasks. Our method outperforms the multi-step tree search by 15√ó in terms of computational speed, while reducing the number of resulting components by 16% compared to the current state-of-the-art greedy algorithms, significantly narrowing the sub-optimality gap and enhancing downstream task performance.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">Approximate Convex Decomposition (ACD) aims to approximate complex 3D shapes with convex components, which is widely applied to create compact collision representations for real-time applications, including VR/AR, interactive games, and robotic simulations. Efficiency and optimality are critical for ACD algorithms in approximating large-scale, complex 3D shapes, enabling high-quality decompositions with minimal components. Unfortunately, existing methods either employ sub-optimal greedy strategies or rely on computationally intensive multi-step searches. In this work, we propose RL-ACD, a data-driven, reinforcement learning-based approach for efficient and near-optimal convex shape decomposition. We formulate ACD as a Markov Decision Process (MDP), where cutting planes are iteratively applied based on the current stage&#x27;s mesh fragments rather than the entire fine-grained mesh, leading to a novel, efficient geometric encoding. To train near-optimal policies for ACD, we propose a novel dual-state Bellman loss and analyze its convergence using a Q-learning algorithm. Comprehensive evaluations across diverse datasets validate the efficiency and accuracy of RL-ACD for convex decomposition tasks. Our method outperforms the multi-step tree search by 15√ó in terms of computational speed, while reducing the number of resulting components by 16% compared to the current state-of-the-art greedy algorithms, significantly narrowing the sub-optimality gap and enhancing downstream task performance.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1047">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/e3hvBHgf4ibZKV4K.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763821" target="_blank" rel="noopener">ASIA: Adaptive 3D Segmentation using Few Image Annotations</a></h3>
                    <p class="authors">Sai Raj Kishore Perla, Aditya Vora, Sauradip Nag, Ali Mahdavi-Amiri, Hao Zhang</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763821" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1047', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763821')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1047', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1146">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/QALMc3mM7bHp6Z4P.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763835" target="_blank" rel="noopener">Light-SQ: Structure-aware Shape Abstraction with Superquadrics for Generated Meshes</a></h3>
                    <p class="authors">Yuhan Wang, Weikai Chen, Zeyu Hu, Runze Zhang, Yingda Yin, Ruoyu Wu</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763835" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1146', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763835')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1146', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1188">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/oYjn5Us4jFNvfK5Y.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763840" target="_blank" rel="noopener">MATStruct: High-quality Medial Mesh Computation via Structure-aware Variational Optimization</a></h3>
                    <p class="authors">Ningna Wang, Rui Xu, Yibo Yin, Zichun Zhong, Taku Komura, Wenping Wang</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763840" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1188', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763840')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1188', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1131">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/NCMTV75HsgXLkeoa.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763832" target="_blank" rel="noopener">Temporally Smooth Mesh Extraction for Procedural Scenes with Long-Range Camera Trajectories using Spacetime Octrees</a></h3>
                    <p class="authors">Zeyu Ma, Adam Finkelstein, Jia Deng</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763832" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1131', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763832')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1131', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_2466">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/9nziL6KJTixKA6b6.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3764004" target="_blank" rel="noopener">Design for Descent: What Makes a Shape Grammar Easy to Optimize?</a></h3>
                    <p class="authors">Milin Kodnongbua, Zihan Zhang, Nicholas Sharp, Adriana Schulz</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3764004" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_2466', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3764004')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_2466', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            </div>
        </section>
        
        <section class="session">
            <div class="session-header">
                <h2>Expressive and Structured Gaussian Representations</h2>
                <span class="session-count">6 papers</span>
            </div>
            <div class="papers-grid">
                
            <article class="paper-card" data-paper-id="papers_1228">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/rfDyzbM7rjjjyhrY.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3763290" target="_blank" rel="noopener">One-shot Embroidery Customization via Contrastive LoRA Modulation</a></h3>
                    <p class="authors">Jun Ma, Qian He, Gaofeng He, Huang Chen, Chen Liu, Xiaogang Jin</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3763290" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1228', 'url', 'https://dl.acm.org/doi/10.1145/3763290')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1228', 'abstract', 'Diffusion models have significantly advanced image manipulation techniques, and their ability to generate photorealistic images is beginning to transform retail workflows, particularly in presale visualization. Beyond artistic style transfer, the capability to perform fine-grained visual feature transfer is becoming increasingly important. Embroidery is a textile art form characterized by intricate interplay of diverse stitch patterns and material properties, which poses unique challenges for existing style transfer methods. To explore the customization for such fine-grained features, we propose a novel contrastive learning framework that disentangles fine-grained style and content features with a single reference image, building on the classic concept of image analogy. We first construct an image pair to define the target style, and then adopt a similarity metric based on the decoupled representations of pretrained diffusion models for style-content separation. Subsequently, we propose a two-stage contrastive LoRA modulation technique to capture fine-grained style features. In the first stage, we iteratively update the whole LoRA and the selected style blocks to initially separate style from content. In the second stage, we design a contrastive learning strategy to further decouple style and content through self-knowledge distillation. Finally, we build an inference pipeline to handle image or text inputs with only the style blocks. To evaluate our method on fine-grained style transfer, we build a benchmark for embroidery customization. Our approach surpasses prior methods on this task and further demonstrates strong generalization to three additional domains: artistic style transfer, sketch colorization, and appearance transfer. Our project is available at: https://style3d.github.io/embroidery_customization.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">Diffusion models have significantly advanced image manipulation techniques, and their ability to generate photorealistic images is beginning to transform retail workflows, particularly in presale visualization. Beyond artistic style transfer, the capability to perform fine-grained visual feature transfer is becoming increasingly important. Embroidery is a textile art form characterized by intricate interplay of diverse stitch patterns and material properties, which poses unique challenges for existing style transfer methods. To explore the customization for such fine-grained features, we propose a novel contrastive learning framework that disentangles fine-grained style and content features with a single reference image, building on the classic concept of image analogy. We first construct an image pair to define the target style, and then adopt a similarity metric based on the decoupled representations of pretrained diffusion models for style-content separation. Subsequently, we propose a two-stage contrastive LoRA modulation technique to capture fine-grained style features. In the first stage, we iteratively update the whole LoRA and the selected style blocks to initially separate style from content. In the second stage, we design a contrastive learning strategy to further decouple style and content through self-knowledge distillation. Finally, we build an inference pipeline to handle image or text inputs with only the style blocks. To evaluate our method on fine-grained style transfer, we build a benchmark for embroidery customization. Our approach surpasses prior methods on this task and further demonstrates strong generalization to three additional domains: artistic style transfer, sketch colorization, and appearance transfer. Our project is available at: https://style3d.github.io/embroidery_customization.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1391">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/w2ZMxz5RxwxhMRsn.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763870" target="_blank" rel="noopener">Teamwork: Collaborative Diffusion with Low-rank Coordination and Adaptation</a></h3>
                    <p class="authors">Sam Sartor, Pieter Peers</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763870" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1391', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763870')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1391', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1525">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/oLvS7rr4Eskttuyi.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763888" target="_blank" rel="noopener">Virtually Being: Customizing Camera-Controllable Video Diffusion Models with Volumetric Performance Captures</a></h3>
                    <p class="authors">Yuancheng Xu, Wenqi Xian, Li Ma, Julien Philip, Ahmet Ta≈üel, Yiwei Zhao</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763888" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1525', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763888')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1525', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1925">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/yYeo14RwzAQo57tN.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763949" target="_blank" rel="noopener">Proteus-ID: ID-Consistent and Motion-Coherent Video Customization</a></h3>
                    <p class="authors">Guiyu Zhang, Chen Shi, Zijian Jiang, Xunzhi Xiang, Jingjing Qian, Shaoshuai Shi</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763949" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1925', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763949')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1925', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_2007">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/zPkHU5EuhAzaWPey.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763963" target="_blank" rel="noopener">DreamID: High-Fidelity and Fast diffusion-based Face Swapping via Triplet ID Group Learning</a></h3>
                    <p class="authors">Fulong Ye, Miao Hua, Pengze Zhang, Xinghui Li, Qichao Sun, Songtao Zhao</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763963" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_2007', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763963')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_2007', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_2123">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/cTGvT7CazXo3xQN5.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763979" target="_blank" rel="noopener">HiWave: Training-Free High-Resolution Image Generation via Wavelet-Based Diffusion Sampling</a></h3>
                    <p class="authors">Tobias Vontobel, Seyedmorteza Sadat, Farnood Salehi, Romann Weber</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763979" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_2123', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763979')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_2123', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            </div>
        </section>
        
        <section class="session">
            <div class="session-header">
                <h2>Generative Synthesis, Editing & Customization</h2>
                <span class="session-count">6 papers</span>
            </div>
            <div class="papers-grid">
                
            <article class="paper-card" data-paper-id="papers_1801">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/8Sz3qyLuy3Ws8Q1C.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763926" target="_blank" rel="noopener">Gradient-Weighted Feature Back-Projection: A Fast Alternative to Feature Distillation in 3D Gaussian Splatting</a></h3>
                    <p class="authors">Joji Joseph, Bharadwaj Amrutur, Shalabh Bhatnagar</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763926" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1801', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763926')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1801', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1846">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/Z1NstK7r1LSUEjzA.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763937" target="_blank" rel="noopener">Rigidity-Aware 3D Gaussian Deformation from a Single Image</a></h3>
                    <p class="authors">Jinhyeok Kim, Jaehun Bang, Seunghyun Seo, Kyungdon Joo</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763937" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1846', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763937')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1846', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1988">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/adap6mYWYnuzssF7.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763957" target="_blank" rel="noopener">Neural Texture Splatting: Expressive 3D Gaussian Splatting for View Synthesis, Geometry, and Dynamic Reconstruction</a></h3>
                    <p class="authors">Yiming Wang, Shaofei Wang, Marko Mihajlovic, Siyu Tang</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763957" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1988', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763957')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1988', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_2060">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/W9NGZQgXeY59iWQL.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3763347" target="_blank" rel="noopener">JumpingGS: Level-jump 3D Gaussian Representation for Delicate Textures in Aerial Large-scale Scene Rendering</a></h3>
                    <p class="authors">Jiongming Qin, Kaixuan Zhou, Yu Jiang, Huizhi Zhu, Fei Luo, Chunxia Xiao</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3763347" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_2060', 'url', 'https://dl.acm.org/doi/10.1145/3763347')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_2060', 'abstract', 'Existing 3D Gaussian (3DGS) based methods tend to produce blurriness and artifacts on delicate textures (small objects and high-frequency textures) in aerial large-scale scenes. The reason is that the delicate textures usually occupy a relatively small number of pixels, and the accumulated gradients from loss function are difficult to promote the splitting of 3DGS. To minimize the rendering error, the model will use a small number of large Gaussians to cover these details, resulting in blurriness and artifacts. To solve the above problem, we propose a novel hierarchical Gaussian: JumpingGS. JumpingGS assigns different levels to Gaussians to establish a hierarchical representation. Low-level Gaussians are responsible for the coarse appearance, while high-level Gaussians are responsible for the details. First, we design a splitting strategy that allows low-level Gaussians to skip intermediate levels and directly split the appropriate high-level Gaussians for delicate textures. This level-jump splitting ensures that the weak gradients of delicate textures can always activate a higher level instead of being ignored by the intermediate levels. Second, JumpingGS reduces the gradient and opacity thresholds for density control according to the representation levels, which improves the sensitivity of high-level Gaussians to delicate textures. Third, we design a novel training strategy to detect training views in hard-to-observe regions, and train the model multiple times on these views to alleviate underfitting. Experiments on aerial large-scale scenes demonstrate that JumpingGS outperforms existing 3DGS-based methods, accurately and efficiently recovering delicate textures in large scenes.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">Existing 3D Gaussian (3DGS) based methods tend to produce blurriness and artifacts on delicate textures (small objects and high-frequency textures) in aerial large-scale scenes. The reason is that the delicate textures usually occupy a relatively small number of pixels, and the accumulated gradients from loss function are difficult to promote the splitting of 3DGS. To minimize the rendering error, the model will use a small number of large Gaussians to cover these details, resulting in blurriness and artifacts. To solve the above problem, we propose a novel hierarchical Gaussian: JumpingGS. JumpingGS assigns different levels to Gaussians to establish a hierarchical representation. Low-level Gaussians are responsible for the coarse appearance, while high-level Gaussians are responsible for the details. First, we design a splitting strategy that allows low-level Gaussians to skip intermediate levels and directly split the appropriate high-level Gaussians for delicate textures. This level-jump splitting ensures that the weak gradients of delicate textures can always activate a higher level instead of being ignored by the intermediate levels. Second, JumpingGS reduces the gradient and opacity thresholds for density control according to the representation levels, which improves the sensitivity of high-level Gaussians to delicate textures. Third, we design a novel training strategy to detect training views in hard-to-observe regions, and train the model multiple times on these views to alleviate underfitting. Experiments on aerial large-scale scenes demonstrate that JumpingGS outperforms existing 3DGS-based methods, accurately and efficiently recovering delicate textures in large scenes.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_2424">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/ey2cwPPhoAWTzM9K.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3764001" target="_blank" rel="noopener">TC-GS: A Faster Gaussian Splatting Module Utilizing Tensor Cores</a></h3>
                    <p class="authors">Zimu Liao, Jifeng Ding, Siwei Cui, Ruixuan Gong, Boni Hu, Yi Wang</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3764001" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_2424', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3764001')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_2424', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1347">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/vLgYT2HvNZPPFX8B.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763860" target="_blank" rel="noopener">DeMapGS: Simultaneous Mesh Deformation and Surface Attribute Mapping via Gaussian Splatting</a></h3>
                    <p class="authors">Shuyi Zhou, Shengze Zhong, Kenshi Takayama, Takafumi Taketomi, Takeshi Oishi</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763860" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1347', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763860')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1347', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            </div>
        </section>
        
        <section class="session">
            <div class="session-header">
                <h2>Human Motion Synthesis & Interaction</h2>
                <span class="session-count">5 papers</span>
            </div>
            <div class="papers-grid">
                
            <article class="paper-card" data-paper-id="papers_1036">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/BFB2JFXvg5f3rtj6.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763819" target="_blank" rel="noopener">Physics-Based Motion Imitation with Adversarial Differential Discriminators</a></h3>
                    <p class="authors">Ziyu Zhang, Sergey Bashkirov, Dun Yang, Yi Shi, Michael Taylor, Xue Bin Peng</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763819" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1036', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763819')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1036', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1923">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/bpLByj5pzMP3zLvh.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763948" target="_blank" rel="noopener">Learning Human Motion with Temporally Conditional Mamba</a></h3>
                    <p class="authors">Quang Nguyen, Tri Le, Baoru Huang, Minh Nhat Vu, Ngan Le, Thieu Vo</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763948" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1923', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763948')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1923', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1066">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/uEk8PNoEgT8eB9oW.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763823" target="_blank" rel="noopener">SRBTrack: Terrain-Adaptive Tracking of a Single-Rigid-Body Character Using Momentum-Mapped Space-Time Optimization</a></h3>
                    <p class="authors">Hanyang Cao, Heyuan Yao, Libin Liu, Taesoo Kwon</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763823" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1066', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763823')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1066', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1970">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/J2aCNo9iXQpisMaS.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763954" target="_blank" rel="noopener">Uni-Inter Unifying 3D Human Motion Synthesis Across Diverse Interaction Contexts</a></h3>
                    <p class="authors">Sheng Liu, Yuanzhi Liang, Jiepeng Wang, Sidan Du, Chi Zhang, Xuelong Li</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763954" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1970', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763954')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1970', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1359">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/ooit9z4aEE6NK5dG.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763861" target="_blank" rel="noopener">HOMA: Towards Generic Human-Object Interaction in Multimodal Driven Human Animation with Weak Conditions</a></h3>
                    <p class="authors">Ziyao Huang, Zixiang Zhou, Juan Cao, Yifeng Ma, Yi Chen, Zejing Rao</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763861" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1359', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763861')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1359', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            </div>
        </section>
        
        <section class="session">
            <div class="session-header">
                <h2>Shape Abstraction and Structural Analysis</h2>
                <span class="session-count">3 papers</span>
            </div>
            <div class="papers-grid">
                
            <article class="paper-card" data-paper-id="papers_1497">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/N6z4zczjtGJh6Fht.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763886" target="_blank" rel="noopener">Closed-form Cauchy Coordinates and Their Derivatives for 2D High-order Cages</a></h3>
                    <p class="authors">Shibo Liu, Ligang Liu, Xiao-Ming Fu</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763886" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1497', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763886')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1497', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1260">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/hUaAPnyeHPTDqKvv.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3763292" target="_blank" rel="noopener">Aerial Path Planning for Urban Geometry and Texture Co-Capture</a></h3>
                    <p class="authors">Weidan Xiong, Bochuan Zeng, Ziyu Hu, Jianwei Guo, Ke Xie, Hui Huang</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3763292" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1260', 'url', 'https://dl.acm.org/doi/10.1145/3763292')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1260', 'abstract', 'Recent advances in image acquisition and scene reconstruction have enabled the generation of high-quality structural urban scene geometry, given sufficient site information. However, current capture techniques often overlook the crucial importance of texture quality, resulting in noticeable visual artifacts in the textured models. In this work, we introduce the urban geometry and texture co-capture problem under limited prior knowledge before a site visit. The only inputs are a 2D building contour map of the target area and a safe flying altitude above the buildings. We propose an innovative aerial path planning framework designed to co-capture images for reconstructing both structured geometry and high-fidelity textures. To evaluate and guide view planning, we introduce a comprehensive texture quality assessment system, including two novel metrics tailored for building facades. Firstly, our method generates high-quality vertical dipping views and horizontal planar views to effectively capture both geometric and textural details. A multi-objective optimization strategy is then proposed to jointly maximize texture fidelity, improve geometric accuracy, and minimize the cost associated with aerial views. Furthermore, we present a sequential path planning algorithm that accounts for texture consistency during image capture. Extensive experiments on large-scale synthetic and real-world urban datasets demonstrate that our approach effectively produces image sets suitable for concurrent geometric and texture reconstruction, enabling the creation of realistic, textured scene proxies at low operational cost.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">Recent advances in image acquisition and scene reconstruction have enabled the generation of high-quality structural urban scene geometry, given sufficient site information. However, current capture techniques often overlook the crucial importance of texture quality, resulting in noticeable visual artifacts in the textured models. In this work, we introduce the urban geometry and texture co-capture problem under limited prior knowledge before a site visit. The only inputs are a 2D building contour map of the target area and a safe flying altitude above the buildings. We propose an innovative aerial path planning framework designed to co-capture images for reconstructing both structured geometry and high-fidelity textures. To evaluate and guide view planning, we introduce a comprehensive texture quality assessment system, including two novel metrics tailored for building facades. Firstly, our method generates high-quality vertical dipping views and horizontal planar views to effectively capture both geometric and textural details. A multi-objective optimization strategy is then proposed to jointly maximize texture fidelity, improve geometric accuracy, and minimize the cost associated with aerial views. Furthermore, we present a sequential path planning algorithm that accounts for texture consistency during image capture. Extensive experiments on large-scale synthetic and real-world urban datasets demonstrate that our approach effectively produces image sets suitable for concurrent geometric and texture reconstruction, enabling the creation of realistic, textured scene proxies at low operational cost.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1471">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/zgneA8r6JyFB8w83.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763880" target="_blank" rel="noopener">Compact shape representation utilizing local surface similarities</a></h3>
                    <p class="authors">Albert Garifullin, Nikolay Mayorov, Alexey Budak, Sergei Nikitin, Egor Prikhodko, Roman Rodionov</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763880" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1471', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763880')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1471', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            </div>
        </section>
        
        <section class="session">
            <div class="session-header">
                <h2>Advanced Representations and Rendering for 3D Scenes</h2>
                <span class="session-count">6 papers</span>
            </div>
            <div class="papers-grid">
                
            <article class="paper-card" data-paper-id="papers_1039">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/MJpSoCKLGpBKnHJW.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763820" target="_blank" rel="noopener">In-Context Brush: Zero-shot Customized Subject Insertion with Context-Aware Latent Space Manipulation</a></h3>
                    <p class="authors">Yu Xu, Fan Tang, You Wu, Lin Gao, Oliver Deussen, Hongbin Yan</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763820" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1039', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763820')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1039', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1615">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/rY5mDK9dD7caW3gF.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763897" target="_blank" rel="noopener">BlobCtrl: Taming Controllable Blob for Element-level Image Editing</a></h3>
                    <p class="authors">Yaowei Li, Lingen Li, Zhaoyang Zhang, Xiaoyu Li, Guangzhi Wang, Hongxiang Li</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763897" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1615', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763897')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1615', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1683">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/P4pHAhy1uGExumbi.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763909" target="_blank" rel="noopener">ConsistEdit: Highly Consistent and Precise Training-free Visual Editing</a></h3>
                    <p class="authors">Zixin Yin, Ling-Hao Chen, Lionel Ni, Xili Dai</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763909" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1683', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763909')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1683', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1867">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/UNRZrkbJxRYRPZMs.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763940" target="_blank" rel="noopener">Voost: A Unified and Scalable Diffusion Transformer for Bidirectional Virtual Try-On and Try-Off</a></h3>
                    <p class="authors">Seungyong Lee, Jeonggi Kwak</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763940" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1867', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763940')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1867', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1976">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/TZwUsfdzGQsPYgzE.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763956" target="_blank" rel="noopener">DreamO: A Unified Framework for Image Customization</a></h3>
                    <p class="authors">Chong Mou, Yanze Wu, Wenxu Wu, Zinan Guo, Pengze Zhang, Yufeng Cheng</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763956" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1976', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763956')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1976', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_2211">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/b1qaq5pp3E4CgESR.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763988" target="_blank" rel="noopener">The Aging Multiverse: Generating Condition-Aware Facial Aging Tree via Training-Free Diffusion</a></h3>
                    <p class="authors">Bang Gong, Luchao Qi, Jiaye Wu, Zhicheng Fu, Chunbo Song, John Nicholson</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763988" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_2211', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763988')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_2211', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            </div>
        </section>
        
        <section class="session">
            <div class="session-header">
                <h2>Diffusion-Based Image Editing & Manipulation</h2>
                <span class="session-count">6 papers</span>
            </div>
            <div class="papers-grid">
                
            <article class="paper-card" data-paper-id="papers_1174">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/PGFys8uuNagibFDd.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763838" target="_blank" rel="noopener">LVT: Large-Scale Scene Reconstruction via Local View Transformers</a></h3>
                    <p class="authors">Tooba Imtiaz, Lucy Chai, Kathryn Heal, Xuan Luo, Jungyeon Park, Jennifer Dy</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763838" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1174', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763838')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1174', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1379">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/CfxBqgbog6PWo8Bq.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763866" target="_blank" rel="noopener">CityGo: Lightweight Urban Modeling and Rendering with Proxy Buildings and Residual Gaussians</a></h3>
                    <p class="authors">Weihang Liu, Yuhui Zhong, Yuke Li, Xi Chen, Jiadi Cui, Honglong Zhang</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763866" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1379', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763866')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1379', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_2032">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/VyGUYZzt4NEEcFfm.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763967" target="_blank" rel="noopener">GSWT: Gaussian Splatting Wang Tiles</a></h3>
                    <p class="authors">Yunfan Zeng, Li Ma, Pedro Sander</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763967" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_2032', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763967')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_2032', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1679">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/kMfFfjoum8Pw9m7W.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763907" target="_blank" rel="noopener">Spectral-GS: Taming 3D Gaussian Splatting with Spectral Entropy</a></h3>
                    <p class="authors">Letian Huang, Jie Guo, Jialin Dan, Ruoyu Fu, Yuanqi Li, Yanwen Guo</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763907" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1679', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763907')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1679', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1217">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/79gtNcS2z9G6j618.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763846" target="_blank" rel="noopener">A compact stochastic representation for Monte Carlo Path Traced images</a></h3>
                    <p class="authors">Matthias Sebastian Treder, Pavlos Makridis, Alexis Lechat, Jesus Zarzar, Marina Villanueva Barreiro, Roc Ramon Currius</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763846" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1217', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763846')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1217', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_2062">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/FggZLJjzHBawRG3k.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763971" target="_blank" rel="noopener">Editable Physically-based Reflections in Raytraced Gaussian Radiance Fields</a></h3>
                    <p class="authors">Yohan Poirier-Ginter, Jeffrey Hu, Jean-Francois Lalonde, George Drettakis</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763971" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_2062', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763971')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_2062', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            </div>
        </section>
        
        <section class="session">
            <div class="session-header">
                <h2>Geometry Processing & Representations</h2>
                <span class="session-count">5 papers</span>
            </div>
            <div class="papers-grid">
                
            <article class="paper-card" data-paper-id="papers_1159">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/rxGF1GByzFvqDY85.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3763285" target="_blank" rel="noopener">LEGO-Maker: Autoregressive Image-Conditioned LEGO Model Creation</a></h3>
                    <p class="authors">Jiahao Ge, Mingjun Zhou, Hanyou Zheng, Hao Xu, Chi-Wing Fu</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3763285" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1159', 'url', 'https://dl.acm.org/doi/10.1145/3763285')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1159', 'abstract', 'This paper presents LEGO ¬Æ -Maker, a new learning-based generative model that can effectively consider over 100 unique brick types and rapidly generate hundreds of bricks to create LEGO ¬Æ models conditioned on images. This work has three major technical contributions that enable it to achieve surpassing capabilities beyond existing generative approaches. First, we design a compact LEGO ¬Æ tokenization scheme to serialize LEGO ¬Æ models and bricks into tokens for autoregressive learning. Second, we build LEGO ¬Æ -Maker, an autoregressive image-conditioned architecture, with a multi-token prediction strategy to encourage pre-considering multiple brick attributes and a rollback mechanism for collision-free generation. Third, we propose an effective data preparation pipeline with a procedural generator to synthesize LEGO ¬Æ models and a LEGO ¬Æ -to-real image translator distilled from a large vision language model to translate LEGO ¬Æ renderings into associated photorealistic images, leveraging rich prior to address the scarcity of image-to-LEGO ¬Æ data. Extensive evaluations and comparisons are conducted on two object categories, facade and portrait, over metrics in four aspects: geometry, color, semantics, and structural integrity, together with a user study. Experimental results demonstrate the versatility and compelling strengths of LEGO ¬Æ -Maker in producing structures and details given by the reference image. Also, the evaluation scores manifest that our method clearly surpasses the baselines, consistently for all evaluation metrics.')" title="Edit Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body">This paper presents LEGO ¬Æ -Maker, a new learning-based generative model that can effectively consider over 100 unique brick types and rapidly generate hundreds of bricks to create LEGO ¬Æ models conditioned on images. This work has three major technical contributions that enable it to achieve surpassing capabilities beyond existing generative approaches. First, we design a compact LEGO ¬Æ tokenization scheme to serialize LEGO ¬Æ models and bricks into tokens for autoregressive learning. Second, we build LEGO ¬Æ -Maker, an autoregressive image-conditioned architecture, with a multi-token prediction strategy to encourage pre-considering multiple brick attributes and a rollback mechanism for collision-free generation. Third, we propose an effective data preparation pipeline with a procedural generator to synthesize LEGO ¬Æ models and a LEGO ¬Æ -to-real image translator distilled from a large vision language model to translate LEGO ¬Æ renderings into associated photorealistic images, leveraging rich prior to address the scarcity of image-to-LEGO ¬Æ data. Extensive evaluations and comparisons are conducted on two object categories, facade and portrait, over metrics in four aspects: geometry, color, semantics, and structural integrity, together with a user study. Experimental results demonstrate the versatility and compelling strengths of LEGO ¬Æ -Maker in producing structures and details given by the reference image. Also, the evaluation scores manifest that our method clearly surpasses the baselines, consistently for all evaluation metrics.</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1473">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/6bfodiNPbdcbP9sW.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763881" target="_blank" rel="noopener">LegoACE: Autoregressive Construction Engine for Expressive LEGO Assemblies</a></h3>
                    <p class="authors">Hao Xu, Yuqing Zhang, Yiqian Wu, Xinyang Zheng, Yutao Liu, Xiangjun Tang</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763881" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1473', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763881')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1473', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1432">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/KqkuxWEdBScSAEtF.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763875" target="_blank" rel="noopener">Computational Design of Shape-Aware Sieves</a></h3>
                    <p class="authors">David Cha, Oded Stein</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763875" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1432', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763875')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1432', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1871">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/uKc7TuErSVFC3L9w.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763941" target="_blank" rel="noopener">Designing with Tension: Nearly-Developable Patch Layouts</a></h3>
                    <p class="authors">Anna Maria Eggler, Nico Pietroni, Pengbin Tang, Michal Piovarci, Bernd Bickel</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763941" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1871', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763941')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1871', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            <article class="paper-card" data-paper-id="papers_1136">
                <div class="thumbnail-wrapper">
                    <img class="thumbnail" src="https://sa2025.conference-schedule.org/wp-content/linklings_snippets/representative_images/nmZ8ZE3w7C8W5ppu.jpg" alt="" loading="lazy">
                </div>
                <div class="card-content">
                    <h3><a class="paper-title-link" href="https://dl.acm.org/doi/10.1145/3757377.3763834" target="_blank" rel="noopener">Inverse Tiling of 2D Finite Domains</a></h3>
                    <p class="authors">Rulin Chen, Xuyang Ma, Praveer Tewari, Chi-Wing Fu, Peng Song</p>
                    
                    <div class="paper-actions">
                        
                    <div class="paper-links">
                        <a class="paper-link" href="https://dl.acm.org/doi/10.1145/3757377.3763834" target="_blank" rel="noopener">
                            üîó Open link
                            <span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('papers_1136', 'url', 'https://dl.acm.org/doi/10.1145/3757377.3763834')" title="Edit URL">‚úèÔ∏è</span>
                        </a>
                    </div>
                
                        
                    <details class="abstract-toggle">
                        <summary>
                            üßª Abstract
                            <button class="edit-btn-inline" onclick="event.stopPropagation(); openEditModal('papers_1136', 'abstract', '')" title="Add Abstract">‚úèÔ∏è</button>
                        </summary>
                        <div class="abstract-body abstract-missing">Abstract not available yet (you can paste it into url.json).</div>
                    </details>
                
                    </div>
                
                </div>
            </article>
            
            </div>
        </section>
        
    </main>
    
    <footer>
        <p>Data sourced from <a href="https://sa2025.conference-schedule.org/" target="_blank" rel="noopener">SIGGRAPH Asia 2025 Conference Schedule</a></p>
        <p style="margin-top: 0.5rem; opacity: 0.7;">Generated with Python</p>
    </footer>
    
    <!-- Edit Modal -->
    <div id="editModal" class="edit-modal">
        <div class="edit-modal-content">
            <div class="edit-modal-header">
                <h3 id="modalTitle">Edit</h3>
                <button class="edit-modal-close" onclick="closeEditModal()">√ó</button>
            </div>
            <div class="edit-modal-body">
                <label id="modalLabel" for="editInput">Value:</label>
                <input type="text" id="editInput" style="display: none;">
                <textarea id="editTextarea" style="display: none;"></textarea>
            </div>
            <div class="edit-modal-footer">
                <button class="edit-modal-btn cancel" onclick="closeEditModal()">Cancel</button>
                <button class="edit-modal-btn save" onclick="saveEdit()">Save</button>
            </div>
        </div>
    </div>
    
    <!-- Export Button -->
    <div class="export-btn-container">
        <button class="export-btn" onclick="exportToJson()">üì• Export url.json</button>
    </div>
    
    <script>
        let currentEdit = {'paperId': null, 'field': null};
        const editsKey = 'siggraph_paper_edits';
        
        function openEditModal(paperId, field, currentValue) {
            currentEdit.paperId = paperId;
            currentEdit.field = field;
            
            const modal = document.getElementById('editModal');
            const title = document.getElementById('modalTitle');
            const label = document.getElementById('modalLabel');
            const input = document.getElementById('editInput');
            const textarea = document.getElementById('editTextarea');
            
            if (field === 'url') {
                title.textContent = 'Edit URL';
                label.textContent = 'URL:';
                input.style.display = 'block';
                textarea.style.display = 'none';
                input.value = currentValue || '';
                input.focus();
            } else if (field === 'abstract') {
                title.textContent = 'Edit Abstract';
                label.textContent = 'Abstract:';
                input.style.display = 'none';
                textarea.style.display = 'block';
                textarea.value = currentValue || '';
                textarea.focus();
            }
            
            modal.classList.add('active');
        }
        
        function closeEditModal() {
            document.getElementById('editModal').classList.remove('active');
            currentEdit.paperId = null;
            currentEdit.field = null;
        }
        
        function saveEdit() {
            if (!currentEdit.paperId || !currentEdit.field) return;
            
            const input = document.getElementById('editInput');
            const textarea = document.getElementById('editTextarea');
            const value = currentEdit.field === 'url' ? input.value.trim() : textarea.value.trim();
            
            // Save to localStorage
            let edits = JSON.parse(localStorage.getItem(editsKey) || '{}');
            if (!edits[currentEdit.paperId]) {
                edits[currentEdit.paperId] = {};
            }
            edits[currentEdit.paperId][currentEdit.field] = value;
            localStorage.setItem(editsKey, JSON.stringify(edits));
            
            // Update the UI immediately
            updateUI(currentEdit.paperId, currentEdit.field, value);
            
            closeEditModal();
        }
        
        function updateUI(paperId, field, value) {
            const card = document.querySelector(`[data-paper-id="${paperId}"]`);
            if (!card) return;
            
            if (field === 'url') {
                const linkDiv = card.querySelector('.paper-links');
                if (value) {
                    const link = linkDiv.querySelector('.paper-link');
                    if (link) {
                        link.href = value;
                        // Update or add edit icon inside the link
                        let editIcon = link.querySelector('.edit-icon-inline');
                        const escapedValue = value.replace(/'/g, "\\'");
                        if (!editIcon) {
                            editIcon = document.createElement('span');
                            editIcon.className = 'edit-icon-inline';
                            editIcon.title = 'Edit URL';
                            editIcon.textContent = '‚úèÔ∏è';
                            editIcon.onclick = (e) => {
                                e.preventDefault();
                                e.stopPropagation();
                                openEditModal(paperId, 'url', value);
                            };
                            link.appendChild(editIcon);
                        } else {
                            editIcon.onclick = (e) => {
                                e.preventDefault();
                                e.stopPropagation();
                                openEditModal(paperId, 'url', value);
                            };
                        }
                    } else {
                        const escapedValue = value.replace(/'/g, "\\'");
                        linkDiv.innerHTML = `<a class="paper-link" href="${value}" target="_blank" rel="noopener">üîó Open link<span class="edit-icon-inline" onclick="event.preventDefault(); event.stopPropagation(); openEditModal('${paperId}', 'url', '${escapedValue}')" title="Edit URL">‚úèÔ∏è</span></a>`;
                    }
                }
                // Update title link too
                const titleLink = card.querySelector('.paper-title-link');
                if (titleLink) {
                    titleLink.href = value;
                }
            } else if (field === 'abstract') {
                const details = card.querySelector('.abstract-toggle');
                if (details) {
                    const body = details.querySelector('.abstract-body');
                    if (body) {
                        body.textContent = value || 'Abstract not available yet (you can paste it into url.json).';
                        body.classList.toggle('abstract-missing', !value);
                    }
                }
            }
        }
        
        function exportToJson() {
            const edits = JSON.parse(localStorage.getItem(editsKey) || '{}');
            if (Object.keys(edits).length === 0) {
                alert('No edits to export! Make some edits first.');
                return;
            }
            
            // Load original url.json structure
            fetch('url.json')
                .then(r => r.json())
                .then(original => {
                    // Merge edits into original
                    const updated = original.map(entry => {
                        const pid = entry.id;
                        if (edits[pid]) {
                            if (edits[pid].url !== undefined) entry.url = edits[pid].url;
                            if (edits[pid].abstract !== undefined) entry.abstract = edits[pid].abstract;
                        }
                        return entry;
                    });
                    
                    // Download as JSON file
                    const blob = new Blob([JSON.stringify(updated, null, 2)], {'type': 'application/json'});
                    const url = URL.createObjectURL(blob);
                    const a = document.createElement('a');
                    a.href = url;
                    a.download = 'url.json';
                    a.click();
                    URL.revokeObjectURL(url);
                })
                .catch(() => {
                    alert('Could not load url.json. Edits saved to localStorage only.');
                });
        }
        
        // Load edits from localStorage on page load
        window.addEventListener('DOMContentLoaded', () => {
            const edits = JSON.parse(localStorage.getItem(editsKey) || '{}');
            for (const [paperId, fields] of Object.entries(edits)) {
                if (fields.url !== undefined) updateUI(paperId, 'url', fields.url);
                if (fields.abstract !== undefined) updateUI(paperId, 'abstract', fields.abstract);
            }
        });
        
        // Close modal on background click
        document.getElementById('editModal').addEventListener('click', (e) => {
            if (e.target.id === 'editModal') closeEditModal();
        });
    </script>
</body>
</html>
